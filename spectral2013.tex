%TODO
%-- Add analysis to Section on Affine-invariant algo for mixtures
%-- Add analysis to Section on Isotropic RP
%-- Expand biblio
%-- Resize all overfull pages

\documentclass{book}

\usepackage{epsfig,
%pictex,
amssymb,amsmath,amsthm,
%hyperref
}
\usepackage{color}

\newtheorem{theorem}{Theorem}[chapter]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{algorithm}{Algorithm}[chapter]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{claim}{Claim}

\newtheorem{definition}[theorem]{Definition}
\newtheorem{example}[theorem]{Example}
\newtheorem{exercise}{Exercise}
\numberwithin{exercise}{chapter}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{fact}[theorem]{Fact}

\begin{document}
\frontmatter
\title{\LARGE\bf Spectral Algorithms (draft)}

\author{
Ravindran Kannan
\and
Santosh Vempala
}

\date{August, 2010}

\maketitle

\noindent {\bf Summary.}
Spectral methods refer to the use of eigenvalues, eigenvectors, singular values and singular vectors.
They are widely used in Engineering, Applied Mathematics and Statistics. More recently,
spectral methods have found numerous applications in Computer Science to ``discrete'' as well
``continuous'' problems.
This book describes modern applications of spectral methods,
and novel algorithms for estimating spectral parameters.

In the first part of the book, we present applications of spectral
methods to problems from a variety of topics including combinatorial optimization,
learning and clustering.

The second part of the book is motivated by efficiency considerations.
A feature of many modern applications is the massive amount of input data. While sophisticated algorithms for matrix computations have been developed over
a century, a more recent development
is algorithms based on ``sampling on the fly'' from massive matrices. Good estimates of singular values and low rank approximations of the whole matrix can be provably derived from
a sample. Our main emphasis in the second part of the book is to present these sampling methods
with rigorous error bounds. We also present recent extensions of spectral
methods from matrices to tensors and their applications to some combinatorial optimization
problems.

%This book presents
%algorithms based on spectral methods and applications of these algorithms to problems from
%multiple domains. Typically, a {\em spectral algorithm} takes a matrix or tensor as input
%(either explicitly or implicitly) and works by computing eigenvalues and eigenvectors during
%its execution. The emphasis of this book is on rigorous guarantees, sampling-based approaches which complement
%classical linear algebra, and geometric intuition.

%\setcounter{page}{4}
\tableofcontents

%Amended by AMF September 12, 2002.
%\usepackage{showkeys}

%\addtolength{\textwidth}{1.2in}
%\addtolength{\oddsidemargin}{-1in}
%\addtolength{\evensidemargin}{-0.5in}
%\addtolength{\topmargin}{-0.5in}
%\addtolength{\textheight}{1in}
%\addtolength{\footheight}{-1 in}

\newcommand{\norm}[1]{\|#1\|}
\newcommand{\lrnorm}[1]{\left\|#1\right\|}
\newcommand{\hnorm}[1]{\lvert#1\rvert}
\newcommand{\fnorm}[1]{\norm{#1}_{F}}
\newcommand{\fnorms}[1]{\norm{#1}_{F}^2}
\newcommand{\linspan}{\operatorname{span}}
\newcommand{\proj}{\pi}
\newcommand{\rowspan}{\operatorname{span}}
\newcommand{\suchthat}{\;:\;}
\newcommand{\abs}[1]{\lvert#1\rvert}
\newcommand{\bfm}[1]{\mbox{\boldmath $#1$}}
\newcommand{\reals}{\mbox{\bfm{R}}}
\def\grad{\bigtriangledown}
\def\Hes{\hbox{Hes}}
\def\for{\hbox{for }}
\def\Prob{\hbox{Pr}}
\def\span{\hbox{span}}
%\newcommand{\stack}[2]{\genfrac{}{}{0pt}{}{#1}{#2}}
\newcounter{rot}
\newcommand{\bignote}[1]{\vspace{0.25 in}\fbox{\parbox{6in}{#1}}%
\marginpar{\fbox{{\bf {\large Q \therot}}}}\addtocounter{rot}{1}}
\newcommand{\mnote}[1]{\marginpar{\footnotesize\raggedright\linepenalty=5000 #1}}
\def\eps{\epsilon}
\def\bB{{\bf C}}
\def\cL{{\cal L}}
\def\bh{{\bf h}}
\def\bu{{\bf u}}
\def\bv{{\bf v}}
\def\bg{{\bf g}}
\def\bx{{\bf x}}
\def\by{{\bf y}}
\def\bz{{\bf z}}
\def\cT{{\cal T}}
\def\lat{{\cal L}}
\def\cE{{\mathcal E}}

\newcommand{\Z}{\ensuremath{\mathbb{Z}}}
\newcommand{\B}{\ensuremath{\mathbb{B}}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\llnorm}[1]{{\| #1 \|}}
\newcommand{\comment}[1]{*\marginpar{\begin{footnotesize}*#1\end{footnotesize}}}
\newcommand{\card}[1]{\lvert#1\rvert}
\newcommand{\ignore}[1]{}
\def\FP{FIND}
\def\prob{\hbox{Pr}}
\def\Min{\hbox{Min}}
% Greek letters
\def\a{\alpha} \def\b{\beta} \def\d{\delta} \def\D{\Delta}
\def\e{\epsilon} \def\f{\phi} \def\F{{\Phi}} \def\g{\gamma}
\def\G{\Gamma} \def\k{\kappa}\def\K{\Kappa}
\def\bw{{\bf w}}
\def\z{\zeta} \def\th{\theta} \def\TH{\Theta}  \def\l{\lambda}
\def\La{\Lambda} \def\m{\mu} \def\n{\nu} \def\p{\pi}
\def\r{\rho}
%\def\R{\Rho}
\def\s{\sigma}
\def\S{\Sigma}
\def\t{\tau} \def\om{\omega} \def\OM{\Omega}
\newcommand{\bP}[1]{{\bf P#1}}
\newcommand{\Mix}{{\mathcal{F}}}       %Mixture
\newcommand{\wvar}{{\alpha}}       %Mean
\newcommand{\wmin}{w}    %Weight (mixing)
\newcommand{\Sx}{{\Sigma}}    %Covariance


\def\R{\reals}

%Layouts
\newtheorem{Remark}{Remark}
\newcommand{\proofstart}{{\bf Proof\hspace{2em}}}
\newcommand{\proofend}{\hspace*{\fill}\mbox{$\Box$}}

%%%%%%%%%%%%%%%%%%%%%%math stuff%%%%%%%%%%%%%%%%
\newcommand{\ooi}{(1+o(1))}
\newcommand{\ul}[1]{\mbox{\boldmath$#1$}}
\newcommand{\wh}[1]{\widehat{#1}}
%\newcommand{\wt}[1]{\widetilde{#1}}
\newcommand{\Poly}{{\mathop{\rm poly}\nolimits}}
\newcommand{\mx}{{\mu}}       %Mean


\newcommand{\rdup}[1]{{\lceil #1 \rceil }}
\newcommand{\rdown}[1]{{\lfloor #1 \rfloor}}

\newcommand{\brac}[1]{\left(#1\right)}
\newcommand{\sbrac}[1]{\left({\scriptstyle #1}\right)}
\newcommand{\tbrac}[1]{\left({\textstyle #1}\right)}
\newcommand{\smorl}[1]{{\scriptstyle #1}}
\newcommand{\sbfrac}[2]{\left(\frac{\scriptstyle #1}{\scriptstyle #2}\right)}
\newcommand{\sfrac}[2]{\frac{\scriptstyle #1}{\scriptstyle #2}}
\newcommand{\bfrac}[2]{\left(\frac{#1}{#2}\right)}
\def\half{\sfrac{1}{2}}
\def\tO{\tilde{O}}
\newcommand{\rai}{\rightarrow \infty}
\newcommand{\ra}{\rightarrow}


\def\Inf{\hbox{Inf}}
\def\sm{\setminus}
\def\seq{\subseteq}
\def\es{\emptyset}
\newcommand{\ind}[1]{\mbox{{\large 1}} \{#1\}}

\def\E{\hbox{{\sf E}\;}}
\def\Pr{\mbox{{\bf Pr}}}
\def\whp{\mbox{${\bf whp}\;$}}
\def\Max{\hbox{MAX}}
\def\OPT{{\bf OPT}}
\def\tr{\hbox{Tr}}
\def\vol{\mbox{Vol}}
\def\lnorm{\left| \left| }
\def\rnorm{\right| \right|}
\def\var{\hbox{\sf Var\;}}
\newcommand{\wt}{w}    %Weight (mixing)

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%




\mainmatter

%\include{intro: motivation, some history, models of computation}
%\include{mathreview}
%notation: superscript for column
%singular values, vectors, SVD, Hoffmann-Wielandt, tensor norms, probability,
%chernoff bounds.



\chapter*{Overview}

\def\dist{\hbox{dist}}
\newcommand{\magn}[1]{\|#1\|}

Many computational
problems have explicit matrices as their input (e.g., adjacency matrices of graphs, experimental observations etc.)
while others refer to some matrix implicitly (e.g., document-term matrices, hyperlink structure,
object-feature representations, network traffic etc.).
We refer to algorithms which use the spectrum, i.e., eigenvalues and vectors, singular values and
vectors, of the input data or matrices derived from the input as {\em Spectral Algorithms}.
Such algorithms are the focus of this book. In the first part, we
describe applications of spectral methods in algorithms for problems from combinatorial optimization,
learning, clustering, etc. In the second part of the book, we study efficient randomized algorithms for computing basic spectral quantities such as low-rank approximations.

The Singular Value Decomposition (SVD) from linear algebra and its close relative,
Principal Component Analysis (PCA), are central tools in the design of spectral algorithms.
If the rows of a matrix are viewed as points in a high-dimensional space, with the columns being the
coordinates, then SVD/PCA are typically used to reduce the dimensionality of these points, and solve
the target problem in the lower-dimensional space. The
computational advantages of such a projection are apparent; in addition, these tools are often
able to highlight hidden structure in the data. Chapter 1 provides an introduction to SVD via an application
to a generalization of the least-squares fit problem. The next three chapters are motivated by one of the most popular applications of spectral methods, namely clustering. Chapter 2 tackles a classical problem from Statistics,
learning a mixture of Gaussians from unlabeled samples; SVD leads to the current best guarantees. Chapter 3 studies
spectral clustering for discrete random inputs, using classical results from random matrices, while Chapter 4 analyzes spectral
clustering for arbitrary inputs to obtain approximation guarantees. In Chapter 5, we turn to optimization and see the application of tensors to
solving maximum constraint satisfaction problems with a bounded number of literals in each constraint. This powerful
application of low-rank tensor approximation substantially extends and generalizes a large body of work.

In the second part of the book, we begin with algorithms for matrix multiplication and low-rank matrix approximation.
These algorithms (Chapter 6) are based on sampling rows and columns of the matrix from explicit, easy-to-compute probability distributions
and lead to approximations additive error. In Chapter 7, the sampling methods are refined to obtain multiplicative error guarantees.
Finally, in Chapter 8, we see an affine-invariant extension of standard PCA and a sampling-based algorithm for low-rank
tensor approximation.

\part{Applications}
\chapter{The Best-Fit Subspace}

To provide an in-depth and relatively quick introduction to SVD and its applicability,
in this opening chapter, we consider the {\em best-fit subspace} problem.
Finding the best-fit line for a set of data points
is a classical problem.
%in statistics.
A natural measure of the quality of a line is
the least squares measure, the sum of squared
(perpendicular) distances of the points to the line. A more general problem,
for a set of data points in ${\bf R}^n$, is finding the best-fit
$k$-dimensional subspace. SVD can be used to find a subspace that minimizes the
sum of squared distances to the given set of points in polynomial time.
In contrast, for other measures such as the sum of distances or the maximum
distance, no polynomial-time algorithms are known.

A clustering problem
widely studied in theoretical computer science is the $k$-median problem. The goal is to find a set of $k$ points
that minimize the sum of the distances of the data points to their nearest facilities.
A natural relaxation of the $k$-median problem is to find the
$k$-dimensional subspace for which the sum of the distances
of the data points to the subspace is minimized (we will see that this is a relaxation).
We will apply SVD to solve this relaxed problem and use the solution to approximately solve the original problem.
%As remarked earlier, this relaxation
%is not known to be solvable in polynomial time. This is in contrast to %the
%problem involving squared distances which we call {\em $k$-variance}.

%[Note : Using the criterion of squared distances instead of just
%distances, penalizes far away points more; so this will result in a
%``tighter'' clustering which has fewer far-away points.]

\section{Singular Value Decomposition}

For an $n \times n$ matrix $A$, an eigenvalue $\lambda$ and corresponding
eigenvector $v$ satisfy the equation
\[
Av = \lambda v.
\]
In general, i.e., if the matrix has nonzero determinant, it will have
$n$ nonzero eigenvalues (not necessarily distinct) and $n$ corresponding eigenvectors. For an introduction to the theory of eigenvalues and eigenvectors, several textbooks are available.  

Here we deal with an $m \times n$ rectangular matrix $A$, where the $m$ rows
denoted $A_{(1)},A_{(2)},\ldots A_{(m)}$ are
points in ${\bf R}^n$; $A_{(i)}$ will be a row vector.

If $m\neq n$,
the notion of an eigenvalue or eigenvector does not make
sense, since the vectors $Av$ and $\lambda v$ have different dimensions.
Instead, a \emph{singular value} $\sigma$ and
corresponding \emph{singular vectors} $u \in \R^m, v \in \R^n$
simultaneously satisfy the following two equations
\begin{enumerate}
\item $Av = \sigma u$
\item $u^TA = \sigma v^T$.
\end{enumerate}

%These conditions are quite special.  In general, we would not expect an
%arbitrary pair of vectors to satisfy them.

We can assume, without loss of generality, that $u$ and $v$ are unit
vectors. To see this, note that a pair of singular vectors $u$ and $v$
must have equal length, since $u^TAv = \sigma \|u\|^2 = \sigma \|v\|^2$.
If this length is not $1$, we can rescale both by the same
factor without violating the above equations.

Now we turn our attention to the value $\max_{\|v\| = 1}\|Av\|^2$.
Since the rows of A form a set of $m$ vectors in $R^n$, the vector $Av$
is a list of the projections of these vectors onto the line spanned by
$v$, and $\|Av\|^2$ is simply the sum of the squares of those
projections.

Instead of choosing $v$ to maximize $\|Av\|^2$, the Pythagorean
theorem allows us to equivalently choose $v$ to minimize the sum of
the squared distances of the points to the line through $v$.  In this
sense, $v$ defines the line through the origin that best fits the
points.

\ignore{
\begin{figure}[htb]
\begin{center}
\input{fig1.pictex}
\end{center}
\caption{The vector $A_{(i)}$ projected onto $v$.}
\end{figure}
}

To argue this more formally,
Let $d(A_{(i)},v)$ denote the distance of the point
$A_{(i)}$ to the line through $v$.  Alternatively, we can write
\[
d(A_{(i)},v) = \magn{A_{(i)} - (A_{(i)}v)v^T}.
\]
%This is illustrated in Figure ??.
For a unit vector $v$, the Pythagorean theorem tells us that
\[
\|A_{(i)}\|^2 = \|(A_{(i)}v)v^T\|^2 + d(A_{(i)},v)^2.
\]
Thus we get the following proposition:

\begin{proposition}
$$\max_{\|v\| = 1}\|Av\|^2 = ||A||_F^2-\min_{\|v\| = 1}\|A -
(Av)v^T\|_F^2 = ||A||_F^2- \min_{\|v\| = 1} \sum_{i}\|A_{(i)} -
(A_{(i)}v)v^T\|^2 $$
\end{proposition}

\begin{proof}
We simply use the identity:
$$\|Av\|^2 = \sum_i \|(A_{(i)}v)v^T\|^2 = \sum_i \|A_{(i)}\|^2 - \sum_{i}\|A_{(i)} - (A_{(i)}v)v^T\|^2 $$
\end{proof}

The proposition says that the $v$ which maximizes $\magn{Av}^2$ is the
``best-fit'' vector which also minimizes $\sum_i d(A_{(i)},v)^2$.

Next, we claim that $v$ is in fact a singular vector.

\begin{proposition}\label{topsingularvector}
The vector $v_1 = \arg\max_{\|v\| = 1}\|Av\|^2$ is a
singular vector, and moreover $\|Av_1\|$ is the largest (or ``top'') singular value.
\end{proposition}

\begin{proof}
For any singular vector $v$,
\[
(A^TA)v = \sigma A^T u = \sigma^2 v.
\]
Thus, $v$ is an eigenvector of $A^TA$ with
corresponding eigenvalue $\sigma^2$.  Conversely, an eigenvector of
$A^TA$ is also a singular vector of $A$.  To see this, let $v$ be an
eigenvector of $A^TA$ with corresponding eigenvalue $\lambda$.
Note that $\lambda$ is positive, since
\[
\magn{Av}^2 = v^TA^TAv = \lambda v^Tv
= \lambda\magn{v}^2
\]
and thus
\[
\lambda = \frac{\magn{Av}^2}{\magn{v}^2}.
\]
Now if we let $\sigma = \sqrt{\lambda}$ and $u = \frac{Av}{\sigma}$. it
is easy to verify that $u$,$v$, and $\sigma$ satisfy the singular
value requirements.

The right singular vectors $\{v_i\}$ are thus exactly equal to the
eigenvectors of $A^TA$.  Since $A^TA$ is a real, symmetric matrix, it
has $n$ orthonormal  eigenvectors, which we can label $v_1,...,v_n$.
Expressing a unit vector $v$ in terms of $\{v_i\}$ (i.e. $v = \sum_i
\alpha_i v_i$ where $\sum_i \alpha_i^2 = 1$), we see that $\magn{Av}^2
= \sum_i \sigma_i^2 \alpha_i^2$ which is maximized exactly when $v$ corresponds
to the top eigenvector of $A^TA$. If the top eigenvalue has multiplicity greater than $1$,
then $v$ should belong to the space spanned by the top eigenvectors.
\end{proof}

More generally, we consider a $k$-dimensional subspace that best fits the
data. It turns out that this space is specified by the top $k$ singular
vectors, as stated precisely in the following proposition.

\begin{theorem}\label{thm:SVD}
Define the $k$-dimensional subspace $V_k$ as the span of the following $k$ vectors:
\begin{eqnarray*}
v_1 &=& \arg\max_{\|v\| = 1} \|Av\| \\
v_2 &=& \arg\max_{\|v\| = 1, v\cdot v_1 = 0} \|Av\| \\
&\vdots&\\
v_k &=& \arg\max_{\|v\| = 1, v\cdot v_i = 0 \mbox{ }\forall i < k} \|Av\|,  \\
\end{eqnarray*}
where ties for any $\arg\max$ are broken arbitrarily.
Then $V_k$ is \emph{optimal} in the sense that
\[
V_k = \arg\min_{dim(V) = k}\sum_i d(A_{(i)},V)^2.
\]
Further, $v_1,v_2,...,v_n$ are all singular vectors, with corresponding
singular values $\sigma_1,\sigma_2,...,\sigma_n$ and
\[
\sigma_1 = \|Av_1\| \ge \sigma_2=\|Av_2\| \ge ... \ge \sigma_n = \|Av_n\|.
\]
Finally, $A =\sum_{i=1}^n \sigma_i u_i v_i^T$.
\end{theorem}

Such a decomposition where,
\begin{enumerate}
\item The sequence of $\sigma_i$'s is nonincreasing
\item The sets $\{u_i\},\{v_i\}$ are orthonormal
\end{enumerate}
is called the \emph{Singular Value Decomposition (SVD)} of $A$.

\begin{proof}
We first prove that $V_k$ are optimal by induction on $k$. The case $k=1$ is by definition. Assume that $V_{k-1}$ is optimal.

Suppose $V_k'$ is an optimal subspace of dimension $k$. Then we can
choose an orthonormal basis for $V_k'$, say  $w_1,w_2,\ldots w_k$, such
that $w_k$ is orthogonal to $V_{k-1}$. By the definition of
$V_k'$, we have that
$$||Aw_1||^2+||Aw_2^2||+\ldots ||Aw_k||^2$$
is maximized (among all sets of $k$ orthonormal vectors.) If
we replace $w_i$ by $v_i$ for $i=1,2,\ldots,k-1$, we have
\[
\|Aw_1\|^2+\|Aw_2^2\|+\ldots \|Aw_k\|^2 \le \|Av_1\|^2+\ldots+\|Av_{k-1}\|^2 + \|Aw_k\|^2.
 \]
Therefore we can assume that $V_k'$ is the span of $V_{k-1}$ and $w_k$. It then follows that $\|Aw_k\|^2$ maximizes $\|Ax\|^2$ over all unit vectors $x$ orthogonal to $V_{k-1}$.

Proposition \ref{topsingularvector}
can be extended to show that $v_1,v_2,...,v_n$
are all singular vectors.  The assertion that $\sigma_1 \ge \sigma_2
\ge .... \ge \sigma_n \ge 0$ follows from the definition of the
$v_i$'s.

We can verify that the decomposition
\[
A = \sum_{i=1}^n\sigma_i u_i v_i^T
\]
is accurate.  This is because the
vectors $v_1,v_2,...,v_n$ form an orthonormal basis for $\R^n$, and
the action of $A$ on any $v_i$ is equivalent to the action of
$\sum_{i=1}^n \sigma_i u_i v_i^T$ on $v_i$.
\end{proof}

Note that we could
actually decompose $A$ into the form $\sum_{i=1}^n \sigma_i u_i
v_i^T$ by picking $\{v_i\}$ to be any orthogonal basis of $\R_n$, but
the proposition actually states something stronger: that we can pick
$\{v_i\}$ in such a way that $\{u_i\}$ is also an orthogonal set.

We state one more classical theorem.  We have seen that the span of the top
$k$ singular vectors is the best-fit $k$-dimensional subspace for
the rows of $A$.  Along the same lines, the partial
decomposition of $A$ obtained by using only the top $k$ singular
vectors is the best rank-$k$ matrix approximation to $A$.

\begin{theorem}\label{svdFrob}
Among all rank $k$ matrices $D$, the matrix $A_k = \sum_{i=1}^k
\sigma_i u_i v_i^T $ is the one which minimizes
$\|A-D \|_F^2 = \sum_{i,j} (A_{ij} - D_{ij})^2$. Further,
\[
\|A - A_k\|_F^2 = \sum_{i=k+1}^n \sigma_i^2.
\]
\end{theorem}

\begin{proof}
We have
\[
\|A-D \|_F^2 = \sum_{i=1}^m \|A_{(i)} - D_{(i)}\|^2.
\]
Since $D$ is of rank at most $k$, we can assume that all the $D_{(i)}$ are
projections of $A_{(i)}$ to some rank $k$ subspace and therefore,
\begin{eqnarray*}
\sum_{i=1}^m \|A_{(i)} - D_{(i)}\|^2 &=& \sum_{i=1}^m\|A_{(i)}\|^2 - \|D_{(i)}\|^2\\
&=& \|A\|_F^2 - \sum_{i=1}^m \|D_{(i)}\|^2.
\end{eqnarray*}
Thus the subspace is exactly the SVD subspace given by the span of the first $k$ singular vectors of $A$.
\end{proof}

\section{Algorithms for computing the SVD}

Computing the SVD is a major topic of numerical analysis \cite{Strang, GolubVanLoan, Wilkinson}.
Here we describe a basic algorithm called the power method.

Assume that $A$ is symmetric.
\begin{enumerate}
\item Let $x$ be a random unit vector.
\item Repeat:
\[
x:= \frac{Ax}{\|Ax\|}
\]
\end{enumerate}
For a nonsymmetric matrix $A$, we can simply apply the power iteration to $A^TA$.

\begin{exercise}
Show that the power iteration applied $k$ times to a symmetric matrix $A$ finds a vector $x^k$ such that
\[
\E\left(\|Ax^k\|^2\right) \ge \left(\frac{1}{n}\right)^{1/k} \sigma_1^2(A).
\]
[Hint: First show that $\|Ax^k\| \ge (|x \cdot v|)^{1/k}\sigma_1(A)$ where $x$ is the starting vector and $v$ is the top eigenvector of $A$; then show that for a random unit vector $x$, $\E((x\cdot v)^2)=1/n$].
\end{exercise}

The second part of this book deals with faster, sampling-based algorithms.

\section{The $k$-means clustering problem}

This section contains a description of a clustering problem which is often called $k$-means in the literature and
can be solved approximately using SVD. This illustrates a
typical use of SVD and has a provable bound.

We are given $m$ points ${\mathcal A}=\{A^{(1)},A^{(2)},\ldots A^{(m)}\}$
in $n$-dimensional Euclidean space and a positive integer $k$.
The problem is to find $k$ points
${\mathcal B}=\{B^{(1)},B^{(2)},\ldots,B^{(k)}\}$ such that
$$f_{{\mathcal A}}({\mathcal B}) = \sum_{i=1}^m (\dist(A^{(i)},{\mathcal B}))^2$$
is minimized. Here $\dist(A^{(i)},{\mathcal B})$
is the Euclidean distance of $A^{(i)}$ to its nearest point in ${\mathcal B}$. Thus,
in this problem we wish to minimize the sum of squared distances to the nearest
``cluster center''.  This is commonly called the $k$-means or $k$-means clustering problem.
It is NP-hard even for $k=2$. A popular local search heuristic for this problem is often called the $k$-means algorithm.  
%(via a reduction from minimum bisection).

We first observe that the solution is given by $k$ clusters $S_j$, $j=1,2,\ldots k$. The
cluster center $B^{(j)}$ will be the centroid
of the points in $S_j$, $j=1,2,\ldots,k$. This is seen from the fact that for any set
${\mathcal S}=\{X^{(1)},X^{(2)},\ldots,X^{(r)}\}$ and any point $B$ we have
\begin{equation}\label{99}
\sum_{i=1}^r \|X^{(i)}-B\|^2=\sum_{i=1}^r \|X^{(i)}-\bar{X}\|^2+r\|B-\bar{X}\|^2,
\end{equation}
where $\bar{X}$ is the centroid $(X^{(1)}+X^{(2)}+\cdots+X^{(r)})/r$ of ${\mathcal S}$. The next exercise makes this clear.

\begin{exercise}
Show that for a set of point $X^1, \ldots, X^k \in \R^n$, the point $Y$ that
minimizes $\sum_{i=1}^k |X^i - Y|^2$ is their centroid. Give an example when the centroid is not the optimal choice if we minimize sum of distances rather than squared distances.
\end{exercise}

The $k$-means clustering 
problem is thus the problem of partitioning a set of points into clusters
so that the {\em sum of the squared distances to the means}, i.e., the variances of the clusters is minimized.

We define a relaxation of this problem that we may cal the {\em Continuous Clustering Problem} (CCP): find the subspace $V$ of ${\bf R}^n$
of dimension at most $k$ that minimizes
$$g_{{\mathcal A}}(V) = \sum_{i=1}^m \dist (A^{(i)},V)^2.$$
The reader will recognize that this can be solved using the SVD.
It is easy to see that the optimal value of the $k$-means clustering problem
 is an upper bound for
the optimal value of the CCP. Indeed for any set ${\mathcal B}$ of $k$ points,
\begin{equation}\label{100}
f_{{\mathcal A}}({\mathcal B})\geq g_{{\mathcal A}}(V_{{\mathcal B}})
\end{equation}
where $V_{{\mathcal B}}$ is the subspace generated by the points in ${\mathcal B}$.

We now present a factor $2$
approximation algorithm for the $k$-means clustering problem
using the relaxation to the best-fit subspace. The algorithm has two parts. First we project to the $k$-dimensional SVD subspace, solving the CCP. Then we solve the problem in the low-dimensional space using a brute-force algorithm with the following guarantee.

\begin{theorem}
The $k$-variance problem can be solved
in $O(m^{k^2d/2})$ time when the input
${\mathcal A}\subseteq {\bf R}^d$.
\end{theorem}

We describe the algorithm for the low-dimensional setting.
Each set ${\mathcal B}$ of ``cluster centers'' defines a Voronoi diagram where cell
${\mathcal C}_i=\{X\in {\bf R}^d:\;|X-B^{(i)}|\leq |X-B^{(j)}|$ for $j\neq i\}$ consists of those points
whose closest point in ${\mathcal B}$ is $B^{(i)}$. Each cell is a polyhedron and the total number of faces in
$C_1,C_2,\ldots,C_k$ is no more than $\binom{k}{2}$ since each face is the set of points equidistant from
two points of ${\mathcal B}$.

We have seen in (\ref{99}) that it is the partition of ${\mathcal A}$ that determines the best ${\mathcal B}$ (via
computation of centroids) and so we can move the boundary hyperplanes of the optimal Voronoi diagram, without any face passing through a
point of ${\mathcal A}$,
so that each face contains at least $d$ points of ${\mathcal A}$.

Assume that the points of ${\mathcal A}$ are in general position and $0\notin {\mathcal A}$ (a simple perturbation
argument deals with the general case). This means that each face now contains $d$ affinely independent
points of ${\mathcal A}$. We ignore the information about which side of each face to place these points and
so we must try all possibilities for each face. This leads to the following enumerative procedure for
solving the $k$-means clustering problem:\\


\begin{center}
\fbox{\parbox{4.7in}{
\begin{minipage}{4.5in}
\begin{tt}
{\bf Algorithm: Voronoi-$k$-means}

\begin{enumerate}
\item Enumerate
all sets of $t$ hyperplanes, such that $k\leq t\leq k(k-1)/2$
hyperplanes, and each hyperplane contains $d$ affinely independent points of ${\mathcal A}$.
The number of sets is at most \[
\sum_{t=k}^{\binom{k}{2}}\binom{\binom{m}{d}}{t}
=O(m^{dk^2/2}).
\]
\item Check that the arrangement defined by these hyperplanes has exactly $k$ cells.
\item Make one of $2^{td}$ choices as to which cell to assign each point of ${\mathcal A}$ which
 lies on a hyperplane
\item This defines a unique partition of ${\mathcal A}$. Find the centroid of each set in the partition and
compute $f_{{\mathcal A}}$.

\end{enumerate}
\end{tt}
\end{minipage}
}}
\end{center}
Now we are ready for the complete algorithm.
As remarked previously, CCP can be solved by Linear Algebra. Indeed, let $V$ be a $k$-dimensional subspace
of ${\bf R}^n$ and $\bar{A}^{(1)},\bar{A}^{(2)},\ldots,\bar{A}^{(m)}$ be the orthogonal projections of
${A}^{(1)},{A}^{(2)},\ldots,{A}^{(m)}$ onto $V$. Let $\bar{A}$ be the $m\times n$ matrix with
rows $\bar{A}^{(1)},\bar{A}^{(2)},\ldots,\bar{A}^{(m)}$. Thus $\bar{A}$ has rank at most $k$
and
$$\|A-\bar{A}\|_F^2=\sum_{i=1}^m |A^{(i)}-\bar{A}^{(i)}|^2=
\sum_{i=1}^m (\dist (A^{(i)},V))^2.$$
Thus to solve CCP, all we have to do is find the first $k$ vectors of the
SVD of $A$ (since by Theorem (\ref{svdFrob}), these
minimize $\|A-\bar{{\bf A}}\|_F^2$ over all rank $k$ matrices $\bar {A}$)
and take the space $V_{SVD}$ spanned
by the first $k$ singular vectors in the row space of $A$.

We now show that combining SVD with the above algorithm gives a $2$-approximation to the $k$-variance problem in arbitrary dimension.
Let $\bar{{\mathcal A}}=\{\bar{A}^{(1)},\bar{A}^{(2)},\ldots,\bar{A}^{(m)}\}$ be the projection of ${\mathcal A}$
onto the subspace $V_{k}$. Let $\bar{{\mathcal B}}=\{\bar{B}^{(1)},\bar{B}^{(2)},\ldots,\bar{B}^{(k)}\}$
be the optimal solution to $k$-variance problem with input $\bar{{\mathcal A}}$.

{\bf Algorithm for the $k$-means clustering problem}
\begin{itemize}
\item Compute $V_{k}$.
\item Solve the $k$-means clustering problem
with input $\bar{{\mathcal A}}$ to obtain $\bar{{\mathcal B}}$.
\item Output $\bar{{\mathcal B}}$.
\end{itemize}
It follows from (\ref{100}) that the optimal value $Z_{{\mathcal A}}$
of the $k$-means clustering problem satisfies
\begin{equation}\label{101}
Z_{{\mathcal A}}\geq \sum_{i=1}^m|A^{(i)}-\bar{A}^{(i)}|^2.
\end{equation}
Note also that if $\hat{{\mathcal B}}=\{\hat{B}^{(1)},\hat{B}^{(2)},
 \ldots,\hat{B}^{(k)}\}$ is an optimal
solution to the $k$-means clustering problem
and $\tilde{{\mathcal B}}$ consists of the projection of the points in
$\hat{{\mathcal B}}$ onto $V$, then
$$
Z_{{\mathcal A}}= \sum_{i=1}^m\dist(A^{(i)},\hat{{\mathcal B}})^2
\geq \sum_{i=1}^m\dist(\bar{A}^{(i)},\tilde{{\mathcal B}})^2
\geq \sum_{i=1}^m\dist(\bar{A}^{(i)},\bar{{\mathcal B}})^2.
$$
Combining this with (\ref{101}) we get
\begin{eqnarray*}
2Z_{{\mathcal A}} & \geq & \sum_{i=1}^m(|A^{(i)}-\bar{A}^{(i)}|^2+\dist(\bar{A}^{(i)},\bar{{\mathcal B}})^2)\\
& = &\sum_{i=1}^m\dist({A}^{(i)},\bar{{\mathcal B}})^2\\
& = &f_{{\mathcal A}}(\bar{{\mathcal B}})
\end{eqnarray*}
proving that we do indeed get a $2$-approximation.

\begin{theorem}
The above algorithm for the $k$-means clustering problem finds a factor $2$ approximation for $m$ points in $\R^n$ in $O(mn^2 + m^{k^3/2})$ time.
\end{theorem}

\section{Discussion}

%%ADD: Some discussion of related work.

In this chapter, we reviewed basic concepts in linear algebra from a geometric perspective. The $k$-variance problem is a typical example of how SVD is used: project to the SVD subspace, then solve the original problem. In many application areas, the method known as ``Principal Component Analysis" (PCA) uses the projection of a data matrix to the span of the largest singular vectors. There are several introducing the theory of eigenvalues and eigenvectors as well as SVD/PCA, e.g., \cite{GolubVanLoan, Strang, bhatia}.

The application of SVD to the $k$-means clustering problem is from \cite{DFKVV04} and its hardness is from \cite{DP09}.
The following complexity questions are open:
(1) Given a matrix $A$, is it NP-hard to find a rank-$k$ matrix $D$ that minimizes the error with respect to the $L_1$ norm, i.e., $\sum_{i,j} |A_{ij} - D_{ij}|$? (more generally for $L_p$ norm for $p \neq 2$)?
(2) Given a set of $m$ points in $\R^n$, is it NP-hard to find a subspace of dimension at most $k$ that minimizes the sum of distances of the points to the subspace?
It is known that finding a subspace that minimizes the maximum distance is NP-hard \cite{Megiddo1982}; see also \cite{HV02}.

% need to add references on $k$-means clustering, PTAS and best-known LP-based constant-factor approximation. 
% include a reference to Ostrovski, other stuff ???}

\bibliographystyle{amsalpha}
\bibliography{spectralbook}

\chapter{Mixture Models}\label{chap:mixtures}

This chapter is the first of three motivated by clustering problems. Here we study the setting where the input is a set of points in $\R^n$ drawn randomly from a mixture of probability distributions. The sample points are unlabeled and the basic problem is to correctly classify them according the component distribution which generated them. The special case when the component distributions are Gaussians is a classical problem and has been widely studied. In the next chapter, we move to discrete probability distributions, namely random graphs from some natural classes of distributions. In Chapter \ref{chap:kvv}, we consider worst-case inputs and derive approximation guarantees for spectral clustering.

Let $F$ be a probability distribution in $\R^n$ with the property that
 it is a convex combination of
 distributions of known type, i.e., we can decompose $F$ as
$$F = w_1F_1 + w_2F_2 + ... + w_kF_k$$
where each $F_i$ is a probability distribution with mixing weight $w_i
\ge 0$, and $\sum_i{w_i} = 1$.  A random point from $F$ is drawn
from distribution $F_i$ with probability $w_i$.

Given a sample of points from $F$, we consider the following problems:
\begin{enumerate}
\item Classify the sample according to the component distributions.
\item Learn the component distributions
(find their means, covariances, etc.).
\end{enumerate}

For most of this chapter, we deal with the classical setting:
each $F_i$ is a Gaussian in $\R^n$.  In fact, we begin with the special case of
spherical Gaussians whose density functions (i) depend only on
the distance of a point from the mean and (ii)
can be written as the product of density functions on
each coordinate. The density function of a spherical Gaussian in $\R^n$ is
\[
p(x) = \frac{1}{(\sqrt{2\pi}\sigma)^n}e^{-\|x-\mu\|^2/2\sigma^2}
\]
where $\mu$ is its mean and $\sigma$ is the standard deviation along any direction.

%\begin{exercise}
%Are Gaussians the only functions that satisfy the above properties?
%\end{exercise}

%We use the notation below for the density, mean and variance along any %direction.

%\begin{eqnarray*}
%p(x) &=& \frac{1}{(\sqrt{2}\pi\sigma)^n} \cdot e^\frac{-\|x-\mu\|^2}{2\sigma^2} %\\
%E[X] &=& \mu \\
%\forall v \in \R^n, \|v\|=1: \quad E[((X-\mu)\cdot v)^2] &=& \sigma^2 \\
%\end{eqnarray*}

%This last property states that the variance of $X$ along \emph{any}
%direction $v$ is equal to $\sigma^2$.

If the component distributions are far apart, so that points from one component distribution
are closer to each other than to points from other components, then classification
is straightforward. In the case of spherical Gaussians, making the means sufficiently
far apart achieves this setting with high probability. On the other hand, if the
component
distributions have large overlap, then for a large fraction of the mixture, it is impossible
to determine the origin of sample points. Thus, the classification problem is inherently tied to
some assumption on the separability of the component distributions.

\section{Probabilistic separation}

In order to correctly identify sample points, we require a small
overlap of distributions.  How can we quantify the distance between
distributions?  One way, if we only have two distributions, is to take
the total variation distance,
\[
d_{TV}(f_1,f_2) = \frac{1}{2}\int_{\R^n} |f_1(x)-f_2(x)|\, dx.
\]
We can require this to be large for two well-separated distributions, i.e.,
$d_{TV}(f_1,f_2) \ge 1-\eps$, if we tolerate $\eps$ error. We can incorporate
mixing weights in this condition, allowing for two components to overlap more if the mixing weight of one of them is small:
\[
d_{TV}(f_1, f_2) = \int_{\R^n} |w_1f_1(x)- w_2f_2(x)| \, dx \ge 1-\eps.
\]

This can be generalized in two ways to $k > 2$ components. First, we could require the above condition holds for every pair of components, i.e., pairwise probabilistic separation. Or we could have the following single condition.
\begin{equation}\label{k-TV}
\int_{\R^n} \left(2\max_i w_if_i(x)- \sum_{i=1}^k w_if_i(x)\right)^+ \, dx \ge 1-\eps.
\end{equation}
The quantity inside the integral is simply the maximum $w_if_i$ at $x$,
minus the sum of the rest of the $w_if_i$'s. If the supports of the components are essentially disjoint, the integral will be $1$.

For $k > 2$, it is not known how to efficiently classify mixtures when we are given one of these probabilistic separations.  In what follows, we use stronger assumptions.

\section{Geometric separation}

Here we assume some separation between the means of component distributions.
For two distributions, we require $\|\mu_1-\mu_2\|$ to be large compared to
$\max\{\sigma_1,\sigma_2\}$.
Note this is a
stronger assumption than that of small overlap.  In fact, two
distributions can have the \emph{same} mean, yet still have small
overlap, e.g., two spherical Gaussians with different variances.

\ignore{
\begin{figure}[htb]
\begin{center}
%\input{fig2.pictex}
\end{center}
\caption{Two distributions with the same mean, but small overlap.}
\end{figure}
}

Given a separation between the means, we expect that sample points
originating from the same component distribution will have smaller
pairwise distances than points originating from different
distributions. Let $X$ and $Y$ be two independent samples drawn from the
same $F_i$.

\begin{eqnarray*}
\E\left(\magn{X-Y}^2\right)  &=& \E\left(\magn{(X-\mu_i) - (Y-\mu_i)}^2\right) \\
 &=& 2\E\left(\magn{X-\mu_i}^2\right) - 2\E\left((X-\mu_i)(Y-\mu_i)\right) \\
 &=& 2\E\left(\magn{X-\mu_i}^2\right) \\
 &=& 2\E\left(\sum_{j=1}^n |x^j-\mu_i^j|^2\right) \\
 &=& 2n\sigma_i^2 \\
\end{eqnarray*}

Next let $X$ be a sample drawn from $F_i$ and $Y$ a sample from $F_j$.

\begin{eqnarray*}
\E\left(\magn{X-Y}^2\right)  &=& \E\left(\magn{(X-\mu_i) - (Y-\mu_j) + (\mu_i-\mu_j)}^2\right) \\
&=& \E\left(\magn{X-\mu_i}^2\right) + \E\left(\magn{Y-\mu_j}^2\right) + \magn{\mu_i-\mu_j}^2 \\
&=&  n\sigma_i^2 + n\sigma_j^2 + \magn{\mu_i-\mu_j}^2 \\
\end{eqnarray*}

Note how this value compares to the previous one.
If $\magn{\mu_i-\mu_j}^2$ were large enough, points in the component with smallest variance would all be closer to each other than to any point from the other components. This suggests that we can compute pairwise distances in our sample and use them to identify the subsample from the smallest component.

We consider separation of the form
\begin{equation}\label{geom-sep}
\magn{\mu_i-\mu_j} \ge \beta \max\{\sigma_i, \sigma_j\},
\end{equation}
between every pair of means $\mu_i, \mu_j$.
For $\beta$ large enough, the distance between points from different
components
will be larger in expectation than that between points from the same component.
This suggests the following classification algorithm: we
compute the distances between every pair of points, and
connect those points whose distance is less than some threshold.  The
threshold is chosen to split the graph into two (or $k$) cliques.
Alternatively, we can compute a minimum spanning tree of
the graph (with edge weights equal to distances between points), and
drop the heaviest edge ($k-1$ edges) so that the graph has two ($k$) connected components and each corresponds to a component distribution.

Both algorithms use only the pairwise distances.
In order for any algorithm of this form to work, we need to turn the
above arguments about expected distance between sample points into high probability
bounds. For Gaussians, we can use the following concentration bound.

\begin{lemma}\label{lem:spherical-conc}
Let $X$ be drawn from a spherical Gaussian in $\R^n$ with mean $\mu$ and variance $\sigma^2$ along any direction. Then for any $\alpha > 1$,
$$\Pr\left(|\magn{X-\mu}^2 -\sigma^2n | > \alpha\sigma^2\sqrt{n}\right) \le
2e^{-\alpha^2/8}.$$
\end{lemma}

Using this lemma with $\alpha = 4\sqrt{\ln (m/\delta)}$, to a random point $X$ from component $i$, we have
\[
\Pr(|\|X-\mu_i\|^2 - n\sigma_i^2| > 4\sqrt{n\ln (m/\delta)}\sigma^2)
\le 2\frac{\delta^2}{m^2} \le \frac{\delta}{m}
\]
for $m > 2$. Thus the inequality
\[
|\|X-\mu_i\|^2 - n\sigma_i^2| \le 4\sqrt{n\ln (m/\delta)}\sigma^2
\]
holds for all $m$ sample points with probability at least $1-\delta$.
From this it follows that with probability at least $1-\delta$, for $X,Y$ from the $i$'th and $j$'th Gaussians respectively,
with $i\not= j$,
\begin{eqnarray*}
\|X-\mu_i\|&\leq& \sqrt{ \sigma_i^2n+\alpha^2\sigma_i^2\sqrt n} \leq \sigma_i\sqrt n+\alpha^2\sigma_i\\
\|Y-\mu_j\|&\leq& \sigma_j\sqrt n+\alpha^2\sigma_j \\
\|\mu_i-\mu_j\|-\|X-\mu_i\|-\|Y-\mu_j\|&\leq& \|X-Y\| \leq \|X-\mu_i\|+\|Y-\mu_j\|+\|\mu_i-\mu_j\|\\
\|\mu_i-\mu_j\|-(\sigma_i+\sigma_j)(\alpha^2+\sqrt n)&\leq& \|X-Y\|\leq \|\mu_i-\mu_j\|+
(\sigma_i+\sigma_j)(\alpha^2+\sqrt n)
\end{eqnarray*}

%\[
%n(\sigma_i^2 + \sigma_j^2) - 4\sqrt{n\ln \left(\frac{m}{\delta}\right)}(\sigma_i^2+\sigma_j^2)\le \|X-Y\|^2 \le n(\sigma_i^2 + %\sigma_j^2)
%+ 4\sqrt{n\ln \left(\frac{m}{\delta}\right)}(\sigma_i^2+\sigma_j^2)
%\]

Thus it suffices for $\beta$ in the separation bound (\ref{geom-sep}) to grow
as $\Omega(\sqrt{n})$ for either of the above algorithms (clique or MST). One can be more careful and get a bound that grows only as $\Omega(n^{1/4})$ by identifying components in the order of increasing $\sigma_i$.
We do not describe this here.

The problem with these approaches is that the separation needed
grows rapidly with $n$, the dimension, which in general is much higher than
$k$, the number of components. On the other hand, for classification to be achievable with high probability, the separation does not need a dependence on $n$. In particular, it suffices for the means to be separated by a small number of standard deviations. If such a separation holds, the projection of the mixture to the span of the means
would still give a well-separate mixture and now the dimension is at most $k$. Of course, this is not an
algorithm since the means are unknown.

One way to reduce the dimension and therefore the dependence on $n$ is to project
to a lower-dimensional subspace. A natural idea is random
projection.  Consider a projection from $\R^n
\rightarrow \R^\ell$ so that the image of a point $u$ is $u'$.
Then it can be shown that

$$\E\left(\magn{u'}^2\right) = \frac{\ell}{n}\magn{u}^2$$

In other words, the expected squared length of a vector shrinks by a factor of $\frac{\ell}{n}$. Further, the squared length is concentrated around its expectation.

$$\Pr(|\magn{u'}^2 - \frac{\ell}{n}\magn{u}^2| > \frac{\epsilon
\ell}{n}\magn{u}^2) \le 2e^{-\epsilon^2\ell/4}$$

%In order to insure that distances don't change much with high probability after
%projection, we want the right-hand side to be less than $\frac{1}{n^2}$.  Thus it
%suffices to set $\ell = O(\frac{\ln n}{\epsilon^2})$.

The problem with random projection is that the squared distance between the
means, $\magn{\mu_i-\mu_j}^2$, is also likely to shrink by the same
$\frac{\ell}{n}$ factor, and therefore random
projection acts only as a scaling and provides no benefit.

\section{Spectral Projection}

Next we consider projecting to the
{\em best-fit} subspace given by the top $k$ singular vectors of the mixture.
This is a general methodology --- use principal component analysis (PCA) as a preprocessing step. In this case, it will be
provably of great value.

\begin{center}
\fbox{\parbox{4.7in}{
\begin{minipage}{4.5in}
\begin{tt}
{\bf Algorithm: Classify-Mixture}

\begin{enumerate}
\item[1.] Compute the singular value decomposition of the sample
  matrix.

\item[2.] Project the samples to the rank $k$ subspace spanned by the
  top $k$ right singular vectors.

\item[3.] Perform a distance-based classification in the $k$-dimensional space.

%% \item[4.] Report the mean vectors and variances of each subsample.

\end{enumerate}
\end{tt}
\end{minipage}
}}
\end{center}

We will see that by doing this, a separation given by
\[
\magn{\mu_i-\mu_j} \ge c(k\log m)^{\frac{1}{4}}\max\{\sigma_i,\sigma_j\},
\]
where $c$ is an absolute constant, is sufficient for classifying $m$ points.

The best-fit vector for a \emph{distribution}
is one that minimizes the expected squared distance of a random point to the vector.  Using this definition, it is intuitive that
the best fit vector for a single Gaussian is simply the vector that
passes through the Gaussian's mean. We state this formally below.

\begin{lemma}
The best-fit 1-dimensional subspace for a spherical Gaussian with mean $\mu$ is given by the vector passing through $\mu$.
\end{lemma}

\begin{proof}
For a randomly chosen $x$, we have for any unit vector $v$,
\begin{eqnarray*}
\E\left((x\cdot v)^2\right) &=& \E\left(((x-\mu)\cdot v + \mu \cdot v)^2\right) \\
&=& \E\left(((x-\mu)\cdot v)^2\right) + \E\left((\mu \cdot v)^2\right)  + \E\left(2((x-\mu)\cdot v)(\mu \cdot v)\right) \\
&=& \sigma^2 + (\mu \cdot v)^2 + 0 \\
&=& \sigma^2 + (\mu \cdot v)^2 \\
\end{eqnarray*}
which is maximized when $v=\mu/\|\mu\|$.
\end{proof}

Further, due to the symmetry of the sphere,
the best subspace of dimension $2$ or more is
$\emph{any}$ subspace containing the mean.

%{\color{red} Make Lemma a little more precise... ``The set of possible $k$
%dimensional spaces ...''}.
\begin{lemma}
Any $k$-dimensional subspace containing $\mu$ is an optimal SVD subspace for a spherical Gaussian.
\end{lemma}

A simple consequence of this lemma is the following theorem, which
states that the best $k$-dimensional subspace for a mixture $F$
involving $k$ spherical Gaussians is the space which contains the means of the
Gaussians.

\begin{theorem}\label{thm:svdspansmeans}
The $k$-dim SVD subspace for a mixture of $k$ Gaussians $F$ contains the span of $\{\mu_1,\mu_2,...,\mu_k\}$.
\end{theorem}

Now let $F$ be a mixture of two Gaussians.  Consider what
happens when we project from $\R^n$ onto the best two-dimensional
subspace $\R^2$.  The expected squared distance (after projection) of two
points drawn from the same distribution goes from $2n\sigma_i^2$ to
$4\sigma_i^2$.  And, crucially, since we are projecting onto the best
two-dimensional subspace which contains the two means, the expected
value of $\magn{\mu_1-\mu_2}^2$ does not change!

What property of spherical Gaussians did we use in this analysis?
A spherical Gaussian projected onto the best SVD subspace is
still a spherical Gaussian.  In fact, this only required that the
variance in every direction is equal.
But many other distributions, e.g., uniform over a cube, also have
this property. We address the following questions in the rest of this
chapter.
\begin{enumerate}
\item What distributions does Theorem~\ref{thm:svdspansmeans}
extend to?
\item What about more general distributions?
\item What is the sample complexity?
\end{enumerate}

\section{Weakly Isotropic Distributions}

Next we study how our characterization of the SVD subspace can be extended.

\begin{definition}
Random variable $X\in\mathbb R^n$ has a {\em weakly isotropic} distribution with mean $\mu$ and
variance $\sigma^2$
if
\[\E{(w\cdot (X-\mu))^2}=\sigma^2,\quad\forall w\in\mathbb R^n,~\|w\|=1.\]
\end{definition}

 A spherical Gaussian is clearly weakly isotropic. The uniform distribution in a cube is also weakly isotropic.

\begin{exercise}
Show that the uniform distribution in a cube is weakly isotropic.
\end{exercise}

\begin{exercise}
Show that a distribution is weakly isotropic iff its covariance matrix is a multiple
of the identity.
\end{exercise}

\begin{exercise}\label{theorem:svd.isotropic}
The $k$-dimensional SVD subspace for a mixture $F$ with component means $\mu_1, \ldots, \mu_k$ contains
$\span\{\mu_1,\dots,\mu_k\}$ if each $F_i$ is weakly isotropic.
\end{exercise}

The statement of Exercise~\ref{theorem:svd.isotropic} does not hold for arbitrary distributions, even for $k=1$. Consider a non-spherical Gaussian random vector $X\in\mathbb R^2$, whose mean is $(0,1)$ and whose variance along the $x$-axis is much larger than that along the $y$-axis. Clearly the optimal $1$-dimensional subspace for $X$ (that maximizes the squared projection in expectation) is not the one passes through its mean $\mu$; it is orthogonal to the mean. SVD applied after centering the mixture at the origin works for one Gaussian but breaks down for $k > 1$, even with (nonspherical) Gaussian components.

\ignore{
\begin{figure}[htb]
\begin{center}
%\input{nonspherical}
\caption{\small For a non-spherical Gaussian, the subspace containing the mean is not the best subspace.}
\label{fig:non.spherical}
\end{center}
\end{figure}
}

\section{Mixtures of general distributions}\label{sec:genmix}
For a mixture of general distributions, the subspace that maximizes the squared projections is not the best subspace for our classification purpose any more. Consider two components that resemble ``parallel pancakes'', i.e., two Gaussians that are narrow and separated along one direction and spherical (and identical) in all other directions. They are separable by a hyperplane orthogonal to the line joining their means. However, the 2-dimensional subspace that maximizes the sum of squared projections (and hence minimizes the sum of squared distances)
is parallel to the two pancakes. Hence after projection to this subspace, the two means collapse and we can not separate
the two distributions anymore.

\ignore{
\begin{figure}[htb]
\begin{center}
%\input{pancakes}
\caption{\small Two distributions that are collapsed by spectral projection.}
\label{fig:pancakes}
\end{center}
\end{figure}
}

%Let $\sigma_i^2$ denote the maximum variance of distribution $F_i$ among all %directions: $\sigma_i^2 = \max_{\|v\|=1} {\sf E}_{F_i}[((X-\mu_i)\cdot v)^2]$. %We only need the separation of means to be:
%\[\|\mu_i-\mu_j\|^2\ge c\cdot \max\{\sigma_i^2,\sigma_j^2\}.\]

The next theorem provides an extension of the analysis of spherical Gaussians by showing when the SVD subspace is ``close" to the subspace spanned by the component means.

\begin{theorem}\label{thm:genmix}
Let $F$ be a mixture of arbitrary distributions $F_1,\ldots,F_k$. Let $w_i$ be the mixing weight
of $F_i$, $\mu_i$ be its mean and $\sigma^2_{i,W}$ be the maximum variance of $F_i$ along directions in $W$, the $k$-dimensional SVD-subspace of $F$. Then
\[\sum_{i=1}^k w_i d(\mu_i,W)^2 \le k \sum_{i=1}^k w_i \sigma^2_{i,W}\]
where $d(.,.)$ is the orthogonal distance.
\end{theorem}

 Theorem~\ref{thm:genmix} says that for a mixture of general distributions, the means do not move too much after projection
 to the SVD subspace. Note that the theorem does not solve the case of parallel pancakes, as it requires that the pancakes be separated by a factor proportional to their ``radius" rather than their ``thickness".

%The theorem is true for any mixture, thus is true for samples, with distribution means and variances replaced by sample
%means and variances.

\begin{proof}
Let $M$ be the span of $\mu_1,\mu_2,\ldots,\mu_k$.
For $x\in \R^n$, we write $\pi_M(x)$ for the projection of
$x$ to the subspace $M$ and $\pi_W(x)$ for the projection of $x$ to $W$.

We first lower bound the expected squared length of the projection to the mean subpspace $M$.
\begin{eqnarray*}
\E\left(\|\pi_M(x)\|^2\right) &=& \sum_{i=1}^k w_i\E_{F_i}\left(\|\pi_M(x)\|^2\right)\\
&=& \sum_{i=1}^k w_i\left(\E_{F_i}\left(\|\pi_M(x)- \mu_i\|^2\right)
                           +\|\mu_i\|^2\right)\\
   &\geq& \sum_{i=1}^k w_i\|\mu_i\|^2\\
&=& \sum_{i=1}^k w_i \|\pi_W(\mu_i)\|^2 +
            \sum_{i=1}^k w_i d(\mu_i,W)^2.
\end{eqnarray*}

We next upper bound the expected squared length of the projection to the SVD subspace $W$.
Let $\vec e_1,...,\vec e_k$ be an orthonormal basis for $W$.

\begin{eqnarray*}
\E\left(\|\pi_W(x)\|^2\right) &=&\sum_{i=1}^k w_i\left( \E_{F_i}\left(\|\pi_W(x-\mu_i)\|^2\right)
                                 +\|\pi_W(\mu_i)\|^2\right)\\
&\leq&\sum_{i=1}^kw_i\sum_{j=1}^k\E_{F_i}\left((\pi_W(x-\mu_i)\cdot \vec e_j)^2\right)+
 \sum_{i=1}^k w_i\|\pi_W(\mu_i)\|^2\\
&\leq& k\sum_{i=1}^k w_i\sigma_{i,W}^2 +
    \sum_{i=1}^k w_i\|\pi_W(\mu_i)\|^2.
\end{eqnarray*}
The SVD subspace maximizes the sum of squared projections
among all subspaces of rank at most $k$ (Theorem \ref{thm:SVD}).
Therefore,
\[
\E\left(\|\pi_M(x)\|^2\right) \le \E\left(\|\pi_W(x)\|^2\right)
\]
and the theorem follows from the previous two inequalities.
\end{proof}

The next exercise gives a refinement of this theorem.
\begin{exercise}\label{ex:AM}
Let $S$ be a matrix whose rows are a sample of $m$ points from a mixture of $k$ distributions with $m_i$ points from the $i$'th distribution. Let $\bar{\mu}_i$ be the mean of the subsample from the $i$'th distribution and
$\bar{\sigma}_i^2$ be its largest directional variance. Let $W$ be the $k$-dimensional
SVD subspace of $S$.
\begin{enumerate}
\item Prove that
\[
\|\bar{\mu}_i - \pi_W(\bar{\mu}_i)\| \le \frac{\|S-\pi_W(S)\|}{\sqrt{m_i}}
\]
where the norm on the RHS is the 2-norm (largest singular value).
\item Let $\bar{S}$ denote the matrix where each row of $S$ is replaced by the corresponding $\bar{\mu}_i$. Show that (again with 2-norm),
    \[
    \|S-\bar{S}\|^2 \le \sum_{i=1}^k m_i \bar{\sigma}_i^2.
    \]
\item From the above, derive that for each component,
    \[
    \|\bar{\mu}_i - \pi_W(\bar{\mu}_i)\|^2 \le \frac{\sum_{j=1}^k w_j\bar{\sigma}_j^2}{w_i}
    \]
    where $w_i = m_i/m$.
\end{enumerate}
\end{exercise}



\section{Spectral projection with samples}

So far we have shown that the SVD subspace of a mixture can be quite useful for classification. In reality, we only have samples from the mixture. This section is devoted to establishing bounds on
sample complexity to achieve similar guarantees as we would for the full mixture. The main tool will
be distance concentration of samples.
In general, we are interested in inequalities such as the following for a random point $X$ from
a component $F_i$ of the mixture. Let $R^2=\E(\|X-\mu_i\|^2)$.
\[\Pr\left(\|X-\mu_i\| > t R \right)\le e^{-c t}.\]
This is useful for two reasons:
\begin{enumerate}
\item To ensure that the SVD subspace the sample matrix is not far from
the SVD subspace for the full mixture. Since our analysis shows that the SVD subspace is near the subspace spanned by the means and the distance, all we need to show is that the sample means and sample variances converge to the component means and covariances.
\item To be able to apply simple clustering algorithms such as forming cliques or connected components, we need distances between points of the same component to be not much higher than their expectations.
\end{enumerate}

An interesting general class of distributions with such concentration properties are those whose probability density functions are {\em logconcave}. A function $f$ is logconcave if $\forall x,y,~\forall \lambda\in [0,1]$,
\[f(\lambda x + (1-\lambda) y)\ge f(x)^{\lambda} f(y)^{1-\lambda}\]
or equivalently,
\[\log f(\lambda x + (1-\lambda) y)\ge \lambda \log f(x) + (1-\lambda)\log f(y).\]
Many well-known distributions are log-concave.
In fact, any distribution with a density function $f(x)=e^{g(x)}$ for some concave function $g(x)$, e.g. $e^{-c\|x\|}$ or $e^{c(x\cdot v)}$ is logconcave. Also, the uniform distribution in a convex body is logconcave. The following concentration inequality \cite{Lovasz2007} holds for any logconcave density.

\begin{lemma}\label{logcon-conc}
Let $X$ be a random point from a logconcave density in $\R^n$ with $\mu = \E(X)$ and $R^2 = \E(\|X-\mu\|^2)$. Then,
\[
\Pr(\|X-\mu\|^2 \ge tR) \le e^{-t+1}.
\]
\end{lemma}

Putting this all together, we conclude that Algorithm {\em Classify-Mixture}, which projects samples to the SVD subspace and then clusters, works well for mixtures of well-separated distributions with logconcave densities, where the separation required between every pair of means is proportional to the largest standard deviation.

\begin{theorem}
Algorithm {\em Classify-Mixture} correctly classifies a sample of $m$ points from a mixture of $k$ arbitrary logconcave densities $F_1, \ldots, F_k$, with probability at least $1-\delta$, provided for each pair $i,j$ we have
\[
\|\mu_i-\mu_j\| \ge C k^c\log(m/\delta) \max\{\sigma_i, \sigma_j\},
\]
$\mu_i$ is the mean of component $F_i$, $\sigma_i^2$ is its largest variance and $c,C$ are fixed constants.
\end{theorem}

This is essentially the best possible guarantee for the algorithm. However, it is a bit unsatisfactory since an affine transformation, which does not affect probabilistic separation, could easily turn a well-separated mixture into one that is not well-separated.

\section{An affine-invariant algorithm}

The algorithm described here is an application of isotropic PCA, an algorithm discussed in Chapter \ref{chap:extensions}. Unlike the methods we have seen so far, the algorithm is affine-invariant. For $k=2$ components it has
nearly the best possible guarantees for clustering Gaussian mixtures.  For $k > 2$, it requires
that there be a $(k-1)$-dimensional subspace where the \emph{overlap}
of the components is small in every direction. This condition can be stated in terms of the
Fisher discriminant, a quantity commonly used in the field of Pattern
Recognition with labeled data.  The affine
invariance makes it possible to unravel a much larger set of
Gaussian mixtures than had been possible previously. Here we only describe the
case of two components in detail, which contains the key ideas.

The first step of the algorithm is to place the mixture in isotropic
position via an affine
transformation. This has the effect of making the $(k-1)$-dimensional
Fisher subspace, i.e., the one that minimizes the Fisher discriminant (the fraction of the variance of the mixture taken up the intra-component term; see Section \ref{sec:isopca-analysis} for a formal definition),
the same as the subspace spanned by the means of the components (they
only coincide in general in isotropic position), for {\em any}
mixture. The rest of the algorithm identifies directions close to this
subspace and uses them to cluster, without access to
labels. Intuitively this is hard since after isotropy, standard PCA/SVD
reveals no additional information. Before presenting the ideas and
guarantees in more detail, we describe relevant related work.


%\begin{figure}
%\begin{center}
%\subfigure[Distance Concentration Separability]{
%\includegraphics[width=2in]{dc-sep}\label{fig:dc-sep}}
%\subfigure[Hyperplane Separability]{
%\includegraphics[width=2in]{h-sep}\label{fig:h-sep}}
%\subfigure[Intermean Hyperplane and Fisher Hyperplane.]{
%\includegraphics[width=2in]{tilt}\label{fig:tilt}}
%\caption{Distance concentration separability
%which depends on the maximum directional variance (a).
%Hyperplane separability, which depends only on the
%variance in the separating direction(b).  For non-isotropic mixtures
%the best separating direction may not be between the means of the
%components(c).}\label{fig:pancakes}
%\end{center}
%\end{figure}


As before, we assume we are given a lower bound $\wmin$ on the minimum mixing
weight and $k$, the number of components.  With high
probability,  Algorithm \textsc{Unravel} returns a hyperplane so that each halfspace encloses almost
all of the probability mass of a single component and almost none of
the other component.

The algorithm has three major components: an initial affine
transformation, a reweighting step, and identification of a direction
close to the Fisher direction. The key
insight is that the reweighting technique will either cause the mean
of the mixture to shift in the intermean subspace, or cause the top
principal component of the second moment matrix to approximate
the intermean direction.  In either case, we obtain a direction along
which we can partition the components.

We first find an affine transformation $W$ which when applied to
$\Mix$ results in an isotropic distribution.  That is, we move the
mean to the origin and apply a linear transformation to make the
covariance matrix the identity. We apply this transformation to a new
set of $m_1$ points $\{x_i\}$ from $\Mix$ and then reweight according
to a spherically symmetric Gaussian $\exp(-\|x\|^2/\wvar)$ for
$\wvar = \Theta(n/\wmin)$.  We then compute the mean $\hat{u}$ and second
moment matrix $\hat{M}$ of the resulting set. After the reweighting, the algorithm chooses either the new mean or the direction of maximum second moment and projects the data onto this direction $h$.

\begin{center}
\fbox{\parbox{4.7in}{
\begin{minipage}{4.5in}
\begin{tt}

{\bf Algorithm Unravel}

Input: Scalar $\wmin > 0$. \\*
Initialization: $P = \mathbb{R}^n$.
\begin{enumerate}
\item (Rescale) Use samples to compute an
affine transformation $W$ that makes the distribution nearly isotropic
(mean zero, identity covariance matrix).

\item (Reweight) For each of $m_1$ samples,
compute a weight $e^{-\|x\|^2/\wvar}$.

\item (Find Separating Direction) Find the mean of the reweighted data
$\hat{\mu}$.  If $\|\hat{\mu}\| > \sqrt{\wmin}/(32 \alpha)$
(where $\alpha > n/\wmin$), let $h = \hat{\mu}$.  Otherwise, find the
covariance matrix $\hat{M}$ of the reweighted points and let $h$ be its
top principal component.

\item (Classify) Project $m_2$ sample
points to $h$ and classify the projection based on distances.
\end{enumerate}
\end{tt}
\end{minipage}
}}
\end{center}

\subsection{Parallel Pancakes}\label{sec:pancakes}
We now discuss the case of parallel pancakes in detail.
Suppose $\Mix$ is a mixture of two spherical Gaussians that are
well-separated, i.e. the intermean distance is large compared to the
standard deviation along any direction.  We consider two cases, one
where the mixing weights are equal and another where they are
imbalanced.

After isotropy is enforced, each component will become thin in the
intermean direction, giving the density the appearance of two
parallel pancakes.  When the mixing weights are equal, the means of
the components will be equally spaced at a distance of $1 -
\phi$ on opposite sides of the origin.  For imbalanced weights, the
origin will still lie on the intermean direction but will be much
closer to the heavier component, while the lighter component will be
much further away.  In both cases, this transformation makes the
variance of the mixture $1$ in every direction, so the principal
components give us no insight into the inter-mean direction.

Consider next the effect of the reweighting on the mean of the
mixture.  For the case of equal mixing weights, symmetry assures that
the mean does not shift at all.  For imbalanced weights, however, the
heavier component, which lies closer to the origin will become heavier
still.  Thus, the reweighted mean shifts toward the mean of the
heavier component, allowing us to detect the intermean direction.

Finally, consider the effect of reweighting on the second moments of
the mixture with equal mixing weights.  Because points closer to the
origin are weighted more, the second moment in every direction is
reduced.  However, in the intermean direction, where part of the
moment is due to the displacement of the component means from the
origin, it shrinks less.  Thus, the direction of maximum second
moment is the intermean direction.

\subsection{Analysis}\label{sec:isopca-analysis}

The algorithm has the following guarantee for a two-Gaussian mixture.

\begin{theorem}\label{thrm:k=2-sep}
Let $\wt_1,\mx_1,\Sx_1$ and $\wt_2,\mx_2,\Sx_2$ define a mixture of
two Gaussians and $\wmin = \min \wt_1,\wt_2$.  There is an absolute constant $C$ such that, if there
exists a direction $v$ such that
\[
|\proj_v (\mu_1 - \mu_2)| \geq C \left(\sqrt{v^T\Sx_1v} + \sqrt{v^T\Sx_2v}\right)
\wmin^{-2} \log ^{1/2}\left( \frac{1}{\wmin \delta} + \frac{1}{\eta}\right),
\]
then with probability $1-\delta$ algorithm \textsc{Unravel} returns
two complementary halfspaces that have error at most $\eta$ using
time and a number of samples that is polynomial in
$n,\wmin^{-1},\log (1/\delta)$.
\end{theorem}
So the separation required between the means is comparable to the
standard deviation in {\em some direction}. This separation condition
of Theorem \ref{thrm:k=2-sep} is affine-invariant and much weaker than
conditions of the form $\| \mu_1 - \mu_2\| \gtrsim
\max\{\sigma_{1,\max}, \sigma_{2,\max}\}$ that came up earlier in the chapter.
%See
%Figure \ref{fig:pancakes}.  The dotted line shows how previous work
%effectively treats every component as spherical.
We note that the separating direction need not be the intermean direction.
%as illustrated in Figure \ref{fig:tilt}.  The dotted line illustrates
%hyperplane induced by the intermean direction, which may be far from
%the optimal separating hyperplane shown by the solid line.

It will be insightful to state this result in terms of the Fisher
discriminant, a standard notion from Pattern Recognition
\cite{Duda2001,Fukunaga1990} that is used with labeled data.  In
words, the Fisher discriminant along direction $p$ is
\[
J(p) = \frac{\mbox{ the intra-component variance in direction $p$}}
{\mbox{the total variance in direction $p$}}
\]
Mathematically, this is expressed as
\[
J(p) = \frac{E\left[\| \proj_p(x - \mx_{\ell(x)})\|^2 \right]}
{E\left[\| \proj_p(x) \|^2 \right]} =
\frac{p^T( \wt_1 \Sx_1 + \wt_2 \Sx_2)p}
{p^T(\wt_1 (\Sx_1 + \mx_1\mx_1^T) + \wt_2 (\Sx_2 + \mx_2\mx_2^T))p}
\]
for $x$ distributed according to a mixture distribution with means
$\mu_i$ and covariance matrices $\Sx_i$.  We use $\ell(x)$ to indicate
the component from which $x$ was drawn.

\begin{theorem}\label{thrm:k=2-fisher}
There is an absolute constant $C$ for which the following holds.
Suppose that $\Mix$ is a mixture of two Gaussians such that there
exists a direction $p$ for which
\[
J(p) \leq C \wmin^3 \log^{-1} \left(\frac{1}{\delta\wmin} +
\frac{1}{\eta}\right).
\]
With probability $1-\delta$, algorithm \textsc{Unravel} returns a
halfspace with error at most $\eta$  using time and sample complexity
polynomial in $n,\wmin^{-1},\log(1/\delta)$.
\end{theorem}

In words, the algorithm successfully unravels arbitrary Gaussians
provided there exists a line along which the expected squared distance of a point to its
component mean is smaller than the expected squared distance to the
overall mean by roughly a $1/\wmin^3$ factor. There is no
dependence on the largest variances of the individual components, and
the dependence on the ambient dimension is logarithmic.  Thus
the addition of extra dimensions, even with large variance, has little impact
on the success of the algorithm. The algorithm and its analysis in terms of the Fisher discriminant have been generalized to $k >2$ \cite{Brubaker2008}.

\section{Discussion}
Mixture models are a classical topic in statistics. Traditional methods such as EM or other local search heuristics can get stuck in local optima or take a long time to converge. Starting with Dasgupta's paper \cite{Dasgupta1999} in 1999, there has been much progress on efficient algorithms with rigorous guarantees \cite{Arora2005,Dasgupta2000}, with Arora and Kannan \cite{Arora2005}  addressing the case of general Gaussians using distance concentration methods. PCA was analyzed in this context by Vempala and Wang \cite{Vempala2004} giving nearly optimal guarantees for mixtures of spherical Gaussians (and weakly isotropic distributions). This was extended to general Gaussians and logconcave densities \cite{Kannan2008,Achlioptas2005} (Exercise \ref{ex:AM} is based on \cite{Achlioptas2005}), although the bounds obtained were far from optimal in that the separation required grows with the largest variance of the components or with the dimension of the underlying space. In 2008, Brubaker and Vempala \cite{Brubaker2008} presented an affine-invariant algorithm that only needs hyperplane separability for two Gaussians and a generalization of this condition for $k > 2$.
A related line of work considers learning symmetric product
distributions, where the coordinates are independent.  Feldman et al
\cite{Feldman2006} have shown that mixtures of axis-aligned Gaussians
can be approximated without any separation assumption at all in time
exponential in $k$.  Chaudhuri and
Rao \cite{Chaudhuri2008a} have given a polynomial-time
algorithm for clustering mixtures of product distributions (axis-aligned Gaussians) under mild separation conditions. A.~Dasgupta et al \cite{Dasgupta2005} and later Chaudhuri and Rao \cite{Chaudhuri2008b} gave algorithms for clustering mixtures of
heavy-tailed distributions.


A more general question is ``agnostic" learning of Gaussians, where we are given samples from an arbitrary distribution and would like to find the best-fit mixture of $k$ Gaussians. This problem naturally accounts for noise and appears to be much more realistic. Brubaker \cite{Brubaker2009a} gave an algorithm that makes progress towards this goal, by allowing a mixture to be corrupted by an $\eps$ fraction of noisy points with $\eps < w_{\min}$, and with nearly the same separation requirements as in Section \ref{sec:genmix}.










\chapter{Probabilistic Spectral Clustering}\label{chap:prob}

We revisit the problem of clustering under a model which assumes that the data
is
generated according to a probability distribution in $\R^n$. One line of work
in this area pertains to mixture models where the components are assumed to have
special
distributions (e.g., Gaussians); in this situation, we saw in Chapter \ref{chap:mixtures} that
spectral methods are useful. Another line of work is based on models of
random graphs.
Typically, a random graph $G$ on $n$
vertices is assumed to be partitioned into $k$ ($k<<n$) unknown parts and
an edge from a vertex in the $r$'th part to
a vertex in the $s$'th part appears with probability $p_{rs}$, where these
could be different for different $r,s$. The problem is to find the hidden
partition
and estimate the unknown $p_{rs}$ values. Denoting by $A$ the
adjacency matrix of the graph, the problem can be stated succinctly: given (one realization of) $A$, find $\E A$ the entry-wise expectation (since $\E A$ contains information on the partition as well as the $p_{rs}$ values).

We may view this as a mixture model. Denote by $A$ the adjacency matrix
of the graph. Each row $A_{(i)}$ is a point (with $0$-$1$ coordinates) in ${\bf
R}^n$
generated from a mixture of $k$ probability distributions, where each component
distribution generates the adjacency vectors of vertices in one part.
It is of interest to cluster when the $p_{rs}$ as well as their differences
are small, i.e., $o(1)$. However, since the rows of $A$ are $0$-$1$
vectors, they are very ``far'' along coordinate directions
(measured in standard deviations, say)
from the means of the distributions.
This is quite different from the case of a
Gaussian (which has a very narrow tail). The fat tail is one of the crucial
properties that makes the planted graph problem
very different from the Gaussian mixture problem. Indeed, the literature often treats
them
as different subareas. In spite of this, as we will see in this chapter,
spectral clustering can be used.

\ignore{
We revisit the problem of Clustering under a model which assumes
that the data is generated according to a probability distribution
on $n-$ space. One line of work in this area pertains to mixture
models where the components are assumed to have special
distributions (like the Gaussian); in this situation, we saw
(Chapter \ref{chap:mixtures}) that spectral clustering can be used. Another line
of work has been carried out extensively and more or less
independently of Gaussian and other mixture models. This carries
the name ``Planted Graph Models''. Here one assumes that the input is
a random graph $G$ on $n$ vertices, where the vertices are
divided into $k$ ($k<<n$) UNKNOWN parts and the edges are put in
independently, but an edge from a vertex in the $r$ th part to a
vertex in the $s$ th part is put in with probability $p_{rs}$,
where the $p_{rs}$ are different for different $r,s$. The problem
is to find the parts and estimate the $p_{rs}$. We may view this
as a mixture model. Denote by $A$ the adjacency matrix of the
graph. Each row $A_{(i)}$ of $A$ is a point (with 0-1
coordinates) in ${\bf R}^n$ generated from a mixture of $k$
probability distributions, where each component distribution
corresponds to adjacency vectors of vertices in one part. It is of
interest to do the clustering when the $p_{rs}$ as well as their
differences are $o(1)$. But since the rows of $A$ are 0-1
vectors, the vectors are very ``far'' along coordinate directions
(measured in standard deviations, say) from the means of the
distributions. [This will be quantified later.] This is completely
different from the case of a Gaussian (which has a very ``thin''
tails). The ``fat tails'' is one of the crucial differences that
makes the planted graph problem very different from the Gaussian
mixtures and indeed the literature treats them as different
sub-areas. But we will see that spectral clustering can be used in
both cases.
}
\section{Full independence and the basic algorithm}

The basic tool which has been used to tackle the fat tails is the assumption of {\em full independence}
which postulates that the edges of the graph are mutually independent random variables. This
is indeed a natural conceptual off-shoot of random graphs. Now, under this assumption, the very rough
outline of the spectral clustering algorithm is as follows: we are given $A$ and wish to find
the generative model $\E A$ which tells us the probabilities $p_{rs}$ (and the parts).
 The matrix $A-\E A$ has random
independent entries each with mean 0. There is a rich theory of random matrices where the generative model satisfies full independence and the following celebrated theorem was first stated qualitatively by the
physicist Wigner.

\begin{theorem}\label{wigner}
Suppose $A$ is a symmetric random matrix with independent
(above-diagonal) entries each with standard deviation at most
$\nu$ and bounded in absolute value by 1. Then, with high
probability, the largest eigenvalue of $A-\E A$ is at most $c\nu
\sqrt n$.\footnote{ We use the convention that $c$ refers to a
constant. For example, the statement $a\leq (cp)^{cp}$ will mean
there exist constants $c_1,c_2$ such that $a\leq (c_1p)^{c_2p}$.}
\end{theorem}

%\begin{remark}
The strength of this Theorem is seen from the fact that each row of $A-\E A$
is of length $O(\nu\sqrt n)$, so the Theorem asserts that the top eigenvalue amounts only
to the length of a constant number of rows; i.e., there is almost no correlation among
the rows (since the top eigenvalue $=\max_{|x|=1}\|(A-\E A)x\|$ and hence the higher
the correlation of the rows in some direction $x$, the higher its value).
%\end{remark}

Thus one gets whp an upper bound on the spectral norm of $A-EA$:
$$\|A-\E A\|\leq c\nu\sqrt n.$$
Now an upper bound on the Frobenius norm $\|A-\E A\|_F$ follows from the following basic lemma that we prove shortly.

\begin{lemma}\label{AMtoF}
Suppose $A,B$ are $m\times n$ matrices with rank$(B)=k$. If $\hat A$ is the best rank $k$
approximation to $A$, then
$$\|\hat A-B\|_F^2\leq 5k\|A-B\|^2.$$
\end{lemma}

We use this with $B=\E A$ and $\nu$ equal to the maximum standard deviation of any
row of $A$ in any direction. We can find the SVD of $A$ to get $\hat A$. By the above, we have that
whp,
$$||\hat A-\E A||_F^2\leq c\nu^2nk$$
Let $\epsilon$ be a positive real $<1/(10k)$. The above implies that for all but a small fraction of the rows,
we find the vectors $(\E A)_{(i)}$ within error $c\nu\sqrt k$; i.e.,
for all but $\epsilon n$ of the rows of $A$,
we have (whp)
$$|\hat A_{(i)}-\E A_{(i)}|\leq c\nu\sqrt{\frac{k}{\epsilon}}.$$
Let $G$ be the set of rows of $A$ satisfying this condition.

Now, we assume a {\bf separation condition} between the centers $\mu_r,\mu_s$ of the component distributions $r\not= s$
(as in the case of Gaussian mixtures):
$$\|\mu_r-\mu_s\|\geq \Delta = 20 c\nu\sqrt{\frac{k}{\epsilon}}.$$
We note that $\Delta$ depends only on $k$ and not on $n$ (recall
that $k<<n$). In general, a point $A_{(i)}$ may be at distance $O(\sqrt n\nu)$ from the center of its distribution which is much larger than $\Delta$.

It follows that
points in $G$ are at distance at most $\Delta/20$ from their correct centers and at least $10$ times this distance from any other center. Thus, each point in $G$ is at distance at most $\Delta/10$ from every other point in $G$ in its own part and at distance at least $\Delta/2$ from each point in $G$ in a different part. We use this to cluster most points correctly as follows:

Pick at random a set of $k$ points from the set of projected rows by picking each one
uniformly at random from among those at distance at least $9c\nu\sqrt{k/\epsilon}$
from the ones already picked. This yields with high probability $k$ good points one each from each cluster, asuming $\epsilon<1/(10k)$. We define $k$ clusters, each consisting of the points at distance at most $\Delta /5$ from each of the $k$ points picked.

After this, all known algorithms resort to a {\bf clean-up} phase
where the wrongly clustered vertices are reclassified correctly. The clean-up phase is
often technically very involved and forces stricter (and awkward) separation conditions.
We give a complete algorithm with a clean-up phase in Section \label{LAcluster}.
The algorithm is based only on linear algebraic assumptions rather than
probabilistic ones.


\ignore{
Here we outline very briefly a possible clean-up procedure.

To correctly cluster a particular $i$ : Let $B$ be the matrix obtained from $A$
with row $i$ deleted. [We will see that this is to avoid any conditioning.] As
above, we have with high probability
$$||B - \E B||\leq c\nu \sqrt n$$
[We have to ensure that the failure probability is low enough so that no failure
occurs for any of the $n$ $i$ 's.] Let $\hat B$ be the rank $k$ approximation to $B$. As above,
except for $\epsilon n$ ``bad points", we have
$$||\hat B_{(j)} - \E B_{(j)}|| \leq \Delta/20.$$
We again pick $k$ points from among the rows of $\hat B$ which with high probability
are good points, each from a different cluster. To avoid unnecessary notation,
call these k points $\hat B_{(1)}, \hat B_{(2)},\ldots  \hat B_{(k)}$ and suppose $\hat B_{(1)}$ is within distance $\Delta/10$
of the center of the distribution from which $A_{(i)}$ is picked. Now imagine that we
have done all this before picking $A_{(i)}$; we may do so since $A_{(i)}$ is independent
of all this. Now consider the projection of $A_{(i)}$ onto the $k$ dimensional space
spanned by $\hat B_{(1)}, \hat B_{(2)}, \ldots \hat B_{(k)}$. Under reasonable assumptions, we can show that
in this projection $A_{(i)}$ is closer to $\hat B_{(1)}$ than to $\hat B_{(2)}, \hat B_{(3)}, \ldots  \hat B_{(k)}$. [The assumptions are to the effect that $O(n)$ coordinates of each center are non-zero. This
is to avoid the situation when a distribution is based only on $o(n)$ or in the
extreme case just $O(1)$ coordinates; such ``unbalanced" distributions have fat
tails.] Assuming this, we now can conclude that $i$ is in the same cluster as 1 (since we know all of
$\hat B_{(1)},\ldots \hat B_{(k)}$ and $\hat B_{(i)}$.
We may repeat the process of picking $k$ near-centers from $\hat B$ $O(1)$ times to get
a number of points which belong to the same cluster as $i$. The whole process
has to be repeated with each $i$ and one can complete the clustering by using all
the information gathered on pairs of points in the same cluster.
)}
%
We conclude this section with a proof of the lemma connecting the spectral norm and the Frobenius norm (from \cite{Achlioptas2005}).
\begin{proof} (of Lemma \ref{AMtoF}):
Let $u^{(1)},u^{(2)},\ldots u^{(k)}$ be the top $k$ singular vectors of $A$. Extend this to an
orthonormal basis $u^{(1)},u^{(2)},\ldots u^{(p)}$ of the vector space spanned by the rows of
$\hat A$ and $B$. [Note that $p\leq 2k$.] Then, we have
\begin{eqnarray*}
|\hat A-B||_F^2&=&\sum_{t=1}^k|(\hat A-B)u^{(t)}|^2+\sum_{t=k+1}^p|(\hat A-B)u^{(t)}|^2\\
&=& \sum_{t=1}^k |(A-B)u^{(t)}|^2+\sum_{t=k+1}^p |Bu^{(t)}|^2\\
&\leq& k||A-B||_2^2+\sum_{t=k+1}^p |Au^{(t)}+(B-A)u^{(t)}|^2\\
&\leq& k||A-B||_2^2+2\sum_{t=k+1}^p |Au^{(t)}|^2+2\sum_{t=k+1}^p |(B-A)u^{(t)}|^2\\
&\leq& k||A-B||_2^2+2k\sigma_{k+1}^2(A)+2k||A-B||_2^2.
\end{eqnarray*}
Now the Lemma follows from the claim : $\sigma_{k+1}(A)\leq ||A-B||_2$. This is because,
if not, letting now  $v^{(1)},v^{(2)},\ldots v^{(k)},v^{(k+1)}$ be the top $k+1$ singular vectors
of $A$, we would have
$$|Bv^{(t)}|\geq |Av^{(t)}|-||A-B||_2>0,$$ contradicting the hypothesis that rank of $B$ is $k$.
\end{proof}

\ignore{
\section{Beyond full independence}

Motivated by data mining and information retrieval applications,
the question of inferring the generative model from the data has moved from
random graphs to matrices where the rows represent {\em features} and the columns
represent {\em objects} and the $(i,j)$'th entry is the value of feature $i$ for object $j$.
Two salient examples are product-consumer
matrices, widely used in recommendation systems,
and term-document matrices. Recent work on recommendation systems uses the full independence assumptions,
so that the procedures described earlier can be carried out.

In both these examples, as well as others, it is easy to argue
that the full independence assumption is too strong. While one may
reasonably assume that consumers function independently, a particular consumer might not decide
independently on each product --- at the minimum, he/she might have
constraints on the total budget (and perhaps some others) which
results in correlations of the products bought. Similarly, while
documents in a collection may be drawn, say, from a mixture,
the terms that occur in each document are clearly not chosen
independently. This points to the following model and problem:

In a generative model where a collection of objects is chosen from a mixture distribution,
infer the model, given the objects. So the columns of the matrix are independent vector-valued
random variables.  The entries in a column are not independent.

The crucial tool we need is a Wigner-type theorem in this
situation. Such a theorem follows from results in
functional analysis and probability theory. But these are not
readily accessible, so we will present a
self-contained proof of the following theorem (including the
important classical technique of decoupling used in the
proof). The Theorem states that a Wigner-type bound holds
with just the limited independence assumption (that the columns
are independent) if we allow some extra logarithmic factors (see the corollary
below).

A definition will be useful: for a vector-valued random variable
$Y$, we define the variance of $Y$ denoted $\var(Y)$ as the
maximum variance of the real-valued random variable $v\cdot Y$,
where the maximum is taken over all unit length vectors $v$. I.e.,
it is the maximum variance of $Y$ in any direction. It is easy to
see that it is the maximum eigenvalue of the covariance matrix of $Y$:
\[
\var(Y)=\|\E YY^T\|.
\]

We first need a well-known fact (see for example, \cite{Bhatia},
IV.31). Throughout, $X,X_i$ will represent matrices. They may be
rectangular and the entries are assumed to be real numbers.

\begin{proposition}\label{fact1}
For $p>0$, $||X||_p = (\tr
(XX^T)^{p/2})^{1/p}=(\tr (X^TX)^{p/2})^{1/p}$ is a norm\footnote{
Since $XX^T$ is positive semi-definite, $(XX^T)^{p/2}$ is
well-defined. Namely if $XX^T=\sum_i\lambda_iu^{(i)}u^{(i)^T}$ is
the spectral decomposition, then
$(XX^T)^{p/2}=\sum_i\lambda_i^{p/2}u^{(i)}u^{(i)^T}.$} (called a
Schatten $p-$norm). Hence it is a convex function of the entries
of the matrix $X$.
\end{proposition}

\begin{exercise}
Show that $||X||_p\leq ||X||_q$ for $p\geq q$.
\end{exercise}

Recall that the trace of a matrix is the sum of its eigenvalues and
$||X||_\infty$ (also denoted $||X||$) is the spectral norm.

\begin{theorem}\label{limindep}
Suppose $A$ is an $m\times n$ matrix with independent
vector-valued random variables as its columns. Suppose the
variance of $A^{(i)}$ is $\nu_i^2$. Then for any $p$  which is a power of
2, we have
$$ \E||A-\E A||_p^p\leq (cp)^{cp}
  \left( n\E\;  \max_i|A^{(i)}-\E A^{(i)}|^p+n^{(p/2)+1}\sum_i\nu_i^{p}\right).$$
\end{theorem}

\begin{corollary}
Suppose with probability at least $1-\delta$, we have
$\|A^{(i)}-\E A^{(i)}\|\leq M$ for all $i$, then for all $t>0$,
$$\Pr\left( \|A-\E A\|\geq (c\log n)^{c}t(M+\sqrt n\max_i\nu_i)\right)\leq
\delta+ \frac{1}{n^{\log t/10}}.$$
\end{corollary}

\begin{proof}
We have that, for all $i$,
\[
\|A^{(i)}-\E A^{(i)}\|\leq M.
\]
Apply
the Theorem with $p=2\log n+1$.
\end{proof}

In the full independent case, we can take $M=\nu\sqrt n\log n$ and $\delta$ very small.
So, we get Wigner-type result in that case, but with additional log factors.

\subsection{Sums of matrix-valued random variables}

Throughout this section, $X_1,X_2,\ldots X_n, X'_1,X'_2,\ldots X'_n$ are independent matrix-valued random variables with $X'_i$ having the same distribution as $X_i$ and $\E X_i =0$ for $i=1,2,
\ldots n$. Let $p$ be a positive even integer.

Note that
\[
\|X\|_2^2=\tr XX^T=\|X\|_F^2.
\]
We need the following well-known generalization of H\"older's inequality to
matrices.

\begin{proposition}\label{fact2}
Suppose $A_1,A_2,\ldots A_m$ are matrices (of dimensions so that their
product is well-defined). We have for any positive
reals $r_1,r_2,\ldots r_m$ with $\sum_{i=1}^m {1\over r_i}=1$:
$$\|A_1A_2\ldots A_m\|_p\leq \|A_1\|_{pr_1}\|A_2\|_{pr_2}\ldots \|A_m\|_{pr_m}.$$
\end{proposition}

\begin{theorem} \label{Xi}[Square-Form Theorem]
Suppose $X_i$ are as above and $p$ is a power of $2$. Then,
$$\E\|\sum_{i=1}^nX_i\|_p^p\leq p^{7p}10^p\left( \E\|\sum_{i=1}^nX_iX_i^T\|_{p/2}^{p/2}
+\E\|\sum_{i=1}^nX_i^TX_i\|_{p/2}^{p/2}\right).$$
\end{theorem}

\begin{proof} By induction on $p$. For $p=2$, we have
$$\E||\sum_iX_i||_2^2=\E\tr \sum_{i,j}X_iX_j^T=\tr \E\sum_{i,j}X_iX_j^T=\E\tr \sum_iX_iX_i^T,$$
since
\[
\E X_iX_j^T=\E X_i\E X_j^T=0
\]
for $i\not= j$. Now since $\sum_i
X_iX_i^T$ is p.s.d., all its eigenvalues are non-negative and so,
\[
\tr \sum_i X_iX_i^T=\|\sum_iX_iX_i^T\|_1^1
\]
proving the case of $p=2$.

Now for general $p$,
\begin{eqnarray*}
\E\|\sum_{i=1}^nX_i\|_p^p&\le& \E\|\sum_iX_i\sum_jX_j^T\|_{p/2}^{p/2}
%&\leq& \E\left( ||\sum_iX_iX_i^T||_{p/2}+||\sum_{i\not=  j}X_iX_j^T||_{p/2}\right)^{p/2}\\
%&\leq& 2^{p/2}\E||\sum_i
%X_iX_i^T||_{p/2}^{p/2}+2^{p/2}\E||\sum_{i\not= j}
%X_iX_j^T||_{p/2}^{p/2}\\
 %&\leq&
2^{p/2}\E\|\sum_iX_iX_i^T\|_{p/2}^{p/2}+8^{p/2}\E\|\sum_{i}
X_iY^T\|_{p/2}^{p/2}.
\end{eqnarray*}
where $Y=\sum_jX_j'$ and we have used decoupling as in Lemma \ref{decoupling}
below. Note that
\[
\E X_iY^T=\E X_i\E Y^T=0
\]
since $X_i,Y$ are
independent. We now use induction (with the notation that
$[X_1|X_2|\ldots X_n]$ denotes the matrix with $X_1,X_2,\ldots
X_n$ written in that order)and $D$ is the matrix with $n$ diagonal
blocks, each equal to $Y^T$.
\begin{eqnarray*}
 &&\E||\sum_iX_iY^T||_{p/2}^{p/2}\\
 &\leq& 10^{p/2}(p/2)^{3.5p} \left(\E||\sum_i (YX_i^TX_iY^T)||_{p/4}^{p/4}
 +\E||\sum_i (X_iY^TYX_i^T)||_{p/4}^{p/4}\right)\\
 &=& 10^{p/2}(p/2)^{3.5p} \left(\E||Y\left( \sum_i(X_i^TX_i)\right)Y^T||_{p/4}^{p/4}
 +\E||[ X_1 | X_2|\ldots |X_n]D^TD[X_1|X_2|\ldots X_n]^T)||_{p/4}^{p/4}\right)\\
&\leq& 10^{p/2}(p/2)^{3.5p} \left(
\E||Y||_p^{p/2}||\sum_iX_i^TX_i||_{p/2}^{p/4}+
\E||Y||_p^{p/2}||[X_1|X_2|\ldots X_n]||_p^{p/2}\right)\\
&&\hfill \hbox{using Prop. \ref{fact2}}\\
&\leq& 10^{p/2}(p/2)^{3.5p} \E||Y||_p^{p/2}\left(
||\sum_iX_i^TX_i||_{p/2}^{p/4}+||\sum_iX_iX_i^T||_{p/2}^{p/4}\right)\\
 &\leq& 2\cdot 10^{p/2}(p/2)^{3.5p}\left(
\E||\sum_iX_i||_p^p\right)^{1/2}\left(
  \E||\sum_iX_iX_i^T||_{p/2}^{p/2}+\E||\sum_iX_i^TX_i||_{p/2}^{p/2}\right)^{1/2}
\end{eqnarray*}
where the fourth line is a bit tricky: if $X_i$ are $m\times q$ and
$m_0=\min (m,q)$, we use
\begin{eqnarray*}
&&\|[ X_1 | X_2|\ldots |X_n]D^TD[X_1|X_2|\ldots X_n]^T)\|_{p/4}^{p/4} \\
&=&
\sum_{t=1}^{m_0} \sigma_t^{p/2}([ X_1 | X_2|\ldots |X_n]D^T)\\
&\leq&  (\sum_{t=1}^{m_0}\sigma_t^p[ X_1 | X_2|\ldots |X_n])^{1/2}
(\sum_{t=1}^{m_0}\sigma_t(D)^p)^{1/2}\\
&\leq& \|Y\|_p^{p/2}\|[ X_1 | X_2|\ldots |X_n]\|_p^{p/2}.
\end{eqnarray*}
the last using Jensen's inequality and the fact that $Y, \sum_iX_i$ have same
distribution. Letting
\[
x=\sqrt{ \E||\sum_iX_i||_p^p} \quad \mbox{ and } \quad
b=\E||\sum_iX_iX_i^T||_{p/2}^{p/2}+\E||\sum_iX_i^TX_i||_{p/2}^{p/2},
\]
this yields the following quadratic inequality for $x$:
$$x^2\leq 2^{p/2}b+2\cdot 8^{p/2}10^{p/2}(p/2)^{3.5p}\sqrt b\; x$$
which implies that
$$x^2\leq 10^pp^{7p}b,$$
completing the inductive proof.
\end{proof}



\subsection{Decoupling}

We now introduce a beautiful technique developed by probabilists and functional analysts called
{\em decoupling} which helps
get rid of some dependencies between random variables, making the analysis easier in many contexts.
(See for example \cite{La Pena and Gine??}).
Decoupling looks like sleight of hand, but it is quite useful. It
has been extensively applied in similar contexts.

Suppose $f$ is any convex function from the set of matrices to non-negative reals with
$f(A)=f(-A)$ and satisfying the
condition that there is some $p>0$ such that
$$f(A+B)\leq 2^p (f(A)+f(B)).$$
Typical examples of $f$ will be $p$'th powers of norms.

\begin{lemma}\label{decoupling}
Let $X_1,X_2,\ldots X_n, X'_1,X'_2,\ldots X'_n$ be independent matrix-valued random variables with $X'_i$
having the same distribution as $X_i$ and $\E X_i=0$ for $i=1,2,\ldots n$. Then, for any even convex function $f$,
$$\E f\left(\sum_iX_i\sum_jX_j^T\right)\leq 8^p \E f\left(\sum_i X_i\sum_jX^{\prime T}_j\right)+2^p\E f\left( \sum_iX_iX_i^T\right).$$
\end{lemma}

The point of the Lemma is that the first term on
the RHS is easier to handle than the LHS, since now
$X_i',X_i$ are independent.

\begin{proof}
We let $Y_i=\{ X_i,X'_i\}$ (the set (without order) of the two elements $X_i,X'_i$) and
$Y=(Y_1,Y_2,\ldots Y_n)$. We define
random variables $Z_1,Z_2,\ldots Z_n,Z'_1,Z'_2,\ldots Z'_n$ as follows : for each $i$, independently,
with probability 1/2 each, we let $(Z_i,Z'_i)=(X_i,X'_i)$ or $(Z_i,Z_i')=(X'_i,X_i)$. Then, we clearly
have
\begin{eqnarray*}
\E (Z_iZ^{\prime T}_j|Y_i)
&=&{1\over 4} (X_iX_j^T+X'_iX_j^T+X_iX_j^{\prime T}+X'_iX^{\prime T}_j)\for i\not= j\\
\E (Z_iZ^{\prime T}_i|Y_i)&=& {1\over 2} (X_iX^{\prime T}_i+ X'_iX_i^T).
\end{eqnarray*}
\begin{eqnarray*}
&&\E f\left(\sum_iX_i\sum_jX_j^T\right) \\
&\leq&
2^p\E f(\sum_iX_iX_i^T)+2^p \E f(\sum_{i\not=j}X_iX_j^T) \\
&\leq&
2^p\E f(\sum_iX_iX_i^T)+\\
&&2^p\E f\left(\sum_{i\not=j}\left( X_iX_j^T + \E X_iX_j^{\prime
T}+\E X'_iX_j^T
 + \E X'_iX^{\prime T}_j\right)
+2\sum_{i}(\E X_iX_i^{\prime T}+\E X_i'X_i^T)\right)\\
&\leq& 2^p\E f(\sum_iX_iX_i^T)+\\
&&2^p\E f\left(\sum_{i\not=j}\left(
X_iX_j^T + X_iX_j^{\prime T}+X'_iX_j^T
 + X'_iX_j^{\prime T}
 \right) +2\sum_{i}(X_iX_i^{\prime T}+X_i'X_i^T)\right)\\
&&\hfill \hbox{    using Jensen and convexity of $f$, so $f(\E X)\leq \E f(X)$}\\
&\leq& 2^{p}\E f(\sum_iX_iX_i^T)+8^{p}\E f((\sum_iZ_i\sum_jZ_j^{\prime T})|Y)\\
&\leq& 2^{p}\E f(\sum_iX_iX_i^T)+8^{p}\E f(\sum_iZ_i\sum_jZ_j^{\prime T})\hbox{    using Jensen again }.
\end{eqnarray*}
Now, the Lemma follows noting that $\{ (Z_i,Z_j'):i=1,2,\ldots n\}$,
and $\{ (X_i,X'_j)\, : \, i=1,2,\ldots n\}$ have
the same joint distributions.
\end{proof}

\subsection{Proof of the spectral bound with limited independence}

We need another standard and useful trick:

\begin{lemma}\label{rademacher}
Suppose $X_1,X_2,\ldots X_n$ are independent matrix-valued random
variables with $EX_i=0$. Let $\zeta=(\zeta_1,\zeta_2,\ldots
\zeta_n)$, be a set of independent variables taking on values $\pm
1$ with probability 1/2 each \footnote{These are referred to as
Rademacher random variables in the literature}, which are also
independent of $X=(X_1,X_2,\ldots X_n)$. We have
$$E_X||\sum_i X_i||_p^p\leq 2^{p+1}E_{X,\zeta }||\sum_i \zeta_i X_i||.$$
\end{lemma}

\begin{proof}

Let $X'=(X_1',X_2',\ldots X_n')$ be a set of independent r.v.
(independent of $X_i,\zeta_i$).
\begin{eqnarray*}
&&\E_X||\sum_i X_i||_p^p\\
&=& \E_{X} ||\sum_i (X_i-\E_{X'}X'_i)||_p^p\\
&\leq& \E_{X,X'} ||\sum_i (X_i-X'_i)||_p^p \hbox{   Jensen}\\
&=& \E_{X,X',\zeta } ||\sum_i \zeta_i (X_i-X'_i)||_p^p\hbox{  since
$X_i-X'_i$ is a symmetric random variable }\\
&\leq& 2^p\E||\sum_i
\zeta_iX_i||_p^p+2^p\E||\sum_i\zeta_iX'_i||_p^p=2^{p+1}
\E||\sum_i\zeta_iX_i||_p^p,
\end{eqnarray*}
 as claimed.

\end{proof}

We would like to use the square-form theorem to prove Theorem
\ref{limindep}. But this cannot be done so directly. For example,
if we let $X_i$ to be the matrix with $A^{(i)}-EA^{(i)}$ in the
$i$'th column and 0 elsewhere, then the $X_i$ satisfy the
hypothesis of the Square Form Theorem, but unfortunately, we only
get
$$\E||A-\E A||_p^p\leq \hbox{ some terms} + (***) \E||\sum_i
X_iX_i^T||_{p/2}^{p/2},$$ which is useless since
\[
||\sum_i
X_iX_i^T||_{p/2}^{p/2}
=||(A-\E A)(A^T-\E A^T)||_{p/2}^{p/2}=||A-\E A||_p^p.
\]
We will actually
apply the Square Form theorem with
$$X_i=(A^{(i)}-\E A^{(i)})(A^{(i)^T}-\E A^{(i)^T})-D_i,$$
where,
$$D_i=\E\left( (A^{(i)}-\E A^{(i)})(A^{(i)^T}-\E A^{(i)^T})\right).$$
Then, we have
\begin{eqnarray*}
||A-\E A||_p^p&=&||\sum_i(A^{(i)}-\E A^{(i)})(A^{(i)^T}-\E A^{(i)^T})||_{p/2}^{p/2}\\
&\leq&
2^{p/2}||\sum_iX_i||_{p/2}^{p/2}+2^{p/2}||\sum_iD_i||_{p/2}^{p/2}.
\end{eqnarray*}
Clearly,
\[
||\sum_iD_i||_{p/2}^{p/2}\leq n^{p/2}\sum_i
||D_i||_{p/2}^{p/2}\leq
n^{(p/2)+1}\sum_i \nu_i^{p}
\]
(since $D_i$ is a rank 1 matrix
with singular value at most $\nu^2$) which
matches the second term on the right hand side of the claimed
bound in the Theorem. Now, we bound $\E||\sum_iX_i||_{p/2}^{p/2}$.
Let $\zeta=(\zeta_1,\zeta_2,\ldots \zeta_n)$ be independent $\pm
1$ random variables also independent of $X_i$. Then by Lemma
(\ref{rademacher}), we have (with $B^{(i)}=A^{(i)}-\E A^{(i)}$ for
notational convenience)
\begin{eqnarray*}
\E||\sum_i X_i||_{p/2}^{p/2}&\leq&
2^{(p/2)+1}\E||\sum_i\zeta_iX_i||_{p/2}^{p/2}\\
&\leq& 2^{p+1}
\E||\sum_i\zeta_iB^{(i)}B^{(i)^T}||_{p/2}^{p/2}+2^{p+1}\E||\sum_i\zeta_iD_i||_{p/2}^{p/2}.
\end{eqnarray*}
The term $2^{p+1}\E||\sum_i\zeta_iD_i||_{p/2}^{p/2}$ is easy to
bound as above.  Now applying the square form theorem to the first
term, we get
\begin{eqnarray*}
&&\E||\sum_i\zeta_iB^{(i)}B^{(i)^T}||_{p/2}^{p/2}\leq (cp)^{cp}
\E||\sum_i
B^{(i)}B^{(i)^T}B^{(i)}B^{(i)^T}||_{p/4}^{p/4}\\
&=& (cp)^{cp}\E||\sum_i|B^{(i)}|^2 B^{(i)}B^{(i)^T}||_{p/4}^{p/4}\\
&\leq& (cp)^{cp}\E\max_i |B^{(i)}|^{p/2} ||\sum_i
B^{(i)}B^{(i)^T}||_{p/4}^{p/4}\hbox{
since all $B^{(i)}B^{(i)^T}$ are p.s.d}\\
&\leq& (cp)^{cp}\left( \E\max_i|B^{(i)}|^p\right)^{1/2} \left(
\E||\sum_i
B^{(i)}B^{(i)^T}||_{p/4}^{p/2}\right)^{1/2}\hbox{    Jensen} \\
&\leq& (cp)^{cp}\sqrt n\left( \E\max_i|B^{(i)}|^p\right)^{1/2}
\left( \E||A-\E A||_{p}^{p}\right)^{1/2}
\end{eqnarray*}
since
\[
(\lambda_1^{p/4}+\lambda_2^{p/4}+\ldots
\lambda_n^{p/4})^2\leq n (\lambda_1^{p/2}+\lambda_2^{p/2}+\ldots
\lambda_n^{p/2}).
\]
Putting this all together, and letting
\[
a=(\E||A-\E A||_p^p)^{1/2}, \quad b=(cp)^{cp}\sqrt n\left(
\E\max_i|A^{(i)}-\E A^{(i)}|^p\right)^{1/2}
\]
and
\[
c_0=(cp)^{cp}n^{(p/2)+1}\sum_i\nu_i^{p/2}
\]
we get the following
quadratic inequality on $a$
$$ a^2\leq ab+c_0,$$
which now implies that $a^2\leq b^2+2c_0$ finishing the proof of Theorem \ref{limindep}.
}

\section{Clustering based on deterministic assumptions}

We started earlier with a random generative model of data - $A$. We used
Random Matrix theory to show a bound on $||A-EA||$. Then we argued that
$\hat A$, the best rank $k$ approximation to $A$ is in fact close to $EA$
in spectral norm and used
this to cluster ``most'' points correctly. However, the ``clean-up'' of the
mis-classified points presents a technical hurdle which is overcome often
by extra assumptions and involved technical arguments. Here we make an
attempt to present a simple algorithm which classifies all points correctly
at once. We start by making certain assumptions on the model; these assumptions are
purely geometric - we do not assume any probabilistic model. Under these
assumptions, we prove that a simple algorithm correctly classifies all the points.
A new feature of this proof is the use of the ``Sin $\Theta$''
theorem from Numerical Analysis to argue that not only are the singular values of
$\hat A$ and $EA$ close, but the spaces spanned by these two matrices are close too.
However, our result currently does not subsume earlier results under the
probabilistic model. [See discussion below.]

We are given $m$ points in ${\bf R}^n$ (as the rows of an $m\times n$
matrix $A$) and an integer $k$ and we want to cluster (partition)
the points into $k$ clusters. As in generative models, we assume that
there is an underlying (desirable) partition of $\{ 1,2,\ldots
m\}$ into $T_1,T_2,\ldots T_k$ which forms a ``good'' clustering and the
objective is to find precisely this clustering (with not a single ``misclassified''
point.) For $r=1,2,\ldots k$, define
$\mu_r = {1\over |T_r|}\sum_{i\in T_r}A_{(i)}$ as the center (mean) of the points in
the cluster. Let $C$ be the $m\times n$ matrix with $C_{(i)}=\mu_r$ for all $i\in T_r$.
We will now state the assumptions under which we will prove that
spectral clustering works. [We write
assumptions of the form $a\in \Omega(b)$
below to mean that there is some constant $c>0$ such that
if the assumption $a\geq cb$ holds, then the assertions/algorithms
work as claimed. Similarly for $a\in O(b)$.]
We first assume

{\bf Assumption 0} : $$||A-C||=\Delta\leq O(\sigma_k(C)/\log n).$$
[This is not a major assumption; see discussion below.]
We note that $||A-C||^2$ can be
viewed as the maximum total distance squared in any direction
of the points from their respective centers. So $\Delta$ being small
is the same as saying the displacements of $A_{(i)}$ from their respective
centers are not ``biased'' towards any direction, but sort of spread out.
[This is the intuition leading to Wigner-type bound on the largest singular
value of a random matrix.]

Our main assumptions on the model are stated below.

{\bf Assumption 1 : Boundedness} For all $r$ and all $i\in T_r$,
$$|A_{(i)}-\mu_r|\leq M\quad ;\quad |\mu_r|\leq M.$$

{\bf Assumption 2 : Correct Center is closest.}
Let $$\Delta_2={M\Delta \log n\over \sigma_k(C)}.$$
Let $F_1$ be the orthogonal projection onto the space spanned by the rows of $C$. Then,
for all $r\not= s$ and all $i\in T_r$,
$$|F_1(A_{(i)}-\mu _r)|\leq |F_1(A_{(i)}-\mu_s)| - \Omega (\Delta_2).$$

{\bf Assumption 3 : No Small Clusters} $$|T_r|\geq m_0 \in \Omega (m)\quad \forall r.$$

Note that Assumption 2 implies a {\bf inter-center separation} -
$$|\mu_r-\mu_s| = \Omega (\Delta_2).$$
Such an assumption is a regular feature
of most results.

Now consider the random case when the $A_{ij}$ are Bernoulli random
variables with $EA_{ij}=C_{ij}$.(the Full-Independent
case). For ease of comparison, assume $m\in \Theta(n)$ and that
all (most) $C_{ij}$ are $\Theta(p)$ for a positive real $p$.
In this case, it is easy to see that we can take $M\in \tilde \Theta(\sqrt {np})$.
Also Random Matrix Theory implies that $\Delta \in \Theta (\sqrt {np})$.
%Next, we want a lower bound on $\sigma_k(C)$. Consider the $k$ orthogonal
%vectors - $w_1,w_2,\ldots w_k$, each an indicator vector of
%one $T_r$. Clearly, we have (since $|\mu_r|\approx O(\sqrt np)$)
%$$\sigma_k(C)\geq \Min {|w_rC|\over |w_r|}\geq \Min |\mu_r|\sqrt {m_0}\geq \Omega (np).$$
We also a need a lower bound on $\sigma_k(C)$ or in other words, we need
$C$ have rank $k$. We assume that  $\sigma_k(C) = \Omega(np)$.

Thus $\Delta_2 = \tilde O(1)$. The best known results for probabilistic models
assume a separation of
$$|\mu_r-\mu_s|\geq \hbox{poly}(k)\sqrt p.$$
Thus our otherwise more general result does not match these.

We conjecture that the following clean result holds which would then subsume known
previous results under various probabilistic models.

{\bf Conjecture} We can exactly classify all points provided only the following
assumption holds :
$$\forall r\not= s,\forall i\in T_r, |F_1(A_{(i)}-\mu_r)|\leq |F_1(A_{(i)}-\mu_s|-\Omega\left( \hbox{poly}(k)||A-C||/\sqrt n\right).$$

\subsection{The Algorithm}

We use an approximation algorithm to solve the $k$-means problem on the points -
$\hat A_{(i)}, i=1,2,\ldots m$ to within a factor of say $c_2$. A simple algorithm
has been shown to achieve $c_2\in O(\log n)$ \cite{AV2007}, but $c_2\in O(1)$ can
be achieved by more complex algorithms \cite{CGTS99}.

Suppose the centers produced by the approximation algorithm are $v_1,v_2,\ldots v_r$.
Let $c_1=6\sqrt{ c_2+2}$

Note that the optimal $k-$means solution has optimal value OPT at most
$\sum_i |\hat A_{(i)}-C_{(i)}|^2=||\hat A-C||_F^2$.

\begin{claim} In a $c_2$-approximate solution, we must have that for each
$r,1\leq r\leq k$, there is a center $v_{i_r}$ (in the solution) such that
$|v_{i_r}-\mu_r|\leq {c_1\sqrt k\over \sqrt {m_0}}|| A-C||$.
\end{claim}

\begin{proof}

Let ${c_1\sqrt k\over \sqrt {m_0}}||A-C||=\beta$.
Suppose for some $r$, there is no center in the solution within distance
$\beta$ of $\mu_r$. Then we have using triangle inequality and the fact
that $(a-b)^2\geq {1\over 2}a^2-b^2$ for any reals $a,b$
that the
sum of distances squared of $\hat A_{(i)}, i\in T_r$ to their nearest center
in the solution is at least
$$\sum_{i\in T_r} (\beta -|\hat A_{(i)}-\mu_r|)^2 \geq (|T_r|/2)\beta^2-||\hat A-C||_F^2> c_2\hbox{ OPT}$$
producing a contradiction.

\end{proof}

Now $\sigma_k(C)\leq {1\over\sqrt k}||C||_F\leq {\sqrt m\over\sqrt k}M$;
thus, ${\sqrt k\over\sqrt m}\Delta\in O(\Delta_2)$.
Thus, for a suitable choice of $c_1,c_2$,
there must be $k$ different $v_r$; for notational convenience,
we assume from now on that
\begin{equation}\label{vrmur}
|v_r-\mu_r|\in O(\Delta_2).
\end{equation}
%
Let
$$S_r=\{ i : |\hat A_{(i)}-v_r|\leq |\hat A_{(i)}-v_s|\forall s\}.$$
Now, we will argue using the assumption that $S_r$ is exactly equal to
$T_r$ for all $r$.

To this end let $F_2$ denote (orthogonal) projection
onto the space spanned by the top $k$ right singular vectors of $A$ and recall
that $F_1$
denotes the orthogonal projection onto the space spanned by the rows of $C$.
We argue that $F_1\approx F_2$ using Davis-Kahan Sin$\theta$ theorem.
The theorem applies to Hermitian matrices. Of course $A,C$ are in general
rectangular. So
first let $|A|$ denote $\sqrt {A^TA}$ and similarly $|C|$ denote
$\sqrt {C^TC}$ (standard notation.) It is known [\cite{Bhatia1994}, (5.10)]
that there is a
fixed constant with
$$\left|\left| \; |A|-|C|\; \right|\right| \leq c_3\log n ||A-C||.$$
Clearly $\sigma_k(A)\geq \sigma_k(C)-||A-C||\geq {1\over 2}\sigma_k(C)$.
$F_1^\perp$ can be viewed as the projection onto
the eigenvectors of $|C|$ with eigenvalues less than or equal to 0.
Now we know (\cite{bhatia}  Exercise VII.1.11 and the sine $\theta$ theorem : Theorem VII.3.1)
\begin{equation}\label{F1F2}
||F_1^\perp F_2||=||F_2-F_1||\leq {c_4\log n\Delta \over \sigma_k(C)}\in O(\Delta_2/M).
\end{equation}
Now we use this as follows : for any $r\not= s$ and $i\in T_r$,
\begin{eqnarray*}
|F_2(A_{(i)}-v_r)| &\leq& |F_2(A_{(i)}-\mu_r)|+|F_2(\mu_r-v_r)|\\
&\leq& |F_1(A_{(i)}-\mu_r)|+O(\Delta_2)+|v_r-\mu_r|\hbox{  Assumption 1 and (\ref{F1F2})}\\
%&\leq |F_1(A_{(i)}-\mu_r)|+O(\Delta_2)\hbox{  using (\ref{vrmur}) }\\
&\leq& |F_1(A_{(i)}-\mu_s)| -\Omega(\Delta_2) \hbox{ Assumption 2}\\
&\leq& |F_2(A_{(i)}-\mu_s)|-\Omega(\Delta_2) \hbox{  using (\ref{F1F2}) provided }|A_{(i)}-\mu_s|\in O(M)\\
&\leq& |F_2(A_{(i)}-v_s)| -\Omega(\Delta_2) \hbox{ using (\ref{vrmur}) }
\end{eqnarray*}
Now if $|A_{(i)}-\mu_s|\geq 10M$, then we argue differently. First we have
$$|F_1(A_{(i)}-\mu_s)|^2 = |A_{(i)}-\mu_s|^2-|A_{(i)}-F_1(A_{(i)})|^2\geq |A_{(i)}-\mu_s|^2-|A_{(i)}-\mu_r|^2.$$
Thus, $|F_1(A_{(i)}-\mu_s)|\geq 0.9 |A_{(i)}-\mu_s|$. So we have (recalling Assumption (0))
$$|F_2(A_{(i)}-\mu_s)|\geq |F_1(A_{(i)}-\mu_s)|-|A_{(i)}-\mu_s|{\Delta_2\over M}\geq 0.8|A_{(i)}-\mu_s|
\geq |A_{(i)}-\mu_r|.$$



\section{Proof of the spectral norm bound}\label{sec:eigbound}

Here we prove Wigner's theorem (Thm. \ref{wigner}) for matrices with random $\pm 1$ entries. The proof is probabilistic,
unlike the proof of the general case for symmetric distributions.
The proof has two main steps. In the first step, we use
a discretization (due to Kahn and Szemer\'{e}di) to reduce from all unit vectors to a finite set of lattice points. The second step is a
Chernoff bound working with fixed vectors belonging to the lattice.

Let $\lat$ be the lattice $\left( \frac{1}{r\sqrt{n}} \Z
\right)^{n}$. The diagonal length of its basic parallelepiped is
$diag(\lat) = 1/r$.

\begin{lemma} \label{lem:linear-combination}
Any vector $u \in \R^{n}$ with $\|u\| = 1$ can be written as
\[
u = \lim_{N \rightarrow \infty} \sum_{i=0}^{N} \left( \frac{1}{r}
\right)^{i} u_{i}
\]
where
\[
\|u_{i}\| \leq 1 + \frac{1}{r},~ \forall~ i \geq 0.
\]
and $u_{i} \in \lat,~ \forall~ i \geq 0$.
\end{lemma}

\begin{proof}
Given $u \in \R^{n}$ with $\|u\| = 1$, we pick $u_{0} \in
\lat$ to be its nearest lattice point. Therefore,
\[
\|u_{0}\| \leq 1 + diag(\lat) = 1 +
\frac{1}{r}
\]
Now $(u - u_{0})$ belongs to some basic parallelepiped of $\lat$
and therefore $\|u - u_{0}\| \leq 1/r$. Consider the finer
lattice $\lat/r = \left\{ x/r \suchthat x \in \lat \right\}$, and
pick $u_{1}/r$ to be the point nearest to $(u - u_{0})$ in
$\lat/r$. Therefore,
\[
\|\frac{u_{1}}{r}\| \leq \llnorm{u - u_{0}} + diag(\lat/r)
\leq \frac{1}{r} + \frac{1}{r^{2}} \implies \llnorm{u_{1}} \leq 1 +
\frac{1}{r}
\]
and
\[
\llnorm{u - u_{0} - \frac{1}{r} u_{1}} \leq \frac{1}{r^{2}}
\]
Continuing in this manner we pick $u_{k}/r^{k}$ as the point
nearest to $\left( u - \sum_{i=0}^{k-1} (1/r)^{i} u_{i} \right)$
in the finer lattice $\lat/r^{k} = \left\{ x/r^{k} \suchthat x \in
\lat \right\}$. Therefore, we have
\begin{eqnarray*}
&& \llnorm{\frac{u_{k}}{r^{k}}} \leq \llnorm{u - \sum_{i=0}^{k-1}
\left( \frac{1}{r} \right)^{i} u_{i}} + diag(\lat/r^{k}) \leq
\frac{1}{r^{k}} + \frac{1}{r^{k+1}} \implies \llnorm{u_{k}} \leq 1
+ \frac{1}{r} \\
&& \llnorm{u - \sum_{i=0}^{k} \left( \frac{1}{r} \right)^{i} u_{i}}
\leq \frac{1}{r^{k+1}} \longrightarrow 0
\end{eqnarray*}
That completes the proof.
\end{proof}

Now using Lemma \ref{lem:linear-combination}, we will show that it
suffices to consider only the lattice vectors in $\lat \cap
\B(\bar{0}, 1 + 1/r)$ instead of all unit vectors in order to bound $\lambda(A)$.
Indeed, this bound holds for the spectral norm of a tensor.

\begin{proposition} \label{prop:reduce}
For any matrix $A$,
\[
\lambda(A) \leq~ \left(\frac{r}{r-1}\right)^2~ \left( \sup_{u,v} \in
\lat \cap \B(\bar{0}, 1 + \frac{1}{r}) \left| u^TAv \right| \right)
\]
\end{proposition}

\begin{proof}
From Lemma \ref{lem:linear-combination}, we can write any
$u$ with $\llnorm{u} = 1$ as
\[
u = \lim_{N \rightarrow \infty} \sum_{i=0}^{N} \left(
\frac{1}{r} \right)^{i} u_{i}
\]
where $u_{i} \in \lat \cap \B(\bar{0}, 1 + 1/r),~ \forall~
i$. We similarly define $v_{j}$.
Since $u^TAv$ is a continuous function, we can write
\begin{eqnarray*}
\left| u^TAv \right| &=& \lim_{N \rightarrow
\infty} \left| \left( \sum_{i=0}^{N} \left( \frac{1}{r}
\right)^{i} u_{i}\right)^TA\sum_{j=0}^\infty \left( \frac{1}{r}
\right)^{j} v_{j} \right| \\
&& \leq \left( \sum_{i=0}^{\infty} \left( \frac{1}{r} \right)^{i}
\right)^{2} \sup_{u,v \in \lat \cap
\B(\bar{0}, 1 + \frac{1}{r})} \left| u^TAv \right| \\
&& \leq \left(\frac{r}{r-1} \right)^{2} \sup_{u,v \in \lat \cap \B(\bar{0}, 1 + \frac{1}{r})} \left| u^TAv \right|
\end{eqnarray*}
which proves the proposition.
\end{proof}

We also show that the number of $r$ vectors $u \in \lat \cap \B(\bar{0}, 1 + 1/r)$ that we need
to consider is at most $(2r)^{n}$.

\begin{lemma} \label{lem:size-of-set}
The number of lattice points in $\lat \cap \B(\bar{0}, 1 + 1/r)$
is at most $(2r)^{n}$.
\end{lemma}

\begin{proof}
We can consider disjoint hypercubes of size $1/r \sqrt{n}$
centered at each of these lattice points. Each hypercube has
volume $(r \sqrt{n})^{-n}$, and their union is contained in
$\B(\bar{0}, 1 + 2/r)$. Hence,
\begin{eqnarray*}
\left| \lat \cap \B(\bar{0}, 1 + 1/r) \right| &\leq&
\frac{\text{Vol} \left( \B(\bar{0}, 1 + 1/r) \right)}{(r
\sqrt{n})^{-n}} \\
& \leq &\frac{2 \pi^{n/2} (1 + \frac{2}{r})^{n} r^{n}
n^{n/2}}{\Gamma(n/2)} \\
& \leq &(2r)^{n}
\end{eqnarray*}
\end{proof}

The following Chernoff bound will be used.
\begin{exercise} \label{ex:chernoff}
Let $X_{1}, X_{2}, \ldots, X_{m}$ be independent random variables, $X=\sum_{i=1}^m X_i$,
where each $X_i$ is $a_i$ with probability $1/2$ and $-a_i$ with probability $1/2$. Let $\sigma^2 = \sum_{i=1}^m a_i^2$.
Then, for $t > 0$,
\[
\Pr \left( \left| X \right| \geq t\sigma \right) \leq 2 e^{-t^{2}/2}
\]
\end{exercise}

Now we can prove the spectral norm bound for a matrix with random $\pm 1$ entries.

\begin{proof}

Consider fixed $u,v \in \lat \cap
\B(\bar{0}, 1 + 1/r)$. For $I = (i,j)$,
define a two-valued random variable
\[
X_{I} = A_{ij} u_{i}v_{j}.
\]
Thus $a_{I} = u_i v_j$, $X  = \sum_{I} X_{I} = u^TAv $, and
\[
\sigma^2 = \sum_{I} a_{I}^{2} = \llnorm{u}^{2}\llnorm{v}^{2}
 \le \left(\frac{r+1}{r} \right)^{4}.
\]
So using $t = 4\sqrt{n}\sigma$ in
the Chernoff bound \ref{ex:chernoff},
\[
\Pr \left( \left| u^TAv\right| \geq 4\sqrt{n} \cdot \sigma \right) \leq
2e^{-8n}.
\]
According to Lemma \ref{lem:size-of-set}, there are at most
$(2r)^{2n}$ ways of picking $u,v \in \lat \cap \B(\bar{0}, 1 + 1/r)$.  so we can use union bound to get
\[
\Pr \left( \sup_{u,v \in \lat \cap
\B(\bar{0}, 1 + \frac{1}{r})} \left| u^TAv \right| \geq 4\sqrt{n}\sigma \right) \leq (2r)^{2n} (e)^{-8n} \le e^{-5n}
\]
for $r = 2$.
And finally using Proposition \ref{prop:reduce} and the facts that for our choice of $r$, $\sigma \le 9/4$ and $(r/r-1)^2 \le 4$, we have
\[
\Pr \left( \lambda(A) \geq 36\sqrt{n} \right)
\leq e^{-5n}.
\]
This completes the proof.
\end{proof}

The above bound can be extended to $r$-dimensional tensors.

\begin{exercise}\label{tensor-norm}
Let $A$ be an $n \times n \times \ldots \times n$ $r$-dimensional array with
real entries. Its spectral norm $\lambda(A)$ is defined as
\[
\lambda(A) = \sup_{\llnorm{u^{(1)}} = \llnorm{u^{(2)}} = \ldots =
\llnorm{u^{(r)}} = 1} \left| A \left( u^{(1)}, u^{(2)}, \ldots,
u^{(r)} \right) \right|,
\]
where $A \left( u^{(1)}, u^{(2)}, \ldots, u^{(r)} \right) =
\sum_{i_{1}, i_{2}, \ldots, i_{r}} A_{(i_{1}, i_{2}, \ldots,
i_{r})}~ u^{(1)}_{i_{1}} u^{(2)}_{i_{2}} \cdots u^{(r)}_{i_{r}}$.
Suppose each entry of $A$ is $1$ or $-1$ with equal probability. Show that whp,
\begin{equation}
\lambda(A) = O(\sqrt{n}r \log r).
\end{equation}
\end{exercise}

\ignore{
\subsection{Samples needed to estimate the covariance matrix}

The Theorems above can be used to answer another important question. Namely, suppose
we have an $n-$variate Gaussian $F(x)$. Given a set of $N$ iid samples from $F$, we can find
the covariance matrix $\Sigma^S$ of the samples. It is easy to see that as $N$ goes
to infinity, $\Sigma^S$ goes to $\Sigma$ the true variance-covariance matrix of $F$. A natural
question is: suppose one wants to make {\bf relative error} $\epsilon$ in estimating $\Sigma$,
how large should $N$ be as a function of $n$? Here relative error means:
$$(1-\epsilon) v^T\Sigma v\leq v^T\Sigma^S v\leq (1+\epsilon )v^T\Sigma v\forall v.$$
This question came up in volume computation, with $F$ being either the uniform density over a convex body or an arbitrary
logconcave density.
It is not difficult to see that $N\in O(n^2)$. Improving the results of Bourgain, Rudelson showed that
$N\in O(n\log n)$ for convex bodies. To do this, he proved a version of the square-form Theorem above based closely
on the earlier work of Lust-Picard. Vempala and Lovasz applied this to general logconcave densities.
}


\section{Discussion}

The bounds on eigenvalues of symmetric random matrices, formulated by Wigner, were proved by F\"{u}redi and Komlos \cite{FK81} and tightened by Vu \cite{Vu05}. Unlike the concentration based proof given here, these papers use combinatorial methods and derive sharper bounds. Spectral methods were used for planted problems by Boppana \cite{Bop87} and Alon et al \cite{AKS98}. Subsequently, McSherry gave a simpler algorithm for finding planted partitions \cite{McSherry01}.
Spectral projection
was also used in random models of information retrieval by Papadimitriou et al \cite{PRTV98} and extended by
Azar et al \cite{AFKM2001}.

A body of work that we have not covered here deals with limited independence, i.e., only the rows are i.i.d. but the entries of a row could be correlated.  A. Dasgupta, Hopcroft, Kannan and Mitra \cite{DHKM07} give bounds for spectral norms of such matrices based on the functional analysis work of Rudelson \cite{Rudelson1999} and Lust-Picard \cite{Lust-Picard86}.
It is an open problem to give a simple, optimal clean-up algorithm for probabilistic spectral clustering.
\ignore{
Specifically, the Square form Theorem and its proof are essentially in a rather terse
paper by Lust-Picard \cite{Lust-Picard86}. Here we have put in all the details.
The application of the Square Form Theorem to prove an analog of
Theorem (\ref{limindep}) is from Rudelson's work \cite{Rudelson99}. He deals with the case when the
$A^{(i)}$ are i.i.d. and uses a different version of the Square Form Theorem (with better dependence
on $p$) which he obtains by using some more recent methods from functional analysis. A further reference for
decoupling is \cite{delaPena}.
}


\chapter{Recursive Spectral Clustering}\label{chap:kvv}

In this chapter, we study a spectral algorithm for partitioning a graph. The key algorithmic ingredient is a procedure to  find an approximately minimum conductance cut. This cutting procedure is used recursively to obtain a clustering algorithm.
The analysis is based on a natural bicriteria measure for assessing the quality of a clustering and makes no probabilistic assumptions on the input data. We begin with an important definition. Given a graph $G=(V,E)$, with nonnegative edge weights $a_{ij}$, for a subset of vertices $S$, we let $a(S)$ denote the total weight of edges incident to vertices in $S$. Then the conductance of a subset $S$ is
\[
\phi(S) = \frac{\sum_{i \in S, j \not\in S} a_{ij}}{\min \{ a(S), a(V\setminus S)\}},
\]
and the conductance of the graph is
\[
\phi = \min_{S \subset V} \phi(S).
\]


\section{Approximate minimum conductance cut}

The following simple algorithm takes a weighted graph (or weighted adjacency matrix) as input and outputs a cut of the graph.

\begin{center}
\fbox{\parbox{4.7in}{
\begin{minipage}{4.5in}
\begin{tt}
{\bf Algorithm: Approximate-Cut}

\begin{enumerate}
\item[1.] Normalize the adjancency matrix so each row sum is $1$.

\item[2.] Find the second largest eigenvector of this matrix.

\item[3.] Order the vertices according their components in this vector.

\item[4.] Find the minimum conductance cut among cuts given by this ordering.
\end{enumerate}
\end{tt}
\end{minipage}
}}
\end{center}

The following theorem bounds the conductance of the cut found by this heuristic
with respect to the minimum conductance. This theorem plays an important role in the analysis of Markov chains, where conductance is often easier to estimate than the desired quantity, the spectral gap. The latter determines the mixing rate of the Markov chain.
Later in this chapter, we will use this cutting procedure as a tool to find a clustering.

\begin{theorem}\label{thm:condineq}
Suppose $B$ is a $N\times N$
matrix with non-negative entries with each row sum equal
to 1 and suppose there are positive real numbers $\pi_1,\pi_2,\ldots \pi_N$
summing to 1 such that $\pi_i b_{ij}=\pi_j b_{ji}$ for all $i,j$. If
$v$ is the right eigenvector of $B$ corresponding to the second
largest eigenvalue $\lambda_2$,
and $i_1,i_2,\ldots i_N$ is an ordering of $1,2,\ldots
N$ so that $v_{i_1}\geq v_{i_2}\ldots \geq v_{i_N}$, then
\begin{eqnarray*}
\min_{S\subseteq \{1,2,\ldots N\}} \frac{\sum\limits_{i\in S,j\notin S}\pi_i b_{ij}}
       {\min (\sum\limits_{i\in S}\pi_i,\sum\limits_{j\notin S} \pi_j)}
 \geq  1-\lambda_2
\geq \frac{1}{2} \left( \min_{l, 1\leq l\leq N}
   \frac{\sum\limits_{1\leq u\leq l; l+1\leq v\leq N}\pi_{i_u}b_{i_u i_v}}
   {\min (\sum\limits_{1\leq u\leq l}\pi_{i_u},\sum\limits_{l+1\leq v\leq N} \pi_{i_v})}\right)^2
\end{eqnarray*}
\end{theorem}

We note here that the leftmost term above is just the conductance of the graph with weights $b_{ij}$, while the rightmost term is the square of the minimum conductance of cuts along the ordering given by the second eigenvector of the of the normalized adjacency matrix. Since the latter is trivially at least as large as the square of the overall minimum conductance, we get
\[
\mbox{min conductance} \ge 1 - \lambda_2 \ge \frac{1}{2} \left(\mbox{min conductance}\right)^2.
\]

\noindent
{\bf Proof (of Theorem \ref{thm:condineq}).}
We first evaluate the second eigenvalue. Towards this end, let $D^2={\rm diag}(\pi)$.
Then, from the time-reversibility property of $B$, we have $D^2B=B^TD^2$.
Hence $Q=DBD^{-1}$ is symmetric. The eigenvalues of $B$ and $Q$ are the same, with their
largest eigenvalue equal to $1$. In addition, $\pi^TD^{-1}Q=\pi^TD^{-1}$
and therefore $\pi^T D^{-1}$ is the left eigenvector of $Q$ corresponding to the eigenvalue
$1$. So we have,
$$\lambda_2=\max_{\pi^T D^{-1}x=0}\frac{x^TDBD^{-1}x}{x^Tx}$$
Thus, substituting $y=D^{-1}x$, we obtain
\begin{eqnarray*}
1-\lambda_2 =\min_{\pi^T D^{-1}x=0}\frac{x^TD(I-B)D^{-1}x}{x^Tx}
		=\min_{\pi^T y=0}\frac{y^TD^2(I-B)y}{y^TD^2y}
\end{eqnarray*}
The numerator can be rewritten:
\begin{eqnarray*}
y^TD^2(I-B)y
&=& -\sum_{i\neq j} y_iy_j\pi_ib_{ij}+\sum_{i}\pi_i(1-b_{ii})y_i^2\\
&=&-\sum_{i\neq j}  y_iy_j\pi_ib_{ij}+\sum_{i\neq j}\pi_ib_{ij}\frac{y_i^2+y_j^2}{2}\\
&=&\sum_{i<j}\pi_ib_{ij}(y_i-y_j)^2
\end{eqnarray*}
Denote this final term by $\cE(y,y)$. Then
\begin{eqnarray*}
1-\lambda_2=\min_{\pi^T y=0}\frac{\cE(y,y)}{\sum_i \pi_i y_i^2}
\end{eqnarray*}
To prove the first inequality of the theorem, let $(S,\bar{S})$ be the
cut with the minimum conductance. Define a vector $w$ as follows
$$
w_i=
\begin{cases}
\ \sqrt{\frac{1}{\sum_u a(u)}\,\frac{\pi(\bar{S})}{\pi(S)}}
	&\quad \text{ if } i\in S \\[0.5cm]

-\sqrt{\frac{1}{\sum_u a(u)}\,\frac{\pi(S)}{\pi(\bar{S})}}
	&\quad \text{ if } i\in \bar{S}
\end{cases}
$$
It is then easy to check that $\sum_i \pi_i w_i=0$ and that
$$\phi(S) \ge \frac{\cE(w,w)}{\sum_i \pi_i w_i^2}\ge 1-\lambda_2 $$
Hence we obtain the desired lower bound on the conductance.

We will now prove the second inequality.
Suppose that the minimum above is attained when $y$ is equal to $v$.
Then $Dv$ is the eigenvector of $Q$ corresponding to the eigenvalue $\lambda_2$
and, $v$ is the right eigenvector of $B$ corresponding to $\lambda_2$. Our
ordering is then with respect to $v$ in accordance with the statement of
the theorem. Assume that, for simplicity of notation, the indices are reordered
(i.e. the rows and corresponding columns of $B$ and $D$ are reordered) so that
\[
v_1\geq v_2\geq \cdots \ge v_N.
\]
Now define $r$ to satisfy
\[
\pi_1+\pi_2+\cdots +\pi_{r-1}\leq \frac{1}{2} < \pi_1+\pi_2+\cdots +\pi_{r},
\]
and let $z_i=v_i-v_r$ for $i=1,\ldots,n$. Then
\[
z_1\geq z_2\geq \cdots \geq z_r=0\geq z_{r+1}\geq \cdots \geq z_n,
\] and
\begin{eqnarray*}
\frac{\cE(v,v)}{\sum_i \pi_i v_i^2}
& = & \frac{\cE(z,z)}{-v_r^2+\sum_i \pi_i z_i^2}\\
& \geq &\frac{\cE(z,z)}{\sum_i \pi_i z_i^2}\\
& = &
\frac{\left(\sum\limits_{i<j}\pi_ib_{ij}(z_i-z_j)^2\right)
\left(\sum\limits_{i<j}\pi_ib_{ij}(|z_i|+|z_j|)^2\right)}{
\left(\sum\limits_i \pi_i z_i^2\right)
\left(\sum\limits_{i<j}\pi_ib_{ij}(|z_i|+|z_j|)^2\right)}
%\\
%& = & \frac{\c}{
%\left(\sum\limits_i \pi_i z_i^2\right)\left(\sum\limits_{i<j}\pi_ib_{ij}(|z_i|+|z_j|)^2\right)}
%\qquad (\mathrm{say}) \nonumber
\end{eqnarray*}
Consider the numerator of this final term. By Cauchy-Schwartz
\begin{eqnarray}
\left(\sum\limits_{i<j}\pi_ib_{ij}(z_i-z_j)^2\right)
	\left(\sum\limits_{i<j}\pi_ib_{ij}(|z_i|+|z_j|)^2\right)
&\geq &\left(\sum_{i<j}\pi_ib_{ij}|z_i-z_j|(|z_i|+|z_j|)\right)^2 \nonumber \\
&\geq &\left(\sum_{i<j}\pi_ib_{ij}\sum_{k=i}^{j-1}|z_{k+1}^2-z_k^2|\right)^2 \label{technical}
\end{eqnarray}
Here the second inequality follows from the fact that if $i<j$ then
\[
|z_i-z_j|(|z_i|+|z_j|) \geq \sum_{k=i}^{j-1}|z_{k+1}^2-z_k^2|.
\]
This
follows from the following observations:
\begin{itemize}
\item[a.] If $z_i$ and $z_j$ have the same sign (i.e. $r\not\in \{i,i+1,\ldots,j\}$) then
\[
|z_i-z_j|(|z_i|+|z_j|)=|z_i^2-z_j^2|.
\]
\item[b.] Otherwise, if $z_i$ and $z_j$ have different signs then
\[
|z_i-z_j|(|z_i|+|z_j|)= (|z_i|+|z_j|)^2 > z_i^2+z_j^2.
\]
\end{itemize}
Also,
\begin{eqnarray*}
\sum_{i<j}\pi_ib_{ij}(|z_i|+|z_j|)^2
\leq 2\sum_{i<j}\pi_ib_{ij}(z_i^2+z_j^2) \le  2\sum_i \pi_i z_i^2
\end{eqnarray*}
As a result we have,
\begin{eqnarray*}
\frac{\cE(v,v)}{\sum_i \pi_i v_i^2}&\geq&
\frac{\left(\sum\limits_{i<j}\pi_ib_{ij}(z_i-z_j)^2\right)
\left(\sum\limits_{i<j}\pi_ib_{ij}(|z_i|+|z_j|)^2\right)}
{\left(\sum\limits_i \pi_i z_i^2\right)\left(\sum\limits_{i<j}\pi_ib_{ij}(|z_i|+|z_j|)^2\right)}\\
&\geq&\frac{\left(\sum_{i<j}\pi_ib_{ij}\sum_{k=i}^{j-1}|z_{k+1}^2-z_k^2|\right)^2}
	{2\left(\sum_i \pi_i z_i^2\right)^2}
\end{eqnarray*}
Set $S_k=\{1,2,\ldots,k\}$, $C_k=\{(i,j):i\leq k<j\}$ and
$$\hat{\alpha} = \min_{k,1\leq k\leq N} \frac{\sum\limits_{(i,j)\in C_k}\pi_i b_{ij}}
	{\min (\sum\limits_{i:i\leq k} \pi_i ,\sum\limits_{i: i>k} \pi_i)}$$
Since $z_r=0$, we obtain
\begin{eqnarray*}
\sum_{i<j}\pi_ib_{ij}\sum_{k=i}^{j-1}|z_{k+1}^2-z_k^2|
& = &\sum_{k=1}^{N-1}|z_{k+1}^2-z_k^2| \sum_{(i,j)\in C_k}\pi_ib_{ij}\\
&\geq & \hat{\alpha}\left(\sum_{k=1}^{r-1}(z_k^2-z_{k+1}^2)\pi(S_k)+\sum_{k=r}^{N-1}(z_{k+1}
^2-z_k^2)(1-\pi(S_k))\right)\\
& = &\hat{\alpha}\left(\sum_{k=1}^{N-1}(z_k^2-z_{k+1}^2)\pi (S_k)+(z_N^2-z_r^2)\right)\\
&=& \hat{\alpha}\sum_{k=1}^{N}\pi_kz_k^2.
\end{eqnarray*}
Consequently, if $\pi^T y=0$ then
$$1-\lambda_2=\frac{\cE(v,v)}{\sum_i \pi_i v_i^2}\geq \frac{\hat{\alpha}^2}{2}.$$
\qed


\section{Two criteria to measure the quality of a clustering}

The measure of the quality of a clustering we will use here is based on
expansion-like properties of the underlying pairwise similarity graph.
The quality of a clustering is given by two parameters:
$\alpha$, the minimum conductance
of the clusters, and $\e$, the ratio of the weight
of inter-cluster edges to the total weight of all edges. Roughly speaking,
a good clustering achieves high $\alpha$ and low $\e$.
Note that the conductance provides a measure of the quality of an individual
cluster (and thus of the overall clustering) while the weight of the inter-cluster edges
provides a measure of the cost of the clustering. Hence, imposing a lower bound, $\a$, on the quality
of each individual cluster we seek to minimize the cost, $\e$, of the clustering; or conversely, imposing an upper
bound on the cost of the clustering we strive to maximize its quality. For a detailed motivation of this bicriteria measure we refer the reader to the introduction of \cite{KVV04}.

\begin{definition}\label{condclus}
We call a partition $\{C_1,C_2,\ldots, C_l\}$ of $V$
an $(\alpha,\epsilon)$-clustering if:\\
1. The conductance of each $C_i$ is at least $\alpha$.\\
2. The total weight of inter-cluster edges is at most an $\e$
fraction of the total edge weight.
\end{definition}

Associated with this bicriteria measure is the following
optimization problem:
(P1) Given $\a$, find an $(\alpha,\e)$-clustering that minimizes $\e$
(alternatively, we have (P2) Given $\e$, find an $(\alpha,\e)$-clustering that maximizes $\a$). We note that the number of clusters is not restricted.

\section{Approximation Algorithms}\label{sec:algo}
Problem (P1) is NP-hard. To see this, consider
maximizing $\a$ with $\e$ set to zero. This problem is equivalent to finding the
conductance of a given graph which is well known to be NP-hard \cite{GJ79}. We consider the following heuristic approach.

\bigskip

\begin{center}
\fbox{\parbox{4.7in}{
\begin{minipage}{4.5in}
\begin{tt}
{\bf Algorithm: Recursive-Cluster}

\begin{enumerate}
\item[1.] Find a cut that approximates the minimum conductance cut in $G$.

\item[2.] If the conductance of the cut obtained is below a preset threshold, recurse on the pieces induced by the cut.

\end{enumerate}
\end{tt}
\end{minipage}
}}
\end{center}

The idea behind our algorithm is simple. Given $G$, find a cut $(S,\bar{S})$ of minimum
conductance. Then recurse on the subgraphs induced by $S$ and $\bar{S}$.
Finding a cut of minimum conductance is hard, and hence we need to
use an approximately minimum cut.
There are two well-known approximations for the minimum conductance cut,
one is based on a semidefinite programming relaxation (and precurson on a linear programming relaxation) and the
other is derived from the second eigenvector of the graph. Before we
discuss these approximations, we present a general theorem that captures both for the purpose of analyzing the clustering heuristic.

Let $\mathcal{A}$ be an approximation
algorithm that produces a cut of conductance at most
$Kx^\nu$ if the minimum conductance is $x$, where $K$ is independent
of $x$ ($K$ could be a function of $n$, for example) and $\nu$ is a fixed
constant between between $0$ and $1$. The following theorem provides
a guarantee for the approximate-cluster algorithm using $\mathcal{A}$ as a subroutine.

\begin{theorem}\label{thm:gen}
If $G$ has an $(\a,\e)$-clustering, then the recursive-cluster algorithm, using approximation
algorithm $\mathcal{A}$ as a subroutine, will find a clustering of quality
$$\left(\left(\frac{\a}{6K \log \frac{n}{\e}}\right)^{1/\nu}, \,(12K+2)\e^\nu\,\log \frac{n}{\e}\right).$$
\end{theorem}

\noindent
{\bf Proof.}
Let the cuts produced by the algorithm be $(S_1,T_1),(S_2,T_2),
\ldots $, where we adopt the convention that $S_j$ is the ``smaller''
side (i.e., $a(S_j)\leq a(T_j)$).
Let $C_1,C_2,\ldots C_l$ be an $(\alpha, \epsilon )$-clustering.
We use the
termination condition of
$\a^*=\frac{\a}{6\log \frac{n}{\e}}$.
We will assume that we apply the
recursive step in the algorithm only if the conductance of
a given piece as detected by the heuristic for the minimum conductance
cut is less than $\alpha^*$. In addition, purely for
the sake of analysis we consider a slightly modified algorithm.
If at any point we have a cluster $C_t$ with the property that
$a(C_t)< \frac{\epsilon}{n} a(V)$ then we split $C_t$ into singletons.
The conductance of singletons is defined to be $1$.
Then, upon termination, each cluster has conductance at least
$$\left(\frac{\alpha^*}{K}\right)^{1/\nu}
= \left(\frac{\a}{6K \log \frac{n}{\e}}\right)^{1/\nu}$$
Thus it remains to bound the weight of the
inter-cluster edges.
Observe that $a(V)$ is twice the total edge weight in the graph, and
so $W= \frac{\e}{2}\,a(V)$ is the weight of the inter-cluster edges in this
optimal solution.

Now we divide the cuts into two groups. The first group, $H$, consists
of cuts with ``high'' conductance within clusters. The second group consists of
the remaining cuts.
We will use the notation
$w(S_j,T_j)= \sum_{u\in S_j, v\in T_j} a_{uv}$.
In addition, we denote by $w_{\mbox{\tiny I}}(S_j,T_j)$
the sum of the weights of the intra-cluster edges of the cut $(S_j,T_j)$,
i.e., $w_{\mbox{\tiny I}}(S_j,T_j)=\sum_{i=1}^l w(S_j\cap C_i,T_j\cap C_i)$.
We then set
\begin{eqnarray*}
H=\Big\{ j : w_{\mbox{\tiny I}} (S_j,T_j)\geq 2\a^* \sum_{i=1}^l
\min (a(S_j\cap C_i),a(T_j\cap C_i))\Big\}
\end{eqnarray*}
We now bound the cost of the high conductance group. For all $j\in H$, we have,
\begin{eqnarray*}
\a^* a(S_j) &\geq & w(S_j,T_j) \geq w_{\mbox{\tiny I}}(S_j,T_j) \geq
2\a^* \sum_i \min( a(S_j\cap C_i),a(T_j\cap C_i))
\end{eqnarray*}
Consequently we observe that
$$\sum_i  \min(a(S_j\cap C_i),a(T_j\cap C_i)) \leq \frac{1}{2} a(S_j)$$
From the algorithm's cuts, $\{(S_j,T_j)\}$, and the optimal clustering, $\{C_i\}$,
we define a new clustering via a set of cuts $\{(S_j',T_j')\}$ as follows.
For each $j\in H$, we define a cluster-avoiding cut $(S_j',T_j')$ in $S_j\cup T_j$
in the following manner. For each $i,1\leq i\leq l$, if
$a(S_j\cap C_i)\geq a(T_j\cap C_i)$, then place all of
$(S_j\cup T_j)\cap C_i$ into $S_j'$. If
$a(S_j\cap C_i) < a(T_j\cap C_i)$, then place all of
$(S_j\cup T_j)\cap C_i$ into $T_j'$.
%An example is given in Figure~\ref{fig:avoid}, where the original cut is
%shown by the solid line and the cluster-avoiding cut by the dashed line.

%\begin{figure}[h]
%\unitlength1cm
%\begin{center}
%\begin{picture}(5,5)
%\epsfig{file=avoid2.eps, height=5cm, width=5cm}
%\end{picture}\par
%\caption{\label{fig:avoid} The proof of Theorem \ref{thm:gen}.}
%\end{center}
%\end{figure}

Notice that, since $|a(S_j)-a(S_j')|\leq \frac{1}{2} a(S_j)$, we have
that $\min ( a(S_j'),a(T_j'))\geq \frac{1}{2} a(S_j)$.
Now we will use the approximation guarantee for the cut procedure
to get an upper bound on $w(S_j,T_j)$
in terms of $w(S_j',T_j')$.
\begin{eqnarray*}
\frac{w(S_j,T_j)}{a(S_j)} &\leq&  K \left(\frac{w(S_j',T_j')}{\min \{a(S_j'),a(T_j')\}}\right)^\nu\\
& \leq & K \left(\frac{2w(S_j',T_j')}{a(S_j)}\right)^\nu\\
\end{eqnarray*}
Hence we have bounded the overall cost of the high conductance cuts with respect to
the cost of the cluster-avoiding cuts. We now bound the cost of these
cluster-avoiding cuts. Let $P(S)$ denote the set of inter-cluster
edges incident at a vertex in $S$, for any subset $S$ of $V$.
Also, for a set of edges $F$, let $w(F)$ denote the sum of their weights.
Then, $w(S_j',T_j')\leq w(P(S_j'))$, since every edge in $(S_j',T_j')$ is an
inter-cluster edge. So we have,
\begin{eqnarray}
w(S_j,T_j) & \leq & K \big(2 w(P(S_j'))\big)^\nu a(S_j)^{1-\nu} \label{wsj2}
\end{eqnarray}

Next we prove the following claim.

\noindent{\bf Claim 1. } For each vertex $u\in V$, there are at most $\log \frac{n}{\e}$
values of $j$ such that $u$ belongs to $S_j$. Further, there are at most
$2\log \frac{n}{\e}$ values of $j$ such that $u$ belongs to $S_j'$.

To prove the claim, fix a vertex $u\in V$. Let
$$I_u=\{ j : u\in S_j\}\qquad\qquad J_u=\{ j : u \in S_j'\setminus S_j\}$$
Clearly if $u\in S_j\cap S_k$ (with $k>j$), then $(S_k,T_k)$ must be a partition
of $S_j$ or a subset of $S_j$. Now we have,
$a(S_k)\leq \frac{1}{2} a(S_k\cup T_k)\leq \frac{1}{2} a(S_j)$.
So $a(S_j)$ reduces by a factor of 2 or greater between two
successive times $u$ belongs to $S_j$. The maximum value of
$a(S_j)$ is at most $a(V)$ and the minimum value is at least
$\frac{\epsilon}{n} a(V)$, so the first statement of the claim follows.

Now suppose $j,k\in J_u; j < k$. Suppose also $u\in C_i$. Then
$u\in T_j\cap C_i$.  Also, later,
$T_j$ (or a subset of $T_j$) is partitioned into $(S_k,T_k)$
and, since $u\in S_k'\setminus S_k$, we have $a(T_k\cap C_i)
\leq a(S_k\cap C_i)$.
Thus $a(T_k\cap C_i)
\leq \frac{1}{2}a( S_k\cup T_k)\leq \frac{1}{2}a(T_j\cap C_i)$.
Thus $a(T_j\cap C_i)$
halves between two successive
times that $j\in J_u$. So, $|J_u|\leq \log \frac{n}{\e}$.
This proves the second statement in the claim (since $u\in S_j'$ implies that
$u\in S_j$ or $u\in S_j'\setminus S_j$).
%These concepts are shown pictorially in Figure \ref{fig:claim}, where
%the cuts $(S_j,T_j)$ and $(S_k,T_k)$ are represented by solid lines and
%the cuts $(S_j',T_j')$ and $(S_k',T_k')$ by dashed lines.

Using this claim, we can bound the overall cost of the group of cuts with
high conductance within clusters with respect to the cost of the optimal
clustering as follows:

%\begin{figure}[h]
%\unitlength1cm
%\begin{center}
%\begin{picture}(5,5)
%\epsfig{file=claim.eps, height=5cm, width=5cm}
%\end{picture}\par
%\caption{\label{fig:claim} Proof of Claim 1.}
%\end{center}
%\end{figure}

\begin{eqnarray}
\sum_{j\in H} w(S_j,T_j)
&\leq & \sum_{\mathrm{all}\ j} K\big(2w(P(S_j'))\big)^\nu a(S_j)^{1-\nu}\nonumber\\
&\leq & K\Big( 2\sum_{\mathrm{all}\ j}w(P(S_j'))\Big)^\nu
\Big( \sum_{j} a(S_j)\Big)^{1-\nu}\nonumber\\
&\leq & K\left(2 \epsilon \log\frac{n}{\e}\,a(V)\right)^{\nu}\left(2 \log
\frac{n}{\e}\, a(V)\right)^{1-\nu}\nonumber\\
&\leq & 2K\epsilon^\nu \log\frac{n}{\e}\,a(V)\label{2goodj}
\end{eqnarray}
Here we used H\"{o}lder's inequality: for real sequences $a_1, \ldots, a_n$ and $b_1,\ldots, b_n$,
and any $p,q \ge 1$ with $(1/p)+(1/q)=1$, we have
\[
\sum_{i=1}^n a_i b_i \le \left(\sum_{i=1}^n a_i^p\right)^{\frac{1}{p}}\left(\sum_{i=1}^n b_i^q\right)^{\frac{1}{q}}.
\]

Next we deal with the group of cuts with low conductance within clusters i.e.,
those $j$ not in $H$. First, suppose that all the cuts together
induce a partition of $C_i$ into $P^i_1,P^i_2,\ldots P^i_{r_i}$.
Every edge between two vertices in $C_i$ which belong
to different sets of the partition must be cut by some cut $(S_j,T_j)$
and, conversely, every edge of every cut $(S_j\cap C_i,T_j\cap C_i)$ must have
its two end points in different sets of the partition. So, given that
$C_i$ has conductance $\a$, we obtain
\begin{eqnarray*}
\sum_{\mathrm{all}\ j}w_{\mbox{\tiny I}}(S_j\cap C_i,T_j\cap C_i)
	= \frac{1}{2}\sum_{s=1}^{r_i}w (P^i_s,C_i\setminus P^i_s) \geq  \frac{1}{2}\alpha \sum_{s} \min (a(P^i_s), a(C_i\setminus P^i_s))
\end{eqnarray*}
For each vertex $u\in C_i$
there can be at most $\log \frac{n}{\e}$
values of $j$ such that $u$ belongs to the smaller (according to $a(\cdot )$)
of the two sets $S_j\cap C_i$ and $T_j\cap C_i$. So, we have that
\begin{eqnarray*}
\sum_{s=1}^{r_i}\min(a(P^i_s),a(C_i\setminus P^i_s))
& \geq & \frac{1}{\log \frac{n}{\e}} \sum_j \min (a(S_j\cap C_i), a(T_j\cap C_i))
\end{eqnarray*}
Thus,
\begin{eqnarray*}
\sum_{\mathrm{all}\  j} w_{\mbox{\tiny I}}(S_j,T_j) \geq \frac{\alpha}{2\log \frac{n}{\e}}
\sum_{i=1}^l \sum_j \min (a(S_j\cap C_i),a(T_j\cap C_i))
\end{eqnarray*}
Therefore, from the definition of $H$, we have
\begin{eqnarray*}
\sum_{j\notin H}w_{\mbox{\tiny I}}(S_j,T_j)\leq  2\a^* \sum_{\mathrm{all}\ j}
          \sum_{i=1}^l \min (a(S_j\cap C_i),a(T_j\cap C_i)) \leq  \frac{2}{3} \sum_{\mathrm{all}\  j} w_{\mbox{\tiny I}}(S_j,T_j)
\end{eqnarray*}
Thus, we are able to bound the intra-cluster cost of the low conductance group of cuts in terms
of the intra-cluster cost of the high conductance group. Applying (\ref{2goodj}) then gives
\begin{eqnarray}
\sum_{j\notin H}w_{\mbox{\tiny I}}(S_j,T_j) \leq  2\sum_{j\in H} w_{\mbox{\tiny I}}(S_j,T_j)
\leq 4K \epsilon^\nu \log \frac{n}{\e}a(V) \label{2badj}
\end{eqnarray}
In addition, since each inter-cluster edge belongs to at most one cut $S_j,T_j$, we have that
\begin{eqnarray}
\sum_{j\notin H} (w(S_j,T_j)-w_{\mbox{\tiny I}}(S_j,T_j)) & \leq & \frac{\epsilon}{2}\, a(V) \label{2badjic}
\end{eqnarray}

We then sum up (\ref{2goodj}), (\ref{2badj}) and (\ref{2badjic}). To get
the total cost we note that
splitting up all the $V_t$ with $a(V_t)\leq \frac{\epsilon}{n} a(V)$ into
singletons costs us at most $\frac{\epsilon}{2}\, a(V)$ on the whole.
Substituting $a(V)$ as twice the total sum of edge weights gives
the bound on the
cost of inter-cluster edge weights. This completes the proof of Theorem \ref{thm:gen}.
\qed

The Leighton-Rao algorithm for approximating the conductance finds a cut
of conductance at most $2\log n$ times the minimum \cite{LR99}. In our
terminology, it is an approximation algorithm with $K=2\log n$ and $\nu =1$.
Applying theorem \ref{thm:gen} leads to the following guarantee.

\begin{corollary}\label{cor:LR}
If the input has an $(\alpha, \epsilon)$-clustering, then,
using the Leighton-Rao method for approximating cuts, the recursive-cluster algorithm
finds an
\[
\left(\frac{\a}{12 \log n\log \frac{n}{\e}}, 26\e\,\log n\log \frac{n}{\e}\right)\mbox{-clustering}.
\]
\end{corollary}

We now assess the running time of the algorithm using this heuristic.
The fastest implementation for this heuristic runs in $\tilde{O}(n^2)$ time
(where the $\tilde{O}$ notation suppresses factors of $\log n$).
Since  the algorithm makes less than $n$ cuts, the total running
time is $\tilde{O}(n^3)$.
This might be slow for some real-world applications.
We discuss a potentially more practical algorithm in the next section.
We conclude this section with the guarantee obtained using Arora et al.'s improved approximation \cite{ARV04} of $O(\sqrt{\log n})$.
\begin{corollary}\label{cor:ARV}
If the input to the recursive-cluster algorithm has an $(\alpha, \epsilon)$-clustering, then using the ARV method for approximating cuts, the algorithm finds an
\[
\left(\frac{\a}{C \sqrt{\log n}\log \frac{n}{\e}}, C\e\,\sqrt{\log n}\log \frac{n}{\e} \right)\mbox{-clustering}.
\]
where $C$ is a fixed constant.
\end{corollary}


\section{Worst-case guarantees for spectral clustering}\label{sec:spectral2}
In this section, we describe and analyze a recursive variant
of the spectral algorithm. This algorithm, outlined below, has been
used in computer vision, medical informatics,
%\cite{SM, BBKPVW,WEB, RFV},
web search, spam detection etc..
We note that the algorithm is
a special case of the recursive-cluster algorithm described in
the previous section; here we use a spectral heuristic to approximate
the minimum conductance cut. We assume the input is a weighted adjacency matrix $A$.

\bigskip

\begin{center}
\fbox{\parbox{4.7in}{
\begin{minipage}{4.5in}
\begin{tt}
{\bf Algorithm: Recursive-Spectral}

\begin{enumerate}
\item[1.] Normalize $A$ to have unit row sums and find its second right eigenvector $v$.

\item[2.] Find the best ratio cut along the ordering given by $v$.

\item[3.] If the value of the cut is below a chosen threshold, then recurse on the pieces induced by the cut.

\end{enumerate}
\end{tt}
\end{minipage}
}}
\end{center}

Thus, we find a clustering by repeatedly solving a one-dimensional
clustering problem. Since the latter is easy to solve, the algorithm
is efficient. The fact that it also has worst-case quality guarantees
is less obvious.

We now elaborate upon the basic description of this variant of the
spectral algorithm. Initially, we normalize our matrix $A$ by scaling
the rows so that
the row sums are all equal to one. At any later stage in the algorithm
we have a partition $\{C_1,C_2,\dots,C_s\}$. For each
$C_t$, we consider the $|C_t|\times |C_t|$ submatrix $B$
of $A$ restricted to $C_t$. We normalize $B$ by
setting $b_{ii}$ to $1-\sum_{j\in C_t, j\not= i} b_{ij}$. As a result,
$B$ is also non-negative with row sums equal to one.

Observe that upon normalization
of the matrix, our conductance measure corresponds to
the familiar Markov Chain conductance measure i.e.
$$\phi(S) = \frac{\sum_{i \in S, j\not\in S} a_{ij}}{\min(a(S), a(\bar{S}))}
=\frac{\sum_{i \in S, j\not\in S} \pi_i b_{ij}}{\min(\pi(S), \pi(\bar{S}))}$$
where $\pi$ is the stationary distribution of the Markov Chain.

We then find the second eigenvector of $B$. This is
the right eigenvector $v$ corresponding to the second
largest eigenvalue $\lambda_2$, i.e. $Bv=\lambda_2 v$.
Then order the elements (rows) of $C_t$ decreasingly with respect to
their component in the direction of $v$. Given this ordering,
say $\{ u_1,u_2,\ldots u_r\}$, find the minimum {\em ratio cut} in $C_t$.
This is the cut that minimises $\phi ( \{ u_1,u_2,\ldots u_j\}, C_t)$
for some $j$, $1\le j \le r-1$.
We then recurse on the pieces $\{ u_1,\ldots, u_j\}$
and $C_t\setminus \{ u_1,\ldots, u_j\}$.

We combine Theorem \ref{thm:condineq} with Theorem \ref{thm:gen}
to get a worst-case guarantee for Algorithm Recursive-Spectral. In the terminology of Theorem \ref{thm:gen},
Theorem \ref{thm:condineq} says that the spectral heuristic for minimum conductance
is an approximation algorithm with $K=\sqrt{2}$ and $\nu = 1/2$.

\begin{corollary}
\label{cor:spec}
If the input has an $(\alpha, \epsilon)$-clustering, then,
using the spectral heuristic, the approximate-cluster algorithm
finds an
\[
\left(\frac{\a^2}{72 \log^2 \frac{n}{\e}}, 20 \sqrt{\e}\,\log \frac{n}{\e}\right)
\mbox{-clustering}.\qed
\]
\end{corollary}


\section{Discussion}

This chapter is based on Kannan et al. \cite{KVV04} and earlier work by Jerrum and Sinclair \cite{SJ89}.
Theorem \ref{thm:condineq} was essentially proved by
Sinclair and Jerrum (in their proof of Lemma 3.3 in \cite{SJ89}, although not
mentioned in the statement of the lemma).
Cheng et al. \cite{CKVW06} give an efficient implementation
of recursive-spectral that maintains sparsity, and
has been used effectively on large data sets from diverse applications.

Spectral partitioning has also been shown to have good guarantees for some special classes of graphs. Notably, Spielman and Teng \cite{ST2007} proved that a variant of spectral partitioning produces small separators for bounded-degree planar graphs, which often come up in practical applications of spectral cuts. The key contribution of their work was an upper bound on the second smallest eigenvalue of the Laplacian of a planar graph. This work was subsequently generalized to graphs of bounded genus \cite{Kelner2006}.
%experimental
%evidence shows that it performs favorably compared to other well-known clustering
%algorithms.
%KVV, lots of references to practical work.







\chapter{Optimization via Low-Rank Approximation}\label{chap:maxrcsp}

In this chapter, we study boolean constraint satisfaction problems
(CSP's) with $r$ variables per constraint. The general problem is weighted MAX-$r$CSP: given an $r$CSP with a weight for each constraint, find a boolean assignment that maximizes the total weight of satisfied constraints. This captures numerous interesting special cases, including problems on graphs such as max-cut. We study an approach based on low-rank tensor approximation, i.e., approximating a tensor (multi-dimensional array) by the sum of a small number of rank-$1$ tensors. An algorithm for efficiently approximating a tensor by a small number of rank-$1$ tensors is given in Chapter \ref{chap:extensions}. Here we apply it to the max-$r$CSP problem and obtain a polynomial-time approximation scheme under a fairly general condition (capturing all known cases).

A MAX-$r$CSP problem can be formulated as a problem of maximizing
a homogenous degree $r$ polynomial in the variables $x_1,x_2,
\ldots x_n, (1-x_1),(1-x_2),\ldots (1-x_n)$
(see e.g. \cite{AFKK02}.) Let
$${\bf S}=\{ y=(x_1,\ldots x_n, (1-x_1),\ldots (1-x_n)):
x_i\in \{ 0,1\} \}$$
be the solution set.
Then the problem is
$$\Max_{y\in {\bf S}}
\sum_{i_1,i_2,\ldots i_r=1}^{2n} A_{i_1,i_2,\ldots i_r} y_{i_1}y_{i_2}
\ldots y_{i_r}.$$
where $A$ is a given nonnegative symmetric $r$-dimensional array
i.e.,
\[A_{i_1,i_2,\ldots i_r}= A_{i_{\sigma(1)},i_{\sigma(2)},\ldots i_{\sigma (r)} }
\]
for any permutation $\sigma$.
The entries of the $r$-dimensional array $A$ can be viewed as the
weights of an $r$-uniform hypergraph on $2n$ vertices. Throughout,
we assume that $r$ is fixed.

Our main tool to solve this problem is a generalization of low-rank
matrix approximation. A rank-$1$ tensor is the {\em outer product} of $r$ vectors
 $x^{(1)},\ldots x^{(r-1)},x^{(r)}$, given by the
$r$-dimensional array whose $(i_1,\ldots i_r)$'th
entry is $x^{(1)}_{i_1}x^{(2)}_{i_2},
\ldots x^{(r)}_{i_{r} }$; it is denoted
$x^{(1)}\otimes x^{(2)}\otimes \ldots x^{(r)}$.

In Chapter \ref{chap:extensions}, it is shown that
\begin{enumerate}
\item For any $r$-dimensional array $A$, there exists
a good approximation by the
sum of a small number of rank-1 tensors (Lemma \ref{lem:LowRankEXIST}).

\item We can algorithmically find such an approximation (Theorem \ref{thm:FastTensor}).
\end{enumerate}

In the case of matrices, traditional Linear Algebra algorithms find
good approximations. Indeed, we can find the {\em best} approximations
under both the Frobenius and $L_2$ norms
using the Singular Value Decomposition.
Unfortunately, there is no such theory
for $r$-dimensional arrays when $r\geq 2$.
Nevertheless, the sampling-based algorithm from Chapter \ref{chap:extensions}
will serve our purpose.

%While MAX-$r$CSP's are hard to solve in general, there is an easy
%constant factor approximation (for fixed $r$): pick a random assignment.
%Each constraint has a constant probability
%of being satisfied (we assume $r$ is fixed)
%and thus the expected weight of satisfied constraints
%is at least a constant fraction of the total weight.
%It is hard to do better than this in general.

We conclude this section by defining two norms of interest for tensors, the Frobenius norm and the $2$-norm,
generalizing the corresponding norms for matrices.
\begin{eqnarray*}
||A||_F &=&\left(\sum A^2_{i_1,i_2,\ldots i_r}\right)^{\frac{1}{2}}\\
||A||_2 &=& \max_{x^{(1)},x^{(2)},\ldots x^{(r)}}
{A ( x^{(1)},x^{(2)},\ldots x^{(r-1)},x^{(r)} )
\over |x^{(1)}| |x^{(2)}|\ldots }.
\end{eqnarray*}

\section{A density condition}

We begin with a density condition on tensors. We will see later that if a MAX-$r$CSP viewed
as a weighted $r$-uniform hypergraph satisfies this condition,
then there is a PTAS for the problem. This condition
provides a unified framework
for a large class of weighted MAX-$r$CSP's.

Define the node weights $D_1, \ldots, D_n$ of $A$ and their average as
\[
D_i = \sum_{i_2,i_3,\ldots i_r\in V} A_{i,i_2,\ldots i_r}\qquad
\bar{D} = \frac{1}{2n} \sum_{i=1}^n D_i.
\]
Note that when $r=2$ and $A$ is the adjacency matrix of a
graph, the $D_i$ are the degrees of the vertices and
$\bar D$ is the average degree.
%
\begin{definition}\label{CORE-STRENGTH}
The {\em core-strength} of a weighted $r$-uniform hypergraph
given by an $r$-dimensional tensor
$A$ is
\[
\left( \sum_{i=1}^{2n} D_i\right)^{r-2}\sum_{i_1, i_2, \ldots, i_r }
\frac{A_{i_1,\ldots,i_r}^2}{\prod_{j=1}^r (D_{i_j}+\bar{D})}
\]
\end{definition}

We say that a class of weighted hypergraphs (MAX-$r$CSP's)
is {\em core-dense} if the core-strength is $O(1)$
(i.e., independent of $A,n$).

To motivate the definition, first suppose
the class consists of unweighted hypergraphs.
Then if a hypergraph in the class has $E$ as the edge set
with $|E|=m$ edges, the condition says that
\begin{equation}\label{HYPERGRAPH}
m^{r-2} \sum_{(i_1,\ldots, i_r) \in E} \frac{1}{\prod_{j=1}^r (D_{i_j}+\bar{D})}  \
= O(1).
\end{equation}
Note that here the $D_i$'s are the degrees of the hypergraph vertices in the
usual sense of the number of edges incident to the vertex.
It is easy to see this condition is satisfied for dense hypergraphs, i.e.,
for $r$-uniform hypergraphs with $\Omega (n^r)$ edges, because in this
case, $\bar D\in \Omega(n^{r-1})$. The dense case was the first major milestone of progress on this problem.

The condition can be specialized to the case $r=2$, where it says that
\begin{equation}\label{GRAPH}
\sum_{i,j}\frac{A_{ij}^2}{(D_i+\bar{D})(D_j+\bar{D})} = O(1).
\end{equation}
We will show that all metrics satisfy this condition. Also, so do
{\em quasimetrics}. These are weights that satisfy the triangle
inequality up to a constant factor (e.g., powers of a metric).
%and arise in clustering applications \cite{FKKR03, SS73, FK00}.
So a special case of the main theorem is a PTAS for metrics and quasimetrics.
%(While PTAS's were known for the dense case, they were not known
%previously for the
%metric case.)
The main result of this chapter is the following.
\begin{theorem}\label{thm:coredense}
There is a PTAS for any core-dense weighted MAX-$r$CSP.
\end{theorem}
The algorithm and proof are given in Section \ref{COREDENSE}.
We will also show (in Section \ref{METRIC}) that a generalization of the notion
of metric for higher $r$ also satisfies our core-dense condition.
\begin{theorem}\label{LOCAL}
Suppose for a MAX-$r$CSP,
the tensor $A$ satisfies the following local density condition:
\[
\forall \, i_1, \ldots, i_r, \quad A_{i_1,\ldots,i_r} \le \frac{c}{n^{r-1}}\
 \sum_{j=1}^r
D_{i_j}
\]
where $c$ is a constant. Then there is a PTAS for the MAX-$r$CSP
defined by $A$.
\end{theorem}

The condition in the theorem
says that no entry of $A$ is ``wild'' in that it is
at most a constant times the average entry in the
$r$ ``planes'' passing through the entry. The reason for
calling such tensors ``metric tensors'' will become clear
when we see in Section \ref{METRIC}
that for $r=2$, metrics do indeed satisfy this condition.
When the matrix $A$ is the adjacency matrix of a graph, then
the condition says that for any edge, one of its end points
must have degree $\Omega (n)$. This is like the ``everywhere
dense'' condition in \cite{AKK95}.
Theorem \ref{LOCAL} has the following corollary for
``quasi-metrics'', where the
triangle inequality is only satisfied within constant factors -
$A_{ik}\leq c(A_{ij}+A_{jk})$.
\begin{corollary}\label{QM}
  There exists a PTAS for metric and quasimetric instances of MAX-CSP.
\end{corollary}




\section{The matrix case: MAX-$2$CSP}

In this section, we prove Theorem \ref{thm:coredense} in the case $r=2$.
This case already contains the idea of scaling which we will
use for the case of higher $r$. However, this
case does not need new algorithms for finding low-rank
approximations as they are already available from classical linear algebra.

Recall that we want to find
$$\Max_{y\in {\bf S}} A_{ij}y_iy_j = y^T Ay, $$
where
\[
{\bf S}=\{ y = (x_1,x_2,\ldots x_n,(1-x_1),(1-x_2),
\ldots (1-x_n)), x_i\in \{ 0,1\} \}
\]
is the solution set.
We will describe in this section an algorithm to solve this
problem to within additive error $O(\epsilon n\bar D)$,
under the assumption that that the
core-strength of $A$ is at most a constant $c$.
The algorithm will run in time polynomial in $n$
for each fixed $\epsilon >0$. Note that
\[
\Max_{y\in {\bf S}}y^TAy\geq \E(y^TAy) = {1\over 2}n\bar D,
\]
where
$\E$ denotes expectation over uniform random choice of $x\in \{ 0,1\}^n$.
Thus, this will prove Theorem (\ref{thm:coredense}) for this case (of $r=2$).

In the algorithm below for MAX-$2$CSP, we assume the input is a matrix $A$ whose entries denote the weights of the terms in the CSP instance.

\begin{center}
\fbox{\parbox{4.7in}{
\begin{minipage}{4.5in}
\begin{tt}
{\bf Algorithm: Approximate MAX-$2$CSP}

\begin{enumerate}
\item[1.] Scale the input matrix $A$ as follows:
\[
B = D^{-1}AD^{-1}
\]
where $D$ is the diagonal matrix with $D_{ii} =\sqrt{D_i + \bar{D}}$.
\item[2.] Find a low-rank approximation $\hat{B}$ to $B$ such that
\[
\|B-\hat{B}\|_2 \le \frac{\eps}{2}\|B\|_F
\]
and rank of $\hat{B}$ is $O(1/\eps^2)$.
\item[3.] Set $\hat{A} = D\hat{B}D$.
\item[4.] Solve $\max_{y \in S} y^T\hat{A}y$ approximately.
\end{enumerate}
\end{tt}
\end{minipage}
}}
\end{center}

The last step above will be expanded presently. We note here that it is a low-dimensional problem since $\hat{A}$ is a low-rank matrix.

In the first step, the algorithm scales the matrix $A$.
%to get a matrix $B$ given by :
%$$B=D^{-1}AD^{-1}\quad$$
%where,   $D$ is the diagonal matrix with
%$D_{ii} = \sqrt {D_i+\bar D}$.
A related scaling,
\[
B_{ij} = {A_{ij}\over \sqrt{D_i}\sqrt{ D_j}}
\]
is natural and has been used in
other contexts (for example when $A$ is the transition
matrix of a Markov chain). This scaling
unfortunately scales up ``small degree'' nodes too much for our purpose
and so we use a modified scaling. We will
see that while the addition of $\bar D$ does not increase
the error in the approximation algorithms, it helps by modulating the
scaling up of low degree nodes. From the definition of core-strength, we get the next claim.
\begin{claim}\label{BF}
$||B||_F^2$ is the core-strength of the matrix $A$.
\end{claim}
%
The second step is perfomed using the SVD of the matrix $B$ in polynomial-time.
%for any $\epsilon>0$,a matrix $\hat B$ of rank
%$l\leq 4/\epsilon^2$ such that
%\[
%||B-\hat{B}||_2\leq \frac{\epsilon}{2}||B||_F.
%\]
In fact, as shown in \cite{FKV04}, such a matrix $\hat{B}$ can be computed in
linear in $n$ time with error at most twice as large.

After the third step, the rank of $\hat A$ equals the rank of $\hat B$.
In the last step, we solve the following problem approximately to within
additive error $O(\epsilon n\bar D)$:
\begin{equation}\label{yThatAy}
\max_{y\in {\bf S}} y^T\hat Ay
\end{equation}

We will see how to do this approximate optimization presently.
First, we analyze the error caused by replacing
$A$ by $\hat A$.

\begin{eqnarray*}
\Max_{y\in {\bf S}} |y^T(A-\hat A)y| &=&\Max_{y\in {\bf S}} |y^TD(B-\hat B)Dy|\\
&\leq& \Max_{y\in {\bf S}}|Dy|^2 ||B-\hat B||_2\\
&\leq& \epsilon \sum_i (D_i+\bar D)||B||_F\\
&\leq& 4\epsilon n\bar D (\hbox{core-strength of }A)^{1/2},
\end{eqnarray*}
the last because of Claim  \ref{BF}
and the fact that $\sum_i D_i = 2n\bar D$.

Now for solving the non-linear optimization problem
(\ref{yThatAy}), we proceed as follows: suppose the
SVD of $\hat B$ expressed $\hat B$ as $U\Sigma V$,
where the $U$ is a $2n\times l$ matrix with orthonormal
columns, $\Sigma $ is a $l\times l$ diagonal matrix with
the singular values of $\hat B$ and $V$ is a $l\times 2n$
matrix with orthonormal rows. We write
\begin{eqnarray*}
y^T\hat Ay=(y^TDU)\Sigma (VDy)=u^T\Sigma v\\
\hbox{ where,  } u^T=y^TDU \quad \hbox{and}\quad
v=VDy
\end{eqnarray*}
are two $l-$ vectors. This
implies that there are really only $2l$ ``variables'' -
$u_i,v_i$ in the
problem (and not the $n$ variables - $y_1,y_2,\ldots y_n$).
This is the idea we will exploit.
Note that for $y\in {\bf S}$, we have (since $U,V$ have orthonormal
columns, rows respectively)
\[
|u|^2\leq |y^TD|^2\leq \sum_i (D_i+\bar D)\leq 4n\bar D.
\]
Similarly,  $|v|^2\leq 4n \bar D$.
So letting
$$\alpha =\sqrt {n\bar D},$$
we see that the the vectors $u,v$ live in the rectangle
$$R=\{ (u,v) : -2\alpha \leq u_i,v_j\leq +2\alpha \}.$$
Also, the gradient of the function $u^T\Sigma v$ with respect to
$u$ is $\Sigma v$ and with respect to $v$ is $u^T\Sigma$; in either
case, the length of the gradient vector is at most $2\alpha
\sigma_1(\hat B)\leq 2\alpha \sqrt c$. We now divide up $R$
into small cubes; each small cube will have side
$$\eta ={\epsilon \alpha \over 20 \sqrt {l}},$$
and so there will be $ \epsilon ^{-O(l)}$ small cubes.
The function $u^T\Sigma v$ does not vary by more than
$\epsilon n\bar D\sqrt c/10$ over any small cube. Thus we can
solve (\ref{yThatAy}) by just enumerating all the small cubes
in $R$  and for each determining whether it is feasible
(i.e., whether there exists a 0-1 vector $x$ such that for
some $(u,v)$ in this small cube, we have $u^T=y^TDu, v=VDy$,
for $y=(x,\underline 1-x)$.)

For each small cube $C$ in $R$,
this is easily formulated as an integer program in the $n$
0,1 variables $x_1,x_2,\ldots x_n$ with $4l$ constraints
(arising from the upper and lower bounds on the coordinates of $u,v$
which ensure that $(u,v)$ is in the small cube.)

For a technical reason, we have to define a $D_i$ to be ``exceptional''
if $D_i\geq \epsilon^6 n\bar D/10^6$; also call an $i$
exceptional if either $D_i$ or $D_{i+n}$ is exceptional. Clearly, the
number of exceptional $D_i$ is at most $2\times 10^6/\epsilon^6$ and we
can easily identify them. We enumerate all possible sets of
$2^{O(1/\epsilon^6)}$ 0,1 values of the exceptional $x_i$ and
for each of these set of values, we have an Integer Program again,
but now only on the non-exceptional variables.

We consider the Linear Programming (LP) relaxation of each of these Integer
Programs obtained by relaxing $x_i\in \{ 0,1\}$ to $0\leq x_i\leq 1$.
If one of these
LP's has a feasible solution, then, it has a basic feasible solution
with at most $4l$ fractional variables, Rounding all these fractional
variables to 0 changes $Dy$ by a vector of length at most
$$\sqrt { 4l \epsilon^6 n\bar D/10^6}\leq \eta .$$
Thus, the rounded integer vector $y$ gives us a $(u,v)$ in the small
cube $C$ enlarged (about its center) by a factor of 2 (which we
call $2C$). Conversely, if none of these LP's has a feasible solution,
then clearly neither do the corresponding Integer Programs and so
the small cube $C$ is infeasible. Thus, for each small cube $C$,
we find (i) either $C$ is infeasible or (ii) $2C$ is feasible.
Note that $u^T\Sigma v$ varies by at most $\epsilon n\bar D/5$ over
$2C$. So, it is clear that returning the maximum value of
$u^T\Sigma v$ over all centers of small cubes for which (ii) holds
suffices.

We could have carried this out with any ``scaling'.
 The current choice turns out to be useful for the
two important special cases here. Note that we are able to add
the $\bar D$ almost ``for free'' since we have $\sum _i D_i+\bar D
\leq 2\sum D_i$.

\ignore{
\subsection{Maximum Weighted Bisection and other problems}

The maximum weighted bisection problem in an undirected graph
is to split the vertices into equal parts so as to maximize the
total weight of edges from one part to the other. We will show
that this problem has a PTAS for the case of core-dense weights.
In fact, we will show something more general : consider a
family of problems of the form :
$$\Max_{y\in {\bf S}}y^TAy \hbox{     subject to    }
Cx\leq d\qquad x_i\in \{ 0,1\},$$
where

(i) the number of constraints in $Cx\leq d$ is $O(1)$,

(ii) for every solution of $Cx\leq d\quad ;\quad 0\leq x_i\leq 1$,
we can round only the fractional valued variables to integer values
to get a solution to $Cx\leq d\quad ,\quad x_i\{ 0,1 \}$ and

(iii) the family has a core-dense weight matrix ($A$).

Our result is that any such family admits a PTAS. The argument proceeds
the same way as when there are no ``side-constraints'' $Cx\leq d$. But
we note that using (i), there are still only $O(l)$ fractional variables
in a basic feasible solution of every LP. By (ii), we can round them to
produce an integral solution with the same error bounds (within constant
factors) as we get for the problem with no side-constraints.

Note that for the maximum weighted bisection problem, $Cx\leq d$
has just two constraints - $\sum_i x_i\leq n/2$ and $\sum_i x_i\geq n/2$
and (ii) is easily seen to be valid. Indeed, more generally, we may
also have node weights and require that we split into two parts
of equal node weight, as long as (ii) is valid. More generally,
we can also require some $O(1)$ subsets of vertices must all be
bisected etc.
}
\section{MAX-$r$CSP's}\label{COREDENSE}

In this section, we consider the general case of weighted MAX-$r$CSP's
and prove Theorem \ref{thm:coredense}. The algorithm is a direct generalization of the two-dimensional case.

For any $k$ vectors $x^{(1)},x^{(2)},\ldots x^{(k)}$,
the $r-k$ dimensional tensor
\[
A(x^{(1)},x^{(2)},\ldots x^{(k)},\cdot,\cdot ) =
\sum_{i_1,i_2,\ldots i_{r-1}}
A_{i_1,i_2,\ldots i_{r-1},i} x^{(1)}_{i_1}x^{(2)}_{i_2},
\ldots x^{(r-1)}_{i_{r-1}}.
\]
We wish to solve the problem
\[
\max_{y \in {\bf S}} A(y,y,\ldots,y).
\]


\begin{center}
\fbox{\parbox{4.7in}{
\begin{minipage}{4.5in}
\begin{tt}
{\bf Algorithm: Approximate MAX-$r$CSP}

\begin{enumerate}
\item[1.] Scale the input tensor $A$ as follows:
\[
B_{i_1,\ldots,i_r}= \frac{A_{i_1,\ldots,i_r}}{\prod_{j=1}^r\alpha_{i_j}}
\]
where  $\alpha= (\alpha_1,\ldots,\alpha_n) \in \reals^n$ is defined by
$\alpha_j=\sqrt {\bar D+ D_j}$.
\item[2.] Find a tensor $\hat B$ of
rank at most $k$ satisfying
\[
||B-\hat{B}||_2\leq \frac{\epsilon}{2}||B||_F.
\]
\item[3.] Let $z_j = y_j\alpha_j$, for $y \in S$, so that
\[
A(y,\ldots,y) = B(z,\ldots,z).
\]
\item[4.] Solve
\[
\max_{z: y_j\in {\bf S}_1} \hat{B}(z,z,\ldots,z)
\]
to within
additive error $\epsilon |\alpha|^r ||B||_F/2$.
\end{enumerate}
\end{tt}
\end{minipage}
}}
\end{center}

The error of approximating $B$ by $\hat B$ is bounded by
\begin{eqnarray*}
&&\max_{z\in {\bf S}_1} |(B-\hat{B})(z,\ldots,z)| \\
&&\le
\max_{z: |z| \le |\alpha|\}} |(B-\hat{B})(z,\ldots,z)|\\
&&\le |\alpha|^{r} ||B-\hat{B}||_2 \\
&&\leq \epsilon |\alpha|^{r}  ||B||_F\\
&&\leq \epsilon (\sum_{i=1}^n (\bar D+D_i))^{r/2}
\left(\sum_{i_1,\ldots,i_r} \frac{A_{i_1,\ldots,i_r}^2}{\prod_{j=1}^r D_{i_j}}\right)^{1/2}\\
&&\leq \epsilon 2^{r/2}c(\sum_{i=1}^n D_i)
\end{eqnarray*}
where $c$ is the bound on the core-strength, noting that
$\sum_i (\bar D+D_i) = 2\sum_i D_i$.

\subsection{Optimizing constant-rank tensors}

From the above it suffices to deal with a tensor of constant
rank. Let $A$ be a tensor of dimension $r$ and rank $\ell$, say:
$$
A=\sum_{1\le j\le\ell}A^{(j)}
$$ with
$$
A^{(j)}=a_j x^{(j,1)}\otimes x^{(j,2)}...\otimes x^{(j,r)}
$$
where the $x^{(j,i)}\in {\bf R}^{2n}$ are length one vectors and
moreover we have that $||A^{(j))}||_F\le ||A||_F$ and $\ell=
O(\epsilon^{-2})$. We want to maximize approximately $B(y,y,\cdots
y)$, over the set of vectors $y$ satisfying for each $i\le n$
either $(y_i,y_{n+i})=(0,\alpha_{n+i})$ or
$(y_i,y_{n+i})=(\alpha_i,0)$ where $\alpha$ is a given
$2n$-dimensional positive vector. Let us define the tensor $B$ by
 $$
 B_{i_1,i_2,...i_r}=\alpha_{i_1}\alpha_{i_2},...\alpha_{i_r}A_{i_1,i_2,...i_r}
 ~\forall~ i_1,i_2,...i_r\in V.
 $$
 Then, with $y_j=\alpha_jx_j$, we have that
 $$
 B(x,x,...x)=A(y,y,...y).
 $$
Thus, we can as well maximize approximately $B$ now
 for $y$ in ${\bf S}$.
 We have
\begin{equation}\label{insert}
B(y,y,\cdots y)=\sum_{j=1}^{\ell}
a_j\left(\prod_{k=1}^r(z^{(j,k)}\cdot y\right)
\end{equation}
 with
 $$
z^{(j,r)}=\alpha^Tx^{(j,r)},~1\le j\le\ell,~1\le k\le r.
$$
 Similarly as in the 2-dimensional case, $B(y,y,\cdots y)$ depends
really only on the $\ell r$ variables $u_{j,i}$, say, where
$u_{j,i}=z^{(j,i)}\cdot y,~j=1,2,...,\ell,~i=1,2,...,r,$ and the
values of each of these products are confined to the interval
$[-2|\alpha|,+2|\alpha|]$. Then, exactly similarly as in the
2-dimensional case, we can get in polynomial time approximate
values for the $u_{j,i}$ within $\epsilon|\alpha|$ from the optimal
ones. Inserting then these values in
(\ref{insert}) gives an approximation of $\max B(y)$ with additive
error $O\left(\epsilon|\alpha|^r||B||_F\right)$ which is what we
need (taking $A={\hat B}$ of the previous subsection.)

\section{Metric tensors}\label{METRIC}

\begin{lemma}\label{lem:LOCAL}
Let $A$ be an $r$-dimensional tensor satisfying the following local density condition:
\[
\forall \, i_1, \ldots, i_r \in V, \quad A_{i_1,\ldots,i_r} \le \frac{c}{rn^{r-1}} \sum_{j=1}^r
D_{i_j}
\]
where $c$ is a constant. Then $A$ is a core-dense hypergraph with core-strength $c$.
\end{lemma}

\begin{proof}
We need to bound the core-strength of $A$. To this end,
\begin{eqnarray*}
&&\sum_{i_1, i_2, \ldots, i_r \in V}
\frac{A_{i_1,\ldots,i_r}^2}{\prod_{j=1}^r (D_{i_j}+\bar{D})} \\
&&\le
\frac{c}{rn^{r-1}}\sum_{i_1, i_2, \ldots, i_r \in V}
\frac{A_{i_1,\ldots,i_r} \sum_{j=1}^r D_{i_j}}{\prod_{j=1}^r (D_{i_j}+\bar{D})}\\
&&\le \frac{c}{rn^{r-1}}\sum_{i_1, i_2, \ldots, i_r \in V}
A_{i_1,\ldots,i_r} \sum_{j=1}^r \frac{1}{\prod_{k \in \{1,\ldots,r\}\setminus j} (D_{i_k}+\bar{D})}\\
&&\le \frac{c}{rn^{r-1}}\left(\sum_{i_1, i_2, \ldots, i_r \in E}
A_{i_1,\ldots,i_r}\right)\frac{r}{\bar{D}^{r-1}}\\
&&= \frac{c}{(\sum_{i=1}^n D_i)^{r-2}}.
\end{eqnarray*}

Thus, the core-strength is at most
\[
(\sum_{i=1}^n D_i)^{r-2}\sum_{i_1, i_2, \ldots, i_r \in E}
\frac{A_{i_1,\ldots,i_r}^2}{\Pi_{j=1}^r (D_{i_j}+\bar{D})} \leq c.
\]
\end{proof}

Theorem \ref{LOCAL} follows directly from Lemma \ref{lem:LOCAL} and
Theorem \ref{thm:coredense}. We next prove Corollary \ref{QM} for metrics.

\begin{proof}(of Corollary \ref{QM})
For $r=2$, the condition of Theorem \ref{LOCAL} says that for any $i,j \in V$,
\[
A_{i,j} \leq \frac{c}{2n} (D_i+D_j).
\]
We will verify that this holds for a metric MAX-$2$CSP with $c=2$.
When the entries of $A$ form a metric, for any $i,j,k$, we have
\[
A_{i,j} \leq A_{i,k} + A_{k,j}
\]
and so
\begin{eqnarray*}
A_{i,j} &\leq& \frac{1}{n} \left(\sum_{k=1}^n A_{i,k}+ \sum_{k=1}^n A_{j,k}\right)\\
&=& \frac{1}{n} (D_i+D_j).
\end{eqnarray*}
\end{proof}

A nonnegative real function $d$ defined on $M \times M$ is called a \textit{quasimetric}
%(cf. \cite{MS79}, \cite{S03}; \cite{MP00})
if
  $d(x,y)  =  0~ {\rm when} x=y$, $ d(x,y)  =  d(y,x)$ and
$d(x,z) \le C(d(x,y)+d(y,z))$,
the last for some positive real number $C$, and all $x,y,z \in M$.
Thus if it holds with $C=1$, then $d$
is a metric on $M$. The proof of Corollary \ref{QM} easily extends
to quasimetrics.

%An interesting
%property of a quasimetric $d(x,y)$ is that $d(x,y)^a$ is
%also a quasimetric for every
%positive real number $a$. (cf. \cite{MS79}).
Quasimetrics include a number of interesting
distance functions which are not metrics,
like the squares of Euclidean distances used in
clustering applications.
%We call an instance of weighted MAX-$r$CSP \textit{quasimetric}
%if the weight of each constraint $C(x_1,x_2,\ldots,x_r)$ is
%a linear function of its variables mutual quasimetric distances of $x_i$.
%% I could not follow the above sentence.



\section{Discussion}
This chapter is based on Fernandez de la Vega et al. \cite{FKKV05}.
Prior to that paper,
there was much progress on special cases. In particular, there
were polynomial-time approximation schemes for {\em dense} unweighted
problems \cite{AKK95, F96, FK96, GGR98,FK99,AFKK02},
and several cases of MAX-$2$CSP with metric weights including
maxcut and partitioning \cite{FK98, I99, FKKR03,
FKK04}. It is also shown in \cite{FKKV05} that these methods can be applied to $r$CSPs with an additional constant number of global constraints, such as finding the maximum weight bisection.


\part{Algorithms}




\chapter{Matrix Approximation via Random Sampling}\label{chap:mm}

In this chapter, we study randomized algorithms for matrix multiplication and low-rank approximation. The main motivation is to obtain efficient approximations using only randomly sampled subsets of given matrices. We remind the reader that for a vector-valued random variable $X$, we write
$\var(X)= \E(\|X-\E(X)\|^2)$ and similarly for a matrix-valued random variable, with the norm denoting the Frobenius norm in the latter case.

\section{Matrix-vector product}

In many numerical algorithms, a basic operation is the matrix-vector product. If $A$ is a $m\times n$
matrix and $v$ is an $n$ vector, we have ($A^{(j)}$ denotes the $j$'th column of $A$):
$$Av=\sum_{j=1}^n A^{(j)}v_j.$$
The right-hand side is the sum of $n$ vectors and can be estimated by using a sample of the $n$ vectors. The error is measured by the variance
of the estimate. It is easy to see that a uniform random sample could have high variance ---
consider the example when only one column is nonzero.

This leads to the question: what distribution should the sample columns be chosen from?
Let $p_1,p_2,\ldots p_n$ be nonnegative reals adding up to $1$. Pick $j\in \{ 1,2,\ldots n\}$ with
probability $p_j$ and consider the vector-valued random variable
$$X={A^{(j)}v_j\over p_j}.$$
Clearly $\E X=Av$, so $X$ is an unbiased estimator of $Av$. We also get
\begin{equation}\label{varAv}
\var (X) = \E \|X\|^2-\|\E X\|^2 = \sum_{j=1}^n {\|A^{(j)}\|^2v_j^2\over p_j} - \|Av\|^2.
\end{equation}
Now we introduce an important probability distribution on the columns of a matrix $A$, namely
the {\bf length-squared} (LS) distribution, where a column is picked with probability proportional
to its squared length. We will say
$$j \mbox{ is drawn from } \hbox{LS}_{\hbox{col}}(A)\quad \hbox{   if   } \quad p_j=\|A^{(j)}\|^2/\|A\|_F^2.$$
This distribution has useful properties.
%Many variance expressions will
%greatly simplify with this distribution. More importantly, we will see that
%this distribution minimizes the variance for an estimator of the matrix product $AA^T$.
An {\em approximate} version of this distribution - LS$_{\hbox{col}}(A,c)$, where we only require
that $$p_j\geq c\|A^{(j)}\|^2/\|A\|_F^2$$
for some $c\in (0,1)$
also shares interesting properties.
If $j$ is from $\hbox{LS}_{col}(A,c)$, then note that the expression (\ref{varAv}) simplifies to
yield
\[
\var X\leq \frac{1}{c}\|A\|_F^2\|v\|^2.
\]
Taking the average of $s$ i.i.d. trials
decreases the variance by a factor of $s$.
So, if we take $s$ independent samples
$j_1,j_2,\ldots j_s$ (i.i.d., each picked according to LS$_{\hbox{col}}(A,c)$), then with
\[
Y={1\over s}\sum_{t=1}^s {A^{(j_t)}v_{j_t}\over p_{j_t}},
\]
we have
\[
\E Y=Av
\]
and
\begin{eqnarray}\label{300}
\var Y={1\over s} \sum_j {\|A^{(j)}\|^2v_j^2\over p_j}-{1\over s}\|Av\|^2
\leq {1\over cs}\|A\|_F^2\|v\|^2.
\end{eqnarray}

Such an approximation for matrix vector products is useful only when $\|Av\|$ is comparable to
$\|A\|_F\|v\|$. It is greater value for matrix multiplication.

In certain contexts, it may be easier to sample
according to LS$(A,c)$ than the exact length squared distribution. We have used the subscript
$_{\hbox{col}}$ to denote that we sample columns of $A$; it will be sometimes useful to
sample rows, again with probabilities proportional to the length squared (of the row, now).
In that case, we use the subscript $_{\hbox{row}}$.


\section{Matrix Multiplication}

The next basic problem is that of multiplying two matrices, $A,B$, where
$A$ is $m\times n$ and $B$ is $n\times p$. From the definition of matrix multiplication, we have
$$AB=\left( AB^{(1)},AB^{(2)},\ldots AB^{(p)}\right).$$
Applying (\ref{300}) $p$ times and adding, we get the next theorem (recall the notation that
$B_{(j)}$ denotes row $j$ of $B$).
\begin{theorem}\label{matrixmult}
Let $p_1,p_2,\ldots p_n$
be non-negative reals summing to 1 and let $j_1,j_2,\ldots j_s$ be i.i.d. random variables,
where $j_t$ is picked to be one of $\{ 1,2,\ldots n\}$ with probabilities $p_1,p_2,\ldots
p_n$ respectively. Then with
\begin{eqnarray}
Y&=&{1\over s} \sum_{t=1}^s {A^{(j_t)}B_{(j_t)}\over p_{j_t}},\nonumber\\
\E Y&=&AB\qquad \mbox{ and }\qquad \var Y= {1\over s}\sum_{j=1}^n {\|A^{(j)}\|^2\|B_{(j)}\|^2\over p_j} - \|AB\|_F^2.\label{400}
\end{eqnarray}
If $j_t$ are distributed according to LS$_{\hbox{col}}(A,c)$, then
$$\var Y\leq {1\over cs} \|A\|_F^2\|B\|_F^2.$$
\end{theorem}

%%Say when useful

A special case of matrix multiplication which is both theoretically and practically useful
is the product $AA^T$.

%%***Practical use ------ draw a sentence from DK and Cohen et al.***

The singular values of $AA^T$ are just the squares of the singular values
of $A$. So it can be shown that if $B\approx AA^T$, then the eigenvalues of $B$ will approximate the
squared singular values of $A$. Later, we will want to approximate $A$ itself
well. For this, we will need in a sense a good approximation to not only the singular values,
but also the singular vectors of $A$. This is a more difficult problem. However, approximating
the singular values well via $AA^T$ will be a crucial starting point for the more difficult
problem.

For the matrix product $AA^T$, the expression for $\var Y$ (in (\ref{400}))
simplifies to
$$\var Y = {1\over s}\sum_j {\|A^{(j)}\|^4\over p_j}-\|AA^T\|_F^2.$$
The second term on the right-hand side is independent of $p_j$.
The first term is minimized when the $p_j$ conform to the length-squared
distribution. The next exercise establishes
the optimality of the length-squared distribution.

\begin{exercise}
Suppose $a_1,a_2,\ldots a_n$ are fixed positive reals. Prove that the minimum
of the constrained optimization problem
$$\Min \sum_{j=1}^n {a_j\over x_j}\hbox{  subject to  } x_j\geq 0\; ;\; \sum_j x_j=1$$
is attained at $x_j=\sqrt{a_j}/\sum_{i=1}^n\sqrt{ a_i}$.
\end{exercise}

\section{Low-rank approximation}

When $B=A^T$, we may rewrite the expression (\ref{400}) as
$$Y=CC^T,\quad\hbox{  where,   } C={1\over\sqrt s}\left( {A^{(j_1)}\over \sqrt{p_{j_1}}},{A^{(j_2)}\over \sqrt{p_{j_2}}},\ldots {A^{(j_s)}\over \sqrt{p_{j_s}}}\right)$$
and the next theorem follows.
\begin{theorem}\label{thm:sampleAAT}
Let $A$ be an $m\times n$ matrix and $j_1,j_2,\ldots j_s$ be i.i.d. samples from $\{ 1,2,\ldots n\}$,
each picked according to probabilities $p_1,p_2,\ldots p_n$. Define
$$C={1\over\sqrt s}\left( {A^{(j_1)}\over \sqrt{p_{j_1}}},{A^{(j_2)}\over \sqrt{p_{j_2}}},\ldots {A^{(j_s)}\over \sqrt{p_{j_s}}}\right).$$
Then,
$$\E CC^T=AA^T\qquad \mbox{and}\qquad \E \|CC^T-AA^T\|_F^2 = {1\over s}\sum_{j=1}^n{|A^{(j)}|^4\over p_j}-{1\over s} \|AA^T\|_F^2.$$
If the $p_j$'s conform to the approximate length squared distribution LS$_{\hbox{col}}(A,c)$, then
$$\E \|CC^T-AA^T\|_F^2\leq {1\over cs}\|A\|_F^4.$$
\end{theorem}


The fact that $\|CC^T-AA^T\|_F$ is small implies that the singular values of $A$
are close to the singular values of $C$. Indeed the Hoffman-Wielandt inequality asserts that
\begin{equation}\label{hw}
\sum_t \left( \sigma_t(CC^T)-\sigma_t(AA^T) \right)^2\leq \|CC^T-AA^T\|_F^2.
\end{equation}
(Exercise \ref{ex:hw} asks for a proof of this inequality.)

To obtain a good low-rank approximation of $A$, we will also need a handle on the singular vectors of $A$. A natural question is whether the columns of $C$ already contain a good low-rank approximation to $A$.
To this end, first observe that if $u^{(1)},u^{(2)},\ldots
u^{(k)}$ are orthonormal vectors in ${\bf R}^m$, then
$$\sum_{t=1}^ku^{(t)}u^{(t)^T}A$$
is the projection of $A$ into the space $H$ spanned by $u^{(1)},u^{(2)},\ldots
u^{(k)}$, namely
\begin{enumerate}
\item[(i)] For any $u\in H$, $u^TA=u^T\sum_{t=1}^ku^{(t)}u^{(t)^T}A$ and

\item[(ii)] For any $u\in H^\perp$, $u^T\sum_{t=1}^ku^{(t)}u^{(t)^T}A=0$.
\end{enumerate}
This motivates the following algorithm for low-rank approximation.

\begin{center}
\fbox{\parbox{4.7in}{
\begin{minipage}{4.5in}
\begin{tt}
{\bf Algorithm: Fast-SVD}

\begin{enumerate}
\item[1.] Sample $s$ columns of $A$ from the squared length distribution to form a matrix $C$.
\item[2.] Find $u^{(1)}, \ldots, u^{(k)}$, the top $k$ left singular vectors of $C$.
\item[3.] Output $\sum_{t=1}^k u^{(t)}u^{(t)^T}A$ as a rank-k approximation to $A$.
\end{enumerate}
\end{tt}
\end{minipage}
}}
\end{center}

The running time of the algorithm (if it uses $s$ samples) is $O(ms^2)$.

We now state and prove the main lemma of this section. Recall that $A_k$ stands for the best rank-$k$ approximation to $A$ (in Frobenius norm and $2$-norm) and is given by the first $k$ terms of the SVD.

\begin{lemma}\label{svdC}
Suppose $A,C$ are $m\times n$ and $m\times s$ matrices respectively with $s\leq n$ and $U$ is the  $m\times k$ matrix consisting of the top $k$ singular vectors of $C$. Then,
\begin{eqnarray*}
\|A-UU^TA\|_F^2&\leq& \|A-A_k\|_F^2+2\sqrt{k}\|AA^T-CC^T\|_F\\
\|A-UU^TA\|_2^2&\leq& \|A-A_k\|_2+\|CC^T-AA^T\|_2+\|CC^T-AA^T\|_F.
\end{eqnarray*}
\end{lemma}

\begin{proof}
We have
\[
\|A-\sum_{t=1}^k u^{(t)}u^{(t)^T}A\|_F^2=\|A\|_F^2 - \|U^TA\|_F^2
\]
and
\[
\|C_k\|_F^2 = \|U^TC\|_F^2.
\]
Using these equations,
\begin{eqnarray*}
&&\|A-\sum_{t=1}^k u^{(t)}u^{(t)^T}A\|_F^2 - \|A-A_k\|_F^2 \\
&=&\|A\|_F^2 - \|U^TA\|_F^2 - (\|A\|_F^2 - \|A_k\|_F^2)\\
&=&\left(\|A_k\|_F^2- \|C_k\|_F^2\right)+\|U^TC\|_F^2-\|U^TA\|_F^2\\
&=&\sum_{t=1}^k\left(\sigma_t(A)^2 - \sigma_t(C)^2\right)
+\sum_{t=1}^k\left(\sigma_t(C)^2-\|{u^{(t)}}^TA\|^2\right)\\
&\le&\sqrt{k\sum_{t=1}^k\left(\sigma_t(A)^2 - \sigma_t(C)^2\right)^2}+\sqrt{k\sum_{t=1}^k\left(\sigma_t(C)^2-\|{u^{(t)}}^TA\|^2\right)^2}\\
&=& \sqrt{k\sum_{t=1}^k\left(\sigma_t(AA^T) - \sigma_t(CC^T)\right)^2}+\sqrt{k\sum_{t=1}^k\left({u^{(t)}}^T(CC^T-AA^T)u^{(t)}\right)^2}\\
&\le& 2\sqrt{k}\|AA^T-CC^T\|_F.
\end{eqnarray*}
Here we first used the Cauchy-Schwarz inequality on both summations and then the Hoffman-Wielandt inequality \ref{hw}.

The proof of the second statement also uses the Hoffman-Wielandt inequality.
\end{proof}

We can now combine Theorem \ref{thm:sampleAAT} and Lemma \ref{svdC} to obtain the main theorem of this section.
\begin{theorem}
Algorithm Fast-SVD finds a rank-$k$ matrix $\tilde{A}$ such that
$$\E\left(\|A-\tilde{A}\|_F^2\right)\leq \|A-A_k\|_F^2+2\sqrt{\frac{k}{s}}\|A\|_F^2$$
$$\E\left(\|A-\tilde{A}\|_2^2\right)\leq \|A-A_k\|_2+\frac{2}{\sqrt{s}}\|A\|_F^2.$$
\end{theorem}

% I feel like these excersises don't contribute to understanding the rest of the section. (although they are cute excersises)
% I find problems that slightly alters the premise of one of the preceding theorems or lemmas and requires the proof of a slightly different result to be more useful. (you have a few of these types elsewhere)

\begin{exercise}\label{trace-norm}
Using the fact that $\|A\|_F^2 = \tr(AA^T)$ show that:
\begin{enumerate}
\item For any two matrices $P,Q$, we have $|\tr PQ|\leq \|P\|_F\|Q\|_F$.
\item For any matrix $Y$ and any symmetric matrix $X$, $|\tr XYX|\leq \|X\|_F^2\|Y\|_F$.
\end{enumerate}
\end{exercise}


\begin{exercise}\label{ex:hw}
Prove the Hoffman-Wielandt inequality for symmetric matrices: for any two $n \times n$ symmetric matrices $A$ and $B$,
\[
\sum_{t=1}^n \left(\sigma_t(A)-\sigma_t(B)\right)^2 \le \|A-B\|_F^2.
\]
(Hint: consider  the SVD of both matrices and note that any doubly stochastic matrix is a convex combination of permutation matrices).
\end{exercise}

\begin{exercise}\label{ex:on-the-fly}
(Sampling on the fly)
Suppose you are reading a list of real
numbers $a_1,a_2,\ldots a_n$ in a streaming fashion, i.e., you only have $O(1)$ memory
and the input data comes in arbitrary order in a stream.
Your goal is to output a number $X$ between $1$ and $n$ such that:
$$\prob (X=i) = {a_i^2\over \sum_{j=1}^n a_j^2}.$$
How would you do this? How would you pick values for
$X_1,X_2,\ldots X_s$ ($s\in O(1)$) where the $X_i$ are i.i.d.?
\end{exercise}

%% Put some section title here (to demark the transition between excersises and the next block of material)

%% Why is "good right project" quoted below? Maybe define what a good projection means.

In this section, we considered projection to the span of a set of orthogonal vectors (when the $u^{(t)}$ form the top $k$ left singular vectors of $C$).
In the next section, we will need to deal also with the case when the $u^{(t)}$ are not orthonormal.
A prime example we will deal with is the
following scenario: suppose $C$ is an $m\times s$ matrix, for example
obtained by sampling $s$ columns of $A$ as above. Now suppose
$v^{(1)},v^{(2)},\ldots v^{(k)}$ are indeed an orthonormal set of vectors for which
$C\approx C\sum_{t=1}^kv^{(t)}v^{(t)^T}$; i.e., $\sum_{t=1}^kv^{(t)}v^{(t)^T}$ is a ``good right projection'' space for $C$. Then suppose the $u^{(t)}$ are defined by $u^{(t)}=Cv^{(t)}/|Cv^{(t)}|$. We will see later that $C\approx \sum_{t=1}^ku^{(t)}u^{(t)^T}C$; i.e., that $\sum_{t=1}^ku^{(t)}u^{(t)^T}$ is a good left projection space for $C$.
The following lemma which generalizes some of the arguments we have used here will be useful in this regard.

\begin{lemma}\label{uuTAC}
Suppose $u^{(1)},u^{(2)},\ldots u^{(k)}$ are any $k$ vectors in ${\bf R}^m$. Suppose $A,C$ are any two
matrices, each with $m$ rows (and possibly different numbers of columns.) Then, we have
\begin{eqnarray}
&&\|A-\sum_{t=1}^ku^{(t)}u^{(t)^T}A\|_F^2-\|C-\sum_{t=1}^k u^{(t)}u^{(t)^T}C\|_F^2\nonumber\\
&\le& \|A\|_F^2-\|C\|_F^2\nonumber\\
&+& \|AA^T-CC^T\|_F\|\sum_{t=1}^ku^{(t)}u^{(t)^T}\|_F
\left( 2+\|\sum_{t=1}^ku^{(t)}u^{(t)^T}\|_F\right)\label{uuTAC1}\\
&&\|A-\sum_{t=1}^ku^{(t)}u^{(t)^T}A\|_2^2-\|C-\sum_{t=1}^ku^{(t)}u^{(t)^T}C\|_2^2\nonumber\\
&\le&
\|AA^T-CC^T\|_2\left( \|\sum_{t=1}^ku^{(t)}u^{(t)^T}\|_2+1\right)^2\label{uuTAC2}.
\end{eqnarray}
\end{lemma}

\begin{proof}
\begin{eqnarray*}
&&\|A-\sum_{t=1}^k u^{(t)}u^{(t)^T}A\|_F^2\\
&=&\tr \left( (A-\sum_{t=1}^ku^{(t)}u^{(t)^T}A)(A^T-A^T\sum_{t=1}^ku^{(t)}u^{(t)^T})\right)\\
&=& \tr AA^T +\tr \sum_{t=1}^ku^{(t)}u^{(t)^T}AA^T\sum_{t=1}^ku^{(t)}u^{(t)^T}-2\tr \sum_{t=1}^ku^{(t)}u^{(t)^T}AA^T,
\end{eqnarray*}
where we have used the fact that square matrices commute under trace.
We do the same expansion for $C$ to get
\begin{eqnarray*}
&&\|A-\sum_{t=1}^ku^{(t)}u^{(t)^T}A\|_F^2-\|C-\sum_{t=1}^ku^{(t)}u^{(t)^T}C\|_F^2 -\left(\|A\|_F^2-\|C\|_F^2\right)\\
&=& \tr \sum_{t=1}^ku^{(t)}u^{(t)^T}(AA^T-CC^T)\sum_{t=1}^ku^{(t)}u^{(t)^T}-2\tr \sum_{t=1}^ku^{(t)}u^{(t)^T}(AA^T-CC^T)\\
&\leq& \|\sum_{t=1}^ku^{(t)}u^{(t)^T}\|_F^2\|AA^T-CC^T\|_F+2\|\sum_{t=1}^ku^{(t)}u^{(t)^T}\|_F\|AA^T-CC^T\|_F,
\end{eqnarray*}
where we have used two standard inequalities: $|\tr PQ|\leq \|P\|_F\|Q\|_F$ for any matrices $P,Q$
and $|\tr XYX|\leq \|X\|_F^2\|Y\|_F$ for any $Y$ and a symmetric matrix $X$ (see Exercise \ref{trace-norm}).
This gives us (\ref{uuTAC1}).

For (\ref{uuTAC2}), suppose $v$ is the unit length vector achieving
\[
\|v^T(A-\sum_{t=1}^ku^{(t)}u^{(t)^T}A)\|=
\|A-\sum_{t=1}^ku^{(t)}u^{(t)^T}A\|_2.
\]
Then we expand
\begin{eqnarray*}
&&\|v^T(A-\sum_{t=1}^ku^{(t)}u^{(t)^T}A)\|^2\\
&=&v^T(A-\sum_{t=1}^ku^{(t)}u^{(t)^T}A)(A^T-A^T\sum_{t=1}^ku^{(t)}u^{(t)^T})v\\
&=&v^TAA^Tv-2v^TAA^T\sum_{t=1}^ku^{(t)}u^{(t)^T}v+v^T\sum_{t=1}^ku^{(t)}u^{(t)^T}AA^T\sum_{t=1}^ku^{(t)}u^{(t)^T}v,
\end{eqnarray*}
and the corresponding terms for $C$. Now, (\ref{uuTAC2}) follows by a somewhat tedious but routine calculation.
\end{proof}


\section{Invariant subspaces}

The classical SVD has associated with it the decomposition of space into the {\bf direct sum}
of {\bf invariant subspaces}.

% Explain what "direct sum of invariant subspaces" means.

\begin{theorem} \label{invariant}
Let $A$ be a $m\times n$ matrix and $v^{(1)},v^{(2)},\ldots v^{(n)}$ an
orthonormal basis for ${\bf R}^n$. Suppose for $k, 1\leq k\leq \hbox{rank}(A)$ we have
$$ |Av^{(t)}|^2=\sigma_t^2(A),\quad\for t=1,2,,\ldots k.$$
Then $$u^{(t)}={Av^{(t)}\over |Av^{(t)}|},\quad\for t=1,2,\ldots k$$
form an orthonormal
family of vectors. The following hold:
\begin{eqnarray*}
\sum_{t=1}^k |u^{(t)^T}A|^2&=&\sum_{t=1}^k\sigma_t^2\\
\|A-A\sum_{t=1}^kv^{(t)}v^{(t)^T}\|_F^2 &=& \|A-\sum_{t=1}^ku^{(t)}u^{(t)^T}A\|_F^2\\
&=&\sum_{t=k+1}^n\sigma_t^2(A)\\
\|A-A\sum_{t=1}^kv^{(t)}v^{(t)^T}\|_2 &=& \|A-\sum_{t=1}^ku^{(t)}u^{(t)^T}A\|_2=\sigma_{k+1}(A).
\end{eqnarray*}
\end{theorem}

Given the right singular vectors $v^{(t)}$, a family of left singular vectors
$u^{(t)}$ may be found by just applying $A$ to them and scaling to length 1. The
orthogonality of the $u^{(t)}$ is automatically ensured. So we get that given the optimal
$k$ dimensional ``right projection'' $A\sum_{t=1}^k v^{(t)}v^{(t)^T}$, we also can get the
optimal ``left projection'' $$\sum_{t=1}^k u^{(t)}u^{(t)^T}A.$$
Counting dimensions, it also
follows that for any vector $w$ orthogonal to such a set of $v^{(1)},v^{(2)},\ldots v^{(k)}$,
we have that $Aw$ is orthogonal to $u^{(1)},u^{(2)},\ldots u^{(k)}$. This yields the standard decomposition into the direct sum of subspaces.

\begin{exercise}
Prove Theorem \ref{invariant}.
\end{exercise}

\subsection{Approximate invariance}

The theorem below proves that even if the hypothesis of the previous theorem
$ |Av^{(t)}|^2= \sigma_t^2(A)$ is only approximately satisfied, an
approximate conclusion follows.
We give below a fairly clean statement and proof formalizing this intuition.
It will be useful to define the error measure
\begin{eqnarray}
\Delta(A,v^{(1)},v^{(2)},\ldots v^{(k)})&=&\hbox{Max}_{1 \le t \le k} \sum_{i=1}^t(\sigma_i^2(A) -|Av^{(i)}|^2)\label{7000}.
\end{eqnarray}

\begin{theorem} \label{invariant2}
Let $A$ be a matrix of rank $r$ and $v^{(1)},v^{(2)},\ldots v^{(r)}$ be an
orthonormal set of vectors spanning the row space of $A$ (so that $\{ Av^{(t)}\}$ span the column space of $A$). Then, for $t,1\leq t\leq r$, we have
\begin{eqnarray*}
&&\sum_{s=t+1}^r \left( v^{(t)^T}A^TAv^{(s)}\right)^2\\
&\leq &
|Av^{(t)}|^2\left(\sigma_1^2(A)+\sigma_2^2(A)+\ldots \sigma_t^2(A)-|Av^{(1)}|^2-|Av^{(2)}|^2-\ldots |Av^{(t)}|^2\right).
\end{eqnarray*}
\end{theorem}
 Note that $v^{(t)^T}A^TAv^{(s)}$ is the $(t,s)$ th entry of the matrix $A^TA$ when written with respect
 to the basis $\{ v^{(t)}\}$. So,
 the quantity $\sum_{s=t+1}^r \left( v^{(t)^T}A^TAv^{(s)}\right)^2$ is the sum of
 squares of the above diagonal entries of the $t$ th row of this matrix.
 Theorem (\ref{invariant2}) implies the classical Theorem (\ref{invariant}) : $\sigma_t(A)=|Av^{(t)}|$
 implies that the right hand side of the inequality above is zero. Thus,
 $v^{(t)^T}A^TA$ is colinear with $v^{(t)^T}$ and so $|v^{(t)^T}A^TA|=|Av^{(t)}|^2$ and so on.

\begin{proof} First consider the case when $t=1$. We have
 \begin{eqnarray}
\sum_{s=2}^r(v^{(1)^T}A^TAv^{(s)})^2 &=& |v^{(1)^T}A^TA|^2- (v^{(1)^T}A^TAv^{(1)})^2\nonumber\\
&\leq& |Av^{(1)}|^2\sigma_1(A)^2-|Av^{(1)}|^4\nonumber\\
&\leq& |Av^{(1)}|^2(\sigma_1(A)^2-|Av^{(1)}|^2)\label{600}.
\end{eqnarray}
The proof of the theorem will be by induction on the rank of $A$. If $r=1$, there is nothing to prove.
Assume $r\geq 2$.
Now, Let
$$A'=A-Av^{(1)}v^{(1)^T}.$$
$A'$ is of rank $r-1$.
If $w^{(1)},w^{(2)},\ldots $ are the right singular vectors of $A'$, they are clearly orthogonal to $v^{(1)}$.
So we have for any $s$, $1\leq s\leq r-1$,
\begin{eqnarray}
&&\sigma_1^2(A')+\sigma_2^2(A')+\ldots \sigma_s^2(A') =\sum_{t=1}^s|A'w^{(t)}|^2 =\sum_{t=1}^s|Aw^{(t)}|^2\nonumber\\
&& \quad= |Av^{(1)}|^2+\sum_{t=1}^s|Aw^{(t)}|^2-|Av^{(1)}|^2\nonumber\\
&&\quad\leq \Max_{u^{(1)},u^{(2)}\ldots u^{(s+1)}\atop \hbox{orthonormal}} \sum_{t=1}^{s+1}|Au^{(t)}|^2-|Av^{(1)}|^2\nonumber\\
&&\quad =\sigma_1(A)^2+\sigma_2(A)^2+\ldots \sigma_{s+1}(A)^2-|Av^{(1)}|^2\label{800},
\end{eqnarray}
where we have applied the fact that for any $k$, the $k$-dimensional SVD subspace maximizes the sum of squared projections among all subspaces of dimension at most $k$.
%Ky Fan's Maximum principle (special case of Wielandt's minimax, see for example, Bhatia %\cite{Bh}).

Now, we use the inductive assumption on $A'$ with the orthonormal basis
$v^{(2)},v^{(3)},\ldots v^{(r)}$. This yields for $t, 2\leq t\leq r$,
\begin{eqnarray*}
&&\sum_{s=t+1}^{r}( v^{(t)^T}A'^TA'v^{(s)})^2\\
&&\le |A'v^{(t)}|^2(\sigma^2_1(A')+\sigma^2_2(A')+\ldots \sigma_{t-1}^2(A')
-|A'v^{(2)}|^2-|A'v^{(3)}|^2-\ldots |A'v^{(t)}|^2)
\end{eqnarray*}
Note that for $t\geq 2,$ we have $A'v^{(t)}=Av^{(t)}$. So, we get using
(\ref{800})
\begin{eqnarray*}
&&\sum_{s=t+1}^{r}( v^{(t)^T}A^TAv^{(s)})^2\\
&&\le|Av^{(t)}|^2(\sigma^2_1(A)+\sigma^2_2(A)+\ldots \sigma_{t}^2(A)
-|Av^{(1)}|^2-|Av^{(2)}|^2-\ldots |Av^{(t)}|^2).
\end{eqnarray*}
This together with (\ref{600}) finishes the proof of the Theorem. %Is that the correct reference? I don't think so.
\end{proof}

%\begin{remark}
We will use Theorem (\ref{invariant2}) to prove Theorem (\ref{invariant3}) below.
Theorem (\ref{invariant3}) says that we can get good ``left projections'' from ``good
right projections''. One important difference from the exact case is that now we have to be
more careful of ``near singularities'', i.e. the upper bounds in the Theorem (\ref{invariant3})
will depend on a term
$$\sum_{t=1}^k{1\over |Av^{(t)}|^2}.$$
If some of the $|Av^{(t)}|$ are close to zero, this term is large and the bounds can become useless.
This is not just a technical problem. In defining $u^{(t)}$ in Theorem (\ref{invariant}) as
$Av^{(t)}/|Av^{(t)}|$, the hypotheses exclude $t$ for which the denominator is zero. Now
since we are dealing with approximations, it is not only the zero denominators that bother us, but also
small denominators. We will have to exclude these too (as in Corollary (\ref{invariant4}) below) to get
a reasonable bound.

%\end{remark}

\begin{theorem}\label{invariant3}
Suppose $A$ is a matrix and $v^{(1)},\ldots v^{(k)}$ are orthonormal and let
$\Delta=\Delta(A,v^{(1)},v^{(2)},\ldots v^{(k)})$ be as in (\ref{7000}). Let
$$u^{(t)} = {Av^{(t)}\over |Av^{(t)}|}\quad\for t=1,2,\ldots k.$$
Then
\begin{eqnarray*}
\|\sum_{t=1}^k u^{(t)}u^{(t)^T}A-A\|_F^2 &\le& \|A-\sum_{t=1}^k Av^{(t)}v^{(t)^T}\|^2_F\\
&&+\left(\sum_{t=1}^k{2\over |Av^{(t)}|^2}\right)\left(\sum_{t=1}^k |Av^{(t)}|^2\right)
 \Delta
\end{eqnarray*}
\begin{eqnarray*}
\|\sum_{t=1}^k u^{(t)}u^{(t)^T}A-A\|_2^2 &\le&    \|A-\sum_{t=1}^k Av^{(t)}v^{(t)^T}\|_2^2\\
&&+\left(\sum_{t=1}^k{2\over |Av^{(t)}|^2}\right)\left(\sum_{t=1}^k |Av^{(t)}|^2\right)
\Delta.
\end{eqnarray*}
\end{theorem}
\begin{proof}

Complete $\{ v^{(1)},v^{(2)},\ldots v^{(k)}\}$  to an orthonormal set
$\{ v^{(1)},v^{(2)},\ldots v^{(r)}\}$  such that $\{ Av^{(t)}:t=1,2,\ldots r\}$
span the range of $A$. Let
$$w^{(t)^T}=v^{(t)^T}A^TA-|Av^{(t)}|^2v^{(t)^T} $$
be the component of $v^{(t)^T}A^TA$ orthogonal to $v^{(t)^T}$.
We have $$u^{(t)}u^{(t)^T}A={Av^{(t)}v^{(t)^T}A^TA\over |Av^{(t)}|^2
}=Av^{(t)}v^{(t)^T}+Av^{(t)}w^{(t)^T}.$$
Using $||X+Y||_F^2=\tr ((X^T+Y^T)(X+Y))=||X||_F^2+||Y||_F^2+2\tr X^TY$
and the convention that $t$ runs over $1,2,\ldots k$, we have
\begin{eqnarray*}
&&||\sum_t u^{(t)}u^{(t)^T}A-A||_F^2=\left|\left|\sum_t Av^{(t)}v^{(t)^T}+\sum_t{Av^{(t)}w^{(t)^T}\over |Av^{(t)}|^2} -A\right|\right|_F^2\\
&=& ||A-\sum_t Av^{(t)}v^{(t)^T} ||_F^2+\left( \sum_t \left| {Av^{(t)}\over |Av^{(t)}|^2}\right|\left| w^{(t)}\right|\right)^2\\
&&\qquad -2\sum_{s=1}^r\sum_t(v^{(s)^T}w^{(t)}){v^{(t)^T}A^T\over |Av^{(t)}|^2}(A-\sum_t Av^{(t)}v^{(t)^T})v^{(s)}
\\
&\leq& ||A-\sum_t Av^{(t)}v^{(t)^T} ||_F^2+
\left( \sum_t|w^{(t)}|^2\right)\left(\sum_t{1\over |Av^{(t)}|^2}\right)
-2\sum_{s=k+1}^r\sum_t{(v^{(t)^T}A^TAv^{(s)})^2\over|Av^{(t)}|^2}\\
&&\qquad \hbox{since $(A-\sum_t Av^{(t)}v^{(t)^T})v^{(s)}=0\quad\for s\leq k$
and $v^{(s)^T}w^{(t)}=v^{(s)^T}A^TAv^{(t)}$}\\
&\leq& ||A-\sum_t Av^{(t)}v^{(t)^T} ||_F^2+\left(\sum_t{1\over |Av^{(t)}|^2}\right)
\left(2\sum_t\sum_{s=t+1}^r(v^{(t)^T}A^TAv^{(s)})^2\right)\\
&\leq& |A-\sum_t Av^{(t)}v^{(t)^T} ||_F^2+\left(\sum_t{2\over |Av^{(t)}|^2}\right)\left(\sum_t |Av^{(t)}|^2\right)
\Delta ,
\end{eqnarray*}
using Theorem (\ref{invariant2}).

For the 2-norm, the argument is similar. Suppose a vector $p$ achieves
\[
\|\sum_t u^{(t)}u^{(t)^T}A-A\|_2=|(\sum_t u^{(t)}u^{(t)^T}A-A)p|.
\]
We now use
\[
|(X+Y)p|^2=p^TX^TXp+p^TY^TYp+2p^TX^TYp
\]
to get
\begin{eqnarray*}
&&||\sum_t u^{(t)}u^{(t)^T}A-A||_2^2
\leq ||A-\sum_t Av^{(t)}v^{(t)^T} ||_2^2
\\&&+
\left( \sum_t|w^{(t)}|^2\right)\left(\sum_t{1\over |Av^{(t)}|^2}\right)
-2\sum_t(p^Tw^{(t)}){v^{(t)^T}A^T\over |Av^{(t)}|^2}(A-\sum_t Av^{(t)}v^{(t)^T})p.
\end{eqnarray*}
If now we write $p=p^{(1)}+p^{(2)}$, where $p^{(1)}$ is the component of $p$ in the span of $v^{(1)},v^{(2)},\ldots v^{(k)}$, then we have
\begin{eqnarray*}
\sum_t(p^Tw^{(t)}){v^{(t)^T}A^T\over |Av^{(t)}|^2}(A-\sum_t Av^{(t)}v^{(t)^T})p
&=& \sum_t(p^{(2)^T}w^{(t)}){v^{(t)^T}A^T\over |Av^{(t)}|^2}Ap^{(2)}\\
&=&{\sum_t (v^{(t)^T}A^TAp^{(2)})^2\over |Av^{(t)}|^2},
\end{eqnarray*}
where we have used the fact that $p^{(2)}$ is orthogonal to $v^{(t)}$ to get $p^{(2)^T}w^{(t)}=v^{(t)^T}A^TAp^{(2)}$.

\end{proof}

We will apply the Theorem as follows. As remarked earlier, we have to be careful about near singularities. Thus while we seek a good approximation of rank $k$ or less, we cannot automatically take all of the $k$ terms. Indeed, we only take terms for which $|Av^{(t)}|$ is at least a certain threshold.

\begin{corollary}\label{invariant4}
Suppose $A$ is a matrix, $\delta$ a positive real and $v^{(1)},\ldots v^{(k)}$ are orthonormal vectors produced
by a randomized algorithm and suppose
$$\E\left(\sum_{j=1}^t\left( \sigma_j^2(A)-|Av^{(j)}|^2\right)\right)\leq\delta ||A||_F^2\quad
t=1,2,\ldots k.$$
Let
$$u^{(t)} = {Av^{(t)}\over |Av^{(t)}|}\quad\for t=1,2,\ldots k.$$
Define $l$ to be the largest integer in $\{ 1,2,\ldots k\}$ such that
$|Av^{(l)}|^2\geq \sqrt\delta ||A||_F^2$. Then,
$$\E||A-\sum_{t=1}^lu^{(t)}u^{(t)^T}A||_F^2\leq \E||A-A\sum_{t=1}^kv^{(t)}v^{(t)^T}||_F^2+
3 k\sqrt\delta||A||_F^2.$$
$$\E||A-\sum_{t=1}^lu^{(t)}u^{(t)^T}A||_2^2\leq \E||A-A\sum_{t=1}^kv^{(t)}v^{(t)^T}||_2^2+
3 k\sqrt\delta||A||_F^2$$
       \end{corollary}
\begin{proof}
We apply the Theorem with $k$ replaced by $l$ and taking expectations of both sides (which are now random
variables) to get
\begin{eqnarray*}
&&\E||A-\sum_{t=1}^lu^{(t)}u^{(t)^T}||_F^2\leq \E||A-A\sum_{t=1}^lv^{(t)}v^{(t)^T}||_F^2+\\
&&+{2k\over\sqrt\delta}
\E\left( \sum_{t=1}^l\left( \sigma_t^2(A)-|Av^{(t)}|^2\right)\right)\\
&\leq& \E||A-A\sum_{t=1}^kv^{(t)}v^{(t)^T}||_F^2+\sum_{t=l+1}^k|Av^{(t)}|^2+2k\sqrt\delta||A||_F^2,
\end{eqnarray*}
where, we have used the fact that from the minimax principle and
$|Av^{(1)}|\geq |Av^{(2)}|\geq\ldots |Av^{(k)}|>0$, we get that
$\sigma_t(A)\geq |Av^{(t)}|$ for $t=1,2,\ldots k$. Now first assertion in the Corollary follows.
For the 2-norm bound, the proof is similar. Now we use the fact that
$$||A-A\sum_{t=1}^lv^{(t)}v^{(t)^T}||_2^2\leq ||A-A\sum_{t=1}^kv^{(t)}v^{(t)^T}||_2^2+\sum_{t=l+1}^k|Av^{(t)}|^2.$$
To see this, if $p$ is the top left singular vector of $A-A\sum_{t=1}^lv^{(t)}v^{(t)^T}$, then
\begin{eqnarray*}
|p^T(A-A\sum_{t=1}^lv^{(t)}v^{(t)^T})|^2 &=& p^TAA^Tp-p^TA\sum_{t=1}^lv^{(t)}v^{(t)^T}A^Tp\\
&\leq&
||A-A\sum_{t=1}^kv^{(t)}v^{(t)^T}||_2^2+\sum_{t=l+1}^k|p^TAv^{(t)}|^2.
\end{eqnarray*}


\end{proof}


\section{SVD by sampling rows and columns}

Suppose $A$ is an $m\times n$ matrix and $\epsilon >0$
and $c$ a real number in $[0,1]$. In this section, we will use several constants
which we denote $c_1,c_2\ldots$ which we do not specify.

We pick a sample of
$$s={c_1k^5\over c\epsilon^4}$$
columns of $A$ according to LS$_{\hbox{col}}(A,c)$ and scale to form an $m\times s$ matrix $C$. Then we sample a set of
$s$
rows of $C$ according to a LS$_{\hbox{row}}(C,c)$ distribution to form a $s\times s$ matrix $W$. By Theorem \ref{thm:sampleAAT}, we have
\begin{equation}\label{C-W}
\E||C^TC-W^TW||_F\leq {1\over \sqrt{cs}}\E||C||_F^2= {c_2\epsilon^2\over k^{2.5}}||A||_F^2,
\end{equation}
where we have used H\"{o}lder's inequality ($\E X\leq (\E X^2)^{1/2}$) and the fact that
$\E||C||_F^2=\E\tr (CC^T)=\tr (AA^T)$.

We now find the SVD of $W^TW$, (note : This is just an $s\times s$ matrix !) say
$$W^TW=\sum_t \sigma_t^2(W)v^{(t)}v^{(t)^T}.$$

We first wish to claim that $\sum_{t=1}^kv^{(t)}v^{(t)^T}$ forms a ``good right projection'' for $C$. This
follows from Lemma (\ref{svdC}) with
$C$ replacing $A$ and $W$ replacing $C$ in that Lemma and right projections instead of left projections. Hence
we get (using (\ref{C-W}))
\begin{eqnarray}
\E||C-C\sum_{t=1}^kv^{(t)}v^{(t)^T}||_F^2&\leq& \E||C||_F^2-\E\sum_{t=1}^k\sigma_t^2(C)
         +{c_3\epsilon^2\over k^2}||A||_F^2\\
\E||C-C\sum_{t=1}^kv^{(t)}v^{(t)^T}||_2^2&\leq&
\E\sigma_{k+1}(C)^2+(2+4k)O(\frac{\epsilon^2}{k^3})\E||C||_F^2\\
&\leq& \sigma_{k+1}^2(A)+{c_4\epsilon^2\over k^2}||A||_F^2\label{401}.
 \end{eqnarray}
Since $||C-C\sum_{t=1}^kv^{(t)}v^{(t)^T}||_F^2=||C||_F^2-\sum_{t=1}^k|Cv^{(t)}|^2$, we get
from (\ref{401})
\begin{equation}\label{constsvd1}
\E\sum_{t=1}^k\left( \sigma_t^2(C)-|Cv^{(t)}|^2\right) \leq
{c_5\epsilon^2\over k^2} ||A||_F^2.
\end{equation}
(\ref{401}) also yields
\begin{eqnarray}
\E||C-C\sum_{t=1}^kv^{(t)}v^{(t)^T}||_F^2&\leq& ||A||_F^2-\sum_{t=1}^k\sigma_t^2(A)+||A||_F^2
{c_6\epsilon^2\over k^2}\nonumber\\
\hbox{Thus,}\quad
\E||C-C\sum_{t=1}^kv^{(t)}v^{(t)^T}||_F^2&\leq& \sum_{t=k+1}^n\sigma_t^2(A)+{c_6\epsilon^2\over k^2}||A||_F^2
\label{500}.
\end{eqnarray}

Now we wish to use Corollary (\ref{invariant4}) to derive a good left projection for $C$ from the right
projection above. To this end, we define
$$u^{(t)}={Cv^{(T)}\over |Cv^{(t)}|}\quad\for t=1,2,\ldots k.$$
Define $l$ to be the largest integer in $\{ 1,2,\ldots k\}$ such that
$|Cv^{(l)}|^2\geq {\sqrt{c_5}\epsilon\over k}||A||_F^2$. Then from the Corollary, we get
\begin{eqnarray}
\E||C-\sum_{t=1}^lu^{(t)}u^{(t)^T}C||_F^2&\leq&  \E||C-C\sum_{t=1}^kv^{(t)}v^{(t)^T}||_F^2+O(\epsilon)||A||_F^2
  \nonumber\\
&\leq&\sum_{t=k+1}^n \sigma_t^2(A)+O(\epsilon)||A||_F^2.\\
\E||C-\sum_{t=1}^lu^{(t)}u^{(t)^T}C||_2^2&\leq& \sigma_{k+1}^2(A)+O(\epsilon) ||A||_F^2\label{601}.
\end{eqnarray}
Finally,we use Lemma (\ref{uuTAC}) to argue that $\sum_{t=1}^lu^{(t)}u^{(t)^T}$ is a good left projection for $A$. To do so, we first note that
$||\sum_{t=1}^lu^{(t)}u^{(t)^T}||_F\leq\sum_{t=1}^l|u^{(t)}|^2\leq k$. So,
\begin{eqnarray*}
\E||A-\sum_{t=1}^lu^{(t)}u^{(t)^T}A||_F^2&\leq& \E||C-\sum_{t=1}^lu^{(t)}u^{(t)^T}C||_F^2+{1\over\sqrt{cs}}||A||_F^2k(2+k)\\
&\leq& \sum_{t=k+1}^n\sigma_t^2(A)+O(\epsilon)||A||_F^2\\
\E||A-\sum_{t=1}^lu^{(t)}u^{(t)^T}A||_2^2&\leq& \sigma_{k+1}^2(A)+O(\epsilon)||A||_F^2.
\end{eqnarray*}
Thus, we get the following lemma:
\begin{lemma}\label{conssvd}
Suppose we are given an $m\times n$ matrix $A$, a positive integer
$k\leq m,n$ and a real $\epsilon>0$. Then for the $u^{(1)},u^{(2)},\ldots u^{(l)}$
produced by the constant-time-SVD algorithm, we have the following two bounds:
$$\E||A-\sum_{t=1}^lu^{(t)}u^{(t)^T}A||_F^2\leq\sum_{t=k+1}^n\sigma_t^2(A)+\epsilon ||A||_F^2$$
$$\E||A-\sum_{t=1}^lu^{(t)}u^{(t)^T}A||_2^2\leq \sigma_{k+1}^2(A)+\epsilon ||A||_F^2.$$
\end{lemma}

The proof is already given.

\begin{center}
\fbox{\parbox{4.7in}{
\begin{minipage}{4.5in}
\begin{tt}

{\bf Algorithm: Constant-time SVD}

\begin{enumerate}
\item Pick a sample of
$$s={c_8k^5\over c\epsilon^4}$$
columns of $A$ according to LS$_{\hbox{col}}(A,c)$ and scale to form an $m\times s$ matrix $C$.

\item Sample a set of $s$
rows of $C$ according to a LS$_{\hbox{row}}(C,c)$ distribution and scale to form a $s\times s$ matrix $W$.

\item Find the SVD of $W^TW$:
$$W^TW=\sum_t \sigma_t^2(W)v^{(t)}v^{(t)^T}.$$

\item Compute
$$u^{(t)}={Cv^{(t)}\over |Cv^{(t)}|}\quad\for t=1,2,\ldots k.$$
Let $l$ to be the largest integer in $\{ 1,2,\ldots k\}$ such that
\[
|Cv^{(l)}|^2\geq c_9\epsilon||C||_F^2/k.
\]

\item Return
$$\sum_{t=1}^lu^{(t)}u^{(t)^T}A$$ as the approximation to $A$.

\end{enumerate}
\end{tt}
\end{minipage}
}}
\end{center}




\section{CUR: An interpolative low-rank approximation}

In this section, we wish to describe an algorithm to get an approximation of any matrix $A$ given just a sample of rows and a sample of columns of $A$. Clearly if the sample is picked according to the uniform distribution, this attempt would fail in general. We will see that again the length squared distribution comes to our rescue; indeed, we will show that if the samples are picked according to the length squared or approximate length squared distributions, we can get an approximation for $A$. Again, this will hold for an arbitrary matrix $A$.

First suppose $A$ is a $m\times n$ matrix and $R$ ($R$ for rows) is a $s\times n$ matrix construced by picking $s$ rows of $A$ in i.i.d. samples, each according to LS$_{\hbox{row}(A,c)}$ and scaled. Similarly, let $C$
(for columns) be a $m\times s$ matrix consisting of columns picked according to LS$_{\hbox{col}(A,c)}$ and scaled. The motivating question for this section is: Can we get an approximation to $A$ given just $C,R$?

%--- Matrix Reconstruction, Colloborative Filtering....

Intuitively, this should be possible since we know that $CC^T \approx AA^T$ and
$R^TR \approx A^TA$.
Now it is easy to see that if we are given both $AA^T$ and $A^TA$ and $A$ is in ``general position'', i.e., say all its singular values are distinct, then $A$ can be found: indeed, if the SVD of $A$ is
$$A=\sum_t \sigma_t(A) u^{(t)}v^{(t)^T},$$
then
$$AA^T=\sum_t \sigma_t^2(A) u^{(t)}u^{(t)^T}\qquad A^TA=\sum_t\sigma_t^2(A) v^{(t)}v^{(t)^T},$$
and so from the SVD's of $AA^T,A^TA$, the SVD of $A$ can be read off if the $\sigma_t(A)$ are all distinct. [This is not the case if the $\sigma_t$ are not distinct; for example, for any square $A$ with orthonormal columns, $AA^T=A^TA=I$.]
The above idea leads intuitively to the guess that at least in general position, $C,R$ are sufficient to produce some approximation to  $A$.

The approximation of $A$ by the product $CUR$ is reminiscent of the usual PCA approximation based on taking the leading $k$ terms of the SVD decomposition. There, instead of $C,R$, we would have orthonormal matrices consisting of the leading singular vectors and instead of $U$, the diagonal matrix of singular values. The PCA decomposition of course gives the best rank-$k$ approximation, whereas what we will show below for $CUR$ is only that its error is bounded in terms of the best error we can achieve. There are two main advantages of $CUR$ over PCA:
\begin{enumerate}
 \item $CUR$ can be computed much faster from $A$ and also we only need to make two passes over $A$ which can be assumed to be stored on external memory.

 \item $CUR$ preserves the sparsity of $A$ - namely $C,R$ are columns and rows of $A$ itself. ($U$ is a small matrix since typically $s$ is much smaller than $m,n$). So any further matrix vector products $Ax$ can be approximately computed as $C(U(Rx))$ quickly.
\end{enumerate}

The main theorem of this section is the following.

\begin{theorem} Suppose $A$ is any $m\times n$ matrix, $C$ is any $m\times s$ matrix of rank at least $k$. Suppose $i_1,i_2,\ldots i_s$ are obtained from $s$ i.i.d. trials each according to probabilities $\{ p_1,p_2,\ldots p_m\}$ conforming to
LS$_{\hbox{rows}(A,c)}$ and let $R$ be the $s\times n$ matrix with $t$ th row equal to $A_{i_t}/\sqrt{sp_{i_t}}$.
Then, from $C,R, \{ i_t\}$, we can find an $s\times s$ matrix $U$ such that
\begin{eqnarray*}
\E(\|CUR-A\|_F)&\leq& \|A-A_k\|_F +\sqrt {k\over cs}||A||_F+\sqrt{2}k^{\frac{1}{4}}||AA^T-CC^T||_F^{1/2}\\
\E(\|CUR-A\|_2)&\leq& \|A-A_k\|_2 +\sqrt {k\over cs}||A||_F+\sqrt{2}||AA^T-CC^T||_F^{1/2}
\end{eqnarray*}
\end{theorem}

\begin{proof}
The selection of rows and scaling used to obtain $R$ from $A$ can be represented by as
$$R=DA,$$
where $D$ has only one non-zero entry per row.
Let the SVD of $C$ be
$$C=\sum_{t=1}^r \sigma_t(C)x^{(t)}y^{(t)^T}.$$
By assumption $\sigma_k(C)> 0$.
Then the SVD of $C^TC$ is
$$C^TC=\sum_{t=1}^r\sigma_t^2(C) y^{(t)}y^{(t)^T}.$$
Then, we prove the theorem with $U$ defined by
$$U= \sum_{t=1}^k{1\over\sigma_t^2(C)}y^{(t)}y^{(t)^T}C^TD^T.$$

Then, using the orthonormality of $\{ x^{(t)}\},\{ y^{(t)}\}$,
\begin{eqnarray*}
CUR&=&\sum_{t=1}^r\sigma_t(C)x^{(t)}y^{(t)^T}\sum_{s=1}^k{1\over\sigma_s^2(C)}y^{(s)}y^{(s)^T}\sum_{p=1}^r\sigma_p(C)y^{(p)}x^{(p)^T}D^TDA\\
&=&\sum_{t=1}^k x^{(t)}x^{(t)^T}D^TDA
\end{eqnarray*}
Consider the
matrix multiplication
$$\left( \sum_{t=1}^kx^{(t)}x^{(t)^T}\right)\left( A\right).$$
$D^TD$ above can be viewed precisely as selecting some rows of the matrix $A$ and the corresponding columns of
$\sum_tx^{(t)}x^{(t)^T}$ with suitable scaling. Applying Theorem \ref{matrixmult} directly, we thus get
using $||\sum_{t=1}^kx^{(t)}x^{(t)^T}||_F^2=k$ (Note : in the theorem one is selecting columns of the first matrix according to LS$_{\hbox{col}}$ of that matrix; here symmetrically, we are selecting rows of the second matrix according to LS$_{\hbox{row}}$ of that matrix.)
$$\E\left|\left| \sum_{t=1}^k x^{(t)}x^{(t)^T}D^TDA - \sum_{t=1}^k x^{(t)}x^{(t)^T}A\right|\right|_F^2\leq {k\over cs}||A||_F^2.$$
Thus,
$$\E||CUR-\sum_{t=1}^k x^{(t)}x^{(t)^T}A||_F^2\leq {k\over cs}||A||_F^2.$$
Next, from Lemma \ref{svdC} it follows that
\begin{eqnarray*}
\|\sum_{t=1}^k x^{(t)}x^{(t)^T}A-A\|_F^2&\leq& \|A-A_k\|_F^2+2\sqrt{k}\|AA^T-CC^T\|_F \\
\|\sum_{t=1}^k x^{(t)}x^{(t)^T}A-A\|_2^2&\leq& \|A-A_k\|_2+2\|AA^T-CC^T\|_F.
\end{eqnarray*}
Now the theorem follows using the triangle inequality on the norms.
\end{proof}

As a corollary, we have the following:
\begin{corollary}\label{cor:CUR}
Suppose we are given $C$, a set of independently chosen columns of $A$ from LS$_{col(A,c)}$ and $R$, a set of $s$ independently chosen rows of $A$ from LS$_{rows(A,c)}$. Then, in time $O((m+n)s^2)$, we can find an $s \times s$ matrix $U$ such that for any $k$,
\[
\E\left(\|A-CUR\|_F\right) \leq \|A-A_k\|_F + \left(\frac{k}{s}\right)^{1/2}\|A\|_F + \left(\frac{4k}{s}\right)^{1/4}\|A\|_F
\]
\end{corollary}



The following open problem, if answered affirmatively, would generalize the theorem.

{\bf Problem} Suppose $A$ is any $m\times n$ matrix and $C,R$ are {\bf any} $m\times s$ and $s\times n$ (respectively) matrices with
$$||AA^T-CC^T||_F, ||A^TA-R^TR||_F\leq \delta ||A||_F^2.$$
Then, from just $C,R$, can we find a $s\times s$ matrix $U$ such that
$$||A-CUR||_F\leq \hbox{poly}(\frac{\delta}{s})||A||_F?$$

So we do not assume that $R$ is a random sample as in the theorem.

\section{Discussion}
Sampling from the length square distribution was introduced in a paper by Frieze, Kannan and Vempala \cite{FKV-focs98, FKV04} in the context of a constant-time algorithm for low-rank approximation. It has been used many times subsequently. There are several advantages of sampling-based algorithms for matrix approximation. The first is efficiency. The second is the nature of the approximation, namely it is often interpolative, i.e., uses rows/columns of the original matrix. Finally, the methods can be used in the streaming model where memory is limited and entries of the matrix arrive in arbitrary order.

The analysis for matrix multiplication is originally due to Drineas and Kannan \cite{DK2001}. The linear-time low-rank approximation was given by Drineas et al. \cite{DFKVV04}. The CUR decomposition first appeared in \cite{DK2003}. The best-know sample complexity for the constant-time algorithm is $O(k^2/\eps^4)$ and other refinements are given in \cite{DKM1, DKM2, DKM3}. An alternative sampling method which sparsifies a given matrix and uses a low-rank approximation of the sparse matrix was given in \cite{AM2007}.

We conclude this section with a description of some typical applications.
A recommendation system is a marketing tool with wide use. Central to
this is the consumer-product matrix $A$ where $A_{ij}$ is the
``utility'' or ``preference'' of consumer $i$ for product $j$. If the
entire matrix were available, the task of the system is simple -
whenever a user comes up, it just recommends to the user the
product(s) of maximum utility to the user. But this assumption is
unrealistic; market surveys are costly, especially if one wants to ask
each consumer. So, the essential problem in Recommendation Systems is
Matrix Reconstruction - given only a sampled part of $A$, reconstruct
(implicitly, because writing down the whole of $A$ requires too much
space) an approximation $A'$ to $A$ and make recommendations based on $A'$.
A natural assumption is to say that we have a set of sampled rows (we
know the utilities of some consumers- at least their top choices) and
a set of sampled columns (we know the top buyers of some products).
This model very directly suggests the use of the
CUR decomposition below which says that for any matrix $A$ given a set
of sampled rows and columns, we can construct an approximation $A'$ to $A$
from them. Some well-known recommendation systems in practical use relate to
on-line book sellers, movie renters etc.

In the first mathematical model for Recommendation Systems
Azar et al. \cite{AFKM2001} assumed a generative model where there
were k types of consumers and each is a draw from a
probability distribution (a mixture model). It is easy to see then
that A is close to a low-rank matrix. The CUR type model and analysis
using CUR decomposition was by \cite{DKR02}.

We note an important philosophical difference in the use of sampling here
from previous topics discussed. Earlier, we assumed
that there was a huge matrix $A$ explicitly written down somewhere and
since it was too expensive to compute with all of it, one used
sampling to extract a part of it and computed with this. Here, the
point is that it is expensive to get the whole of A, so we have to do
with a sample  from which we ``reconstruct'' implicitly the whole.


%% TO DO: add projective clustering, add last section, exercises.

\chapter{Adaptive Sampling Methods}\label{adaptive}

In this chapter, we continue our study of sampling methods for matrix approximation, including linear regression and low-rank approximation.
In the previous chapter, we saw that any matrix $A$ has a subset of
$k/\eps$ rows whose
span contains an approximately optimal rank-$k$ approximation to
$A$. We recall the precise statement.
\begin{theorem}\label{FKV}
Let $S$ be a sample of $s$ rows of an $m \times n$ matrix $A$,
each chosen independently from the following distribution: Row $i$
is picked with probability
\[
P_i \geq c\frac{||A^{(i)}||^2}{\fnorms{A}}.
\]
If $s \ge k/c\eps$, then the span of $S$ contains a matrix
$\tilde{A}_k$ of rank at most $k$ for which
\[
\E(\fnorms{A-\tilde{A}_k}) \leq \fnorms{A-A_k} + \eps\fnorms{A}.
\]
\end{theorem}
This was turned into an efficient algorithm.
The algorithm makes one pass through $A$ to
figure out the sampling distribution and another pass to
compute the approximation. Its complexity is
$O(\min\{m,n\}k^2/\eps^4)$. We also saw a ``constant-time" algorithm
that samples both rows and columns.

These results naturally lead to the following two important
questions: (1) The additive error in Theorem \ref{FKV} is
$\eps\fnorms{A}$ which can be very large since we have no control
on $\fnorms{A}$. Can this error be reduced significantly by using
multiple passes through the data? (2) Can we get multiplicative
$(1+\eps)$ approximations using a small sample?

% Give a brief answer to the questions above. Its not a mystery novel.
% like: This chapter shows how to provide positive answers to both questions.

\section{Adaptive length-squared sampling}\label{sec:pass-eff}

% as its illustrative, provide a picture? (I can make one for you if you like)

As an illustrative example, suppose the
data consists of points along a $1$-dimensional subspace of $\R^n$
except for one point. The best rank-$2$ subspace has zero error.
However, one round of sampling will most likely miss the point far
from the line. So we use a two-round approach. In the first pass,
we get a sample from the squared length distribution and find a
rank-$2$ subspace using it. Then we sample again, but this time
with probability proportional to the squared distance to the first
subspace. If the lone far-off point is missed in the first pass,
it will have a high probability of being chosen in the second
pass. The span of the full sample now contains a good rank $2$
approximation.

The main idea behind the adaptive length-squared sampling scheme is the following
generalization of Theorem \ref{FKV}. Notice that if we put $V =
\emptyset$ in the following theorem then we get exactly Theorem
\ref{FKV}. Recall that for a subspace $V \subseteq \R^n$, we denote by $\proj_{V,k}(A)$ the best rank-$k$ approximation (under the Frobenius norm) of $A$ with rows
in the span of $V$.

\begin{theorem}\label{thm:tworounds}
Let $A \in \R^{m \times n}$. Let $V \subseteq \R^n$ be a vector
subspace. Let $E = A - \proj_V(A)$. For a fixed $c \in \R$, let
$S$ be a random sample of $s$ rows of $A$ from a distribution such
that row $i$ is chosen with probability
\begin{equation}\label{equ:hypothesisPi}
    P_i \geq c \frac{\norm{E^{(i)}}^2}{\fnorms{E}}.
\end{equation}
Then, for any nonnegative integer $k$,
\[
\E_S(\fnorms{A-\proj_{V + \rowspan(S),k}(A)}) \leq
\fnorms{A-\proj_k(A)} + \frac{k}{cs} \fnorms{E}.
\]
\end{theorem}

\begin{proof}
For $S = (r_i)_{i=1}^s$ a sample of rows of $A$ and $1 \leq j \leq
r$%($r$ is the rank of $A$)
, let
%$S'=\linspan\{w^{(j)}\}_{j=1}^k$, where
\[
w^{(j)} = \proj_V(A)^T {u^{(j)}} + \frac{1}{s}\sum_{i=1}^s
\frac{u_{r_i}^{(j)}}{P_{r_i}} E^{(r_i)}.
\]
Then, $\E_{S}(w^{(j)}) = \proj_V(A)^T {u^{(j)}} + E^T u^{(j)} =
\sigma_j v^{(j)}$. Now we will bound $\E_{S} (\norm{w^{(j)} -
\sigma_j v^{(j)}}^2)$. Use the definition of $w^{(j)}$ to get
\[
w^{(j)} - \sigma_j v^{(j)}
    = \frac{1}{s}\sum_{i=1}^s
\frac{u_{r_i}^{(j)}}{P_{r_i}} E^{(r_i)} - E^T u^{(j)}.
\]
Apply the norm squared to each side and expand the left hand side:
\begin{equation}\label{equ:normsquared}
\norm{w^{(j)} - \sigma_j v^{(j)}}^2 =
\lrnorm{\frac{1}{s}\sum_{i=1}^s \frac{u_{r_i}^{(j)}}{P_{r_i}}
E^{(r_i)}}^2 - \frac{2}{s} \sum_{i=1}^s
\frac{u_{r_i}^{(j)}}{P_{r_i}} E^{(r_i)} \cdot (E^T u^{(j)}) +
\norm{E^T u^{(j)}}^2.
\end{equation}
Observe that
\begin{equation}\label{equ:intermediate5}
\E_{S} \left( \frac{u_{r_i}^{(j)}}{P_{r_i}} E^{(r_i)} \right) =
\sum_{i=1}^m P_i \frac{u_{i}^{(j)}}{P_{i}} E^{(i)} = E^T u^{(j)},
\end{equation}
which implies that
\[
\E_{S} \left( \frac{2}{s} \sum_{i=1}^s
\frac{u_{r_i}^{(j)}}{P_{r_i}} E^{(r_i)} \cdot (E^T u^{(j)})
\right) = 2 \norm{E^T u^{(j)}}^2.
\]
Using this, apply $\E_{S}$ to Equation (\ref{equ:normsquared}) to
get:
\begin{equation}\label{equ:intermediate4}
\E_{S} (\norm{w^{(j)} - \sigma_j v^{(j)}}^2)
    = \E_{S}\left(\lrnorm{\frac{1}{s}\sum_{i=1}^s
\frac{u_{r_i}^{(j)}}{P_{r_i}} E^{(r_i)}}^2\right) - \norm{E^T
u^{(j)}}^2
\end{equation}
Now, from the left hand side, and expanding the norm squared,
\begin{equation}\label{equ:intermediate1}
\begin{aligned}
\E_{S}\left(\lrnorm{\frac{1}{s}\sum_{i=1}^s
\frac{u_{r_i}^{(j)}}{P_{r_i}} E^{(r_i)}}^2 \right)
    &= \frac{1}{s^2}
\sum_{i=1}^s \E_S\left(\frac{\norm{u_{r_i}^{(j)}
E^{(r_i)}}^2}{P_{r_i}^2}\right) + \\ &\quad + \frac{2}{s^2}
\sum_{1\le i<l\le s} \E_S \left(\frac{u_{r_i}^{(j)}
E^{(r_i)}}{P_{r_i}} \cdot \frac{u_{r_l}^{(j)}
E^{(r_l)}}{P_{r_l}}\right)
\end{aligned}
\end{equation}
where
\begin{equation}\label{equ:intermediate2}
\sum_{i=1}^s \E_S\left(\frac{\norm{u_{r_i}^{(j)}
E^{(r_i)}}^2}{P_{r_i}^2}\right)
    = \sum_{i=1}^s \sum_{l=1}^m P_l
\frac{\norm{u_{l}^{(j)} E^{(l)}}^2}{P_{l}^2}
     = s \sum_{l=1}^m \frac{\norm{u_l^{(j)}E^{(l)}}^2}{P_l}
\end{equation}
and, using the independence of the $r_i$'s and Equation
(\ref{equ:intermediate5}),
\begin{equation}\label{equ:intermediate3}
\begin{aligned}
\sum_{1\le i<l\le s} \E_S \left(\frac{u_{r_i}^{(j)}
E^{(r_i)}}{P_{r_i}} \cdot \frac{u_{r_l}^{(j)}
E^{(r_l)}}{P_{r_l}}\right)
    &= \sum_{1\le i<l\le s} \E_S \left(\frac{u_{r_i}^{(j)}
E^{(r_i)}}{P_{r_i}}\right) \cdot \E_S \left(\frac{u_{r_l}^{(j)}
E^{(r_l)}}{P_{r_l}}\right)\\
%    &= \sum_{1\le i<l\le s} \sum_{q,r=1}^m
%P_q P_r \left(\frac{u_{q}^{(j)} E^{(q)}}{P_{q}} \cdot
%\frac{u_{r}^{(j)} E^{(r)}}{P_{r}}\right) \\
    &= \frac{s(s-1)}{2}
\norm{E^T u^{(j)}}^2.
\end{aligned}
\end{equation}
The substitution of Equations (\ref{equ:intermediate2}) and
(\ref{equ:intermediate3}) in (\ref{equ:intermediate1}) gives
\[
\E_{S}\left(\lrnorm{\frac{1}{s}\sum_{i=1}^s
\frac{u_{r_i}^{(j)}}{P_{r_i}} E^{(r_i)}}^2 \right)
    = \frac{1}{s} \sum_{i=1}^m \frac{\norm{u_i^{(j)}E^{(i)}}^2}{P_i}
    + \frac{s-1}{s} \norm{E^T
u^{(j)}}^2.
\]
Using this in Equation (\ref{equ:intermediate4}) we have
\[
\E_{S} (\norm{w^{(j)} - \sigma_j v^{(j)}}^2) = \frac{1}{s}
\sum_{i=1}^m \frac{\norm{u_i^{(j)}E^{(i)}}^2}{P_i} -\frac{1}{s}
\norm{E^T u^{(j)}}^2,
\]
and, using the hypothesis for $P_i$ (Equation
(\ref{equ:hypothesisPi})), remembering that $u^{(j)}$ is a unit
vector and discarding the second term we conclude
\begin{equation}\label{equ:variance}
\begin{aligned}
\E_{S} (\norm{w^{(j)} - \sigma_j v^{(j)}}^2)
    &\leq \frac{1}{cs} \fnorms{E}.
\end{aligned}
\end{equation}

%\begin{equation}\label{equ:variance}
%\begin{aligned}
%\E_{S} (\norm{w^{(j)} - \sigma_j v^{(j)}}^2)
%    &= \E_{S}\left(\norm{\frac{1}{s}\sum_{i=1}^s
%\frac{u_{r_i}^{(j)}}{P_{r_i}} E^{(r_i)} - E^T u^{(j)}}^2\right) \\
%    &= \E_{S}\left(\norm{\frac{1}{s}\sum_{i=1}^s
%\frac{u_{r_i}^{(j)}}{P_{r_i}} E^{(r_i)}}^2\right) - \norm{E^T
%u^{(j)}}^2 \\
%    &= \frac{1}{s^2} \sum_{i=1}^s
%\E_S\left(\frac{\norm{u_{r_i}^{(j)}
%E^{(r_i)}}^2}{P_{r_i}^2}\right) + \\ &\quad + \frac{2}{s^2}
%\sum_{1\le i<l\le s} \E_S \left(\frac{u_{r_i}^{(j)}
%E^{(r_i)}}{P_{r_i}} \cdot \frac{u_{r_l}^{(j)} E^{(r_l)}}{P_{r_l}}\right) - \norm{E^Tu^{(j)}}^2 \\
%    &= \frac{1}{s} \sum_{i=1}^m \frac{||u_i^{(j)}E^{(i)}||^2}{P_i} -\frac{1}{s} \norm{E^T
%u^{(j)}}^2 \\
%    &\leq \frac{1}{cs} \fnorms{E}.
%\end{aligned}
%\end{equation}
Let ${\hat y}^{(j)} = \frac{1}{\sigma_j} w^{(j)}$ for $j=1,
\dotsc, r$, let $k' = \min \{k, r\}$ (think of $k'$ as equal to
$k$, this is the interesting case), let $W = \linspan \{{\hat
y}^{(1)}, \dotsc, {\hat y}^{(k')}\}$, and $\hat F = A
\sum_{t=1}^{k'} v^{(t)} {\hat y}^{(t)}{}^T$. We will bound the
error $\fnorms{A - \proj_{W}(A)}$ using $\hat F$. Observe that the
row space of $\hat F$ is contained in $W$ and $\proj_{W}$ is the
projection operator onto the subspace of all matrices with row
space in $W$ with respect to the Frobenius norm. Thus,
\begin{equation}\label{equ:projection}
\fnorms{A - \proj_{W} (A)} \leq \fnorms{A-\hat F}.
\end{equation}
Moreover,
\begin{equation}\label{equ:effHat}
\fnorms{A - \hat F}
    = \sum_{i=1}^r \norm{(A-\hat F)^T u^{(i)}}^2
    = \sum_{i=1}^{k'} \norm{\sigma_i v^{(i)} - w^{(i)}}^2 +
    \sum_{i=k'+1}^r \sigma_i^2.
\end{equation}
Taking expectation and using (\ref{equ:variance}) we get
\begin{equation*}%\label{equ:expectation}
\E_{S}(\fnorms{A - \hat F} )\leq \sum_{i=k+1}^n \sigma_i^2 +
\frac{k}{cs} \fnorms{E} = \fnorms{A-\proj_k(A)} + \frac{k}{cs}
\fnorms{E}.
\end{equation*}
This and Equation (\ref{equ:projection}) give
\begin{equation}\label{equ:intermediate9}
\E_{S}(\fnorms{A - \proj_{W} (A)})
    \leq \fnorms{A-\proj_k(A)} +
\frac{k}{cs} \fnorms{E}.
\end{equation}
Finally, the fact that $W \subseteq V + \rowspan(S)$ and $\dim(W)
\leq k$ imply that
\[
\fnorms{A-\proj_{V + \rowspan(S),k}(A)} \leq \fnorms{A - \proj_{W}
(A)},
\]
and, combining this with Equation (\ref{equ:intermediate9}), we
conclude
\[
\E_S(\fnorms{A-\proj_{V + \rowspan(S),k}(A)}) \leq
\fnorms{A-\proj_k(A)} + \frac{k}{cs} \fnorms{E}.
\]
\end{proof}

Now we can use Theorem \ref{thm:tworounds} to prove the main theorem of this section by induction.
\begin{theorem}\label{thm:adaptive}
Let $S = S_1 \cup \dotsb \cup S_t$ be a random sample of rows of
an $m \times n$ matrix $A$ where for $j=1,\ldots,t$, each set
$S_j$ is a sample of $s$ rows of $A$ chosen independently from the
following distribution: row $i$ is picked with probability
\[
P_i^{(j)} \geq c \frac{\norm{E_j^{(i)}}^2}{\fnorms{E_j}}
\]
where $E_1 =A$, $E_j = A - \proj_{S_1 \cup \dotsb \cup
S_{j-1}}(A)$ and $c$ is a constant. Then for $s \ge k/c\eps$, the
span of $S$ contains a matrix $\tilde{A}_k$ of rank $k$ such that
\[
\E_{S} (\fnorms{A-\tilde{A}_k}) \leq \frac{1}{1-\eps}
\fnorms{A-A_k} + \eps^t\fnorms{A}.
\]
\end{theorem}
\begin{proof}
We will prove the slightly stronger result
\[
\E_{S} (\fnorms{A-\proj_{S,k}(A)}) \leq
\frac{1-(\frac{k}{cs})^t}{1-\frac{k}{cs}} \fnorms{A-\proj_k(A)} +
\left(\frac{k}{cs}\right)^t \fnorms{A}
\]
by induction on $t$. The case $t=1$ is precisely Theorem
\ref{FKV}.

For the inductive step, let $E = A - \proj_{S_1 \cup \dotsb \cup
S_{t-1}}(A)$. By means of Theorem \ref{thm:tworounds} we have
that,
\[
\E_{S_t} (\fnorms{A-\proj_{S_1 \cup \dotsb \cup S_t,k}(A)}) \leq
\fnorms{A-\proj_k(A)} + \frac{k}{cs} \fnorms{E}.
\]
Combining this inequality with the fact that $\fnorms{E} \leq
\fnorms{A-\proj_{S_1 \cup \dotsb \cup S_{t-1},k}(A)}$ we get
\[
\E_{S_t} (\fnorms{A-\proj_{S_1 \cup \dotsb \cup S_t,k}(A)}) \leq
\fnorms{A-\proj_k(A)} + \frac{k}{cs} \fnorms{A-\proj_{S_1 \cup
\dotsb \cup S_{t-1},k}(A)}.
\]
Taking the expectation over $S_1, \dotsc, S_{t-1}$:
\[
\E_{S} (\fnorms{A-\proj_{S_1 \cup \dotsb \cup S_t,k}(A)}) \leq
\fnorms{A-\proj_k(A)} + \frac{k}{cs} \E _{S_1, \dotsc,
S_{t-1}}\left(\fnorms{A-\proj_{S_1 \cup \dots \cup
S_{t-1},k}(A)}\right)
\]
and the result follows from the induction hypothesis for $t-1$.
\end{proof}

This adaptive sampling scheme suggests the following
algorithm that makes $2t$ passes through the data and computes and
a rank-$k$ approximation within additive error $\epsilon^t$.
\begin{center}
\fbox{\parbox{4.7in}{
\begin{minipage}{4.5in}
\begin{sf}
{\bf Iterative Fast SVD}\\

Input: $A \in \R^{m \times n}$ with $M$ non-zero entries, integers
$k \leq m$, $t$, error $\epsilon > 0$.%.

Output: A set of $k$ vectors in $\R^n$.

\begin{enumerate}
\item Let $S = \emptyset$, $ s= k/\epsilon$.

\item\label{item:loop} Repeat $t$ times:

\begin{enumerate}

%\item Compute $ P_i^{(j)} = \frac{\norm{E^{(i)}}^2}{\fnorms{E}}$
%where $E = A - \proj_{\linspan S}(A)$. To do this, project $A$ onto
%$Y$
\item Let $E = A - \proj_{S}(A)$.
%\item Let $E = E - \proj_{T}(A)$.
%\item Find distribution...

\item Let $T$ be a sample of $s$ rows of $A$ according to the
distribution that assigns probability $
\frac{\norm{E^{(i)}}^2}{\fnorms{E}}$ to row $i$.

\item\label{item:S} Let $S = S \cup T$.
\end{enumerate}

\item\label{item:svd} Let $h_1,\dotsc, h_k$ be the top $k$ right
singular vectors of $\proj_{S}(A)$.

\end{enumerate}
\end{sf}
\end{minipage}
}} \end{center}

\begin{theorem}
Algorithm {\bf Iterative Fast SVD} finds vectors $h_1 \dotsc, h_k \in
\R^{n}$ such that their span $V$ satisfies
\begin{equation}\label{equ:algorithm}
\E(\fnorms{A - \proj_V(A)}) \leq \frac{1}{1-\epsilon} \fnorms{A -
\proj_k(A)} + \epsilon^t \fnorms{A}.
\end{equation}
The running time is $O\left(M \frac{kt}{\epsilon} + (m+n)\frac{k^2
t^2}{\epsilon^2}\right)$.
\end{theorem}
\begin{proof}
For the correctness, observe that $\proj_{V}(A)$ is a random
variable with the same distribution as $\proj_{S,k}(A)$ as defined
in Theorem \ref{thm:adaptive}. Also, $\fnorms{A - \proj_{S,k}(A)} -
\fnorms{A - \proj_k(A)}$ is a nonnegative random variable and
Theorem \ref{thm:adaptive} gives a bound on its expectation:
\[
\E_{S} (\fnorms{A-\proj_{S,k}(A)} - \fnorms{A - \proj_k(A)}) \leq
\frac{\eps}{1-\eps} \fnorms{A-\proj_k (A)} + \eps^t\fnorms{A}.
\]

We will now bound the running time. We maintain a basis of the
rows indexed by $S$. In each iteration, we extend this basis
orthogonally with a new set of vectors $Y$, so that it spans the
new sample $T$. The residual squared length of each row,
$\norm{E^{(i)}}^2$, as well as the total, $\fnorms{E}$, are
computed by subtracting the contribution of $\proj_{T}(A)$ from
the values that they had during the previous iteration. In each
iteration, the projection onto $Y$ needed for computing this
contribution takes time $O(Ms)$.  In iteration $i$, the
computation of the orthonormal basis $Y$ takes time $O(ns^2i)$
(Gram-Schmidt orthonormalization of $s$ vectors in $\reals^n$
against an orthonormal basis of size at most $s(i+1)$). Thus, the
total time in iteration $i$ is $O(M s + n s^2 i)$; with $t$
iterations, this is $O(M s t + ns^2 t^2)$. At the end of Step
\ref{item:loop} we have $\proj_{S}(A)$ in terms of our basis (an
$m \times st$ matrix).  Finding the top $k$ singular vectors in
Step \ref{item:svd} takes time $O(m s^2 t^2)$. Bringing them back
to the original basis takes time $O(nkst)$. Thus, the total
running time is $O(Mst + n s^2 t^2 + ms^2 t^2 + nkst)$ or, in
other words, $O\left(M \frac{kt}{\epsilon} + (m+n)\frac{k^2
t^2}{\epsilon^2}\right)$.
\end{proof}

\section{Volume Sampling}\label{sec:relative}
Volume sampling is a generalization of length-squared sampling. We pick
subsets of $k$ rows instead picking rows one by one. The
probability that we pick a subset $S$ is proportional to the
volume of the $k$-simplex $\Delta(S)$ spanned by these $k$ rows
along with the origin. This method will give us a factor
$(k+1)$ approximation (in expectation) and a proof that any matrix has $k$ rows whose span contains a such an approximation. Moreover, this bound is tight, i.e., there exist matrices for which no $k$ rows can give a better approximation.

\begin{theorem}\label{KROWS}
Let $S$ be a random subset of $k$ rows of a given matrix $A$
chosen with probability
\[
P_S = \frac{\vol(\Delta(S))^2}{\sum_{T:|T|=k} \vol(\Delta(T))^2}.
\]
Then $\tilde{A}_k$, the projection of $A$ to the span of $S$,
satisfies
\[
\E(||A-\tilde{A}_k||_F^2) \leq (k+1) ||A-A_k||_F^2.
\]
\end{theorem}
\begin{proof}
For every $S \subseteq [m]$, let $\Delta_S$ be the simplex formed
by formed by $\{ A^{(i)} | i \in S \}$ and the origin, and let
$H_S$ be the linear subspace spanned by these rows.
\begin{eqnarray*}
\sum_{S, |S|=k+1} \vol_{k+1} (\Delta_S)^2 &=& \frac{1}{k+1}
\sum_{S, |S|=k} \sum_{j=1}^{m} \frac{1}{(k+1)^2}~ \vol_k
(\Delta_S)^2~ d(A^{(j)}, H_S)^2 \\
&=& \frac{1}{(k+1)^3} \sum_{S, |S|=k} \vol_k (\Delta_S)^2~
\sum_{j=1}^{m} d(A^{(j)}, H_S)^2
\end{eqnarray*}
Let $\sigma_1,\ldots, \sigma_n$ be the singular values of $A$. Then,
using Lemma \ref{VOLFORMULA} (proved next), we can rewrite
this as follows:
\[
\frac{1}{((k+1)!)^2} \sum_{1 \leq t_1 <
\ldots < t_{k+1} \leq n} \sigma_{t_1}^2 \ldots \sigma_{t_{k+1}}^2
= \frac{1}{(k+1)^3} \sum_{S, |S|=k} \vol_k (\Delta_S)^2~
\sum_{j=1}^{m} d(A^{(j)}, H_S)^2 \] which means that
\begin{eqnarray*}
\sum_{S, |S|=k} \vol_k (\Delta_S)^2 \| A - \pi_{S,k} (A) \|_F^2
&=& \frac{k+1}{(k!)^2} \sum_{1 \leq t_1 < \ldots <
t_{k+1} \leq n} \sigma_{t_1}^2 \ldots \sigma_{t_{k+1}}^2 \\
&\leq& \frac{k+1}{(k!)^2} \sum_{1 \leq t_1 < \ldots < t_k \leq n}
\sigma_{t_1}^2 \ldots \sigma_{t_k}^2 \sum_{j=k+1}^{m}
\sigma_j^2 \\
&\leq& \left( \sum_{S, |S|=k} \vol_k (\Delta_S)^2) \right) (k+1)
\| A - A_k \|_F^2
\end{eqnarray*}
Therefore \[ \frac{1}{\left( \sum_{S, |S|=k} \vol_k (\Delta_S)^2)
\right)} \sum_{S, |S|=k} \vol_k (\Delta_S)^2 \| A - \pi_{S,k} (A)
\|_F^2 \leq (k+1) \| A - A_k \|_F^2 \] And therefore there must
exist a set $S$ of $k$ rows of $A$ such that \[ \| A - \pi_{S,k}
(A) \|_F^2 \leq (k+1) \| A - A_k \|_F^2.
\]
The coefficient of $\|A-\pi_{S,k}(A)\|_F^2$ on the LHS is
precisely the probability with which $S$ is chosen by volume
sampling. Hence,
\[
\E(\|A-\pi_{S,k}(A)\|_F^2) \le (k+1)\|A-A_k\|_F^2.
\]
\end{proof}

\begin{lemma}\label{VOLFORMULA}
\[ \sum_{S, |S|=k} \vol_k (\Delta_S)^2 = \frac{1}{(k!)^2} \sum_{1 \leq
t_1 < t_2 < \ldots < t_k \leq n} \sigma_{t_1}^2 \sigma_{t_2}^2
\ldots \sigma_{t_k}^2 \] where $\sigma_1, \sigma_2, \ldots,
\sigma_n$ are the singular values of $A$.
\end{lemma}
\begin{proof}
Let $A_S$ be the sub-matrix of $A$ formed by the rows $\{ A^{(i)}~
| i \in S \}$. Then we know that the volume of the $k$-simplex
formed by these rows is given by \[ \vol_k (\Delta_S) =
\frac{1}{k!}~ \sqrt{det(A_S A_S^T)} \] Therefore
\begin{eqnarray*}
\sum_{S, |S|=k} \vol_k (\Delta_S)^2 &=& \frac{1}{(k!)^2} \sum_{S, |S|=k} det(A_S A_S^T) \\
&=& \frac{1}{(k!)^2} \sum_{\tiny \begin{array}{l} B: \mbox{principal} \\ k\mbox{-minor of} AA^T \end{array}} det(B) \\
\end{eqnarray*}
Let $det(AA^T - \lambda I) = \lambda^m + c_{m-1} \lambda^{m-1} +
\ldots + c_0$ be the characteristic polynomial of $AA^T$. From
basic linear algebra we know that the roots of this polynomial are
precisely the eigenvalues of $AA^T$, i.e., $\sigma_1^2,
\sigma_2^2, \ldots, \sigma_n^2$ and $0$ with multiplicity $(m-n)$.
Moreover the coefficient $c_{m-k}$ can be expressed in terms of
these roots as:
\[ c_{m-k} = (-1)^{m-k} \sum_{1 \leq t_1 < t_2 < \ldots < t_k \leq
n} \sigma_{t_1}^2 \sigma_{t_2}^2 \ldots \sigma_{t_k}^2
\]
But we also know that $c_{m-k}$ is the coefficient of
$\lambda^{m-k}$ in $det(AA^T - \lambda I)$, which by Lemma
\ref{COEFF} is
\[ c_{m-k}
= (-1)^{m-k} \sum_{\tiny \begin{array}{l} B: \mbox{principal} \\
k\mbox{-minor of} AA^T \end{array}} det(B) \] Therefore, \[
\sum_{S, |S|=k} \vol_k (\Delta_S)^2 = \frac{1}{(k!)^2} \sum_{1
\leq t_1 < t_2 < \ldots < t_k \leq n} \sigma_{t_1}^2
\sigma_{t_2}^2 \ldots \sigma_{t_k}^2 \]
\end{proof}

\begin{lemma}\label{COEFF}
Let the characteristic polynomial of $M \in \R^{m \times m}$ be
$det(M - \lambda I_m) = \lambda^m + c_{m-1} \lambda^{m-1} + \ldots
+ c_0$. Then \[ c_{m-k} = (-1)^{m-k} \sum_{\tiny \begin{array}{l}
B, B~ principal \\ k-minor~ of~ M \end{array}} det(B) \hspace{2cm}
for~ 1 \leq k \leq m \]
\end{lemma}
\begin{proof}
We use the following notation. Let $M' = M - \lambda I$, and $S_m$
be the set of permutation of $\{ 1, 2, \ldots, m \}$. The sign of
a permutation $sgn(\tau)$, for $\tau \in Perm([m])$, is equal to
$1$ if it can be written as a product of an even number of
transpositions and $-1$ otherwise. For a subset $S$ of rows, we
denote the submatrix of entries $(M_{i,j})_{i,j \in S}$ by $M_S$.
\[ det(M - \lambda I_m) = det(M') = \sum_{\tau \in Perm([m])}
sgn(\tau) M'_{1, \tau(1)} M'_{2, \tau(2)} \ldots M'_{m, \tau(m)}
\] The term $c_{m-k} \lambda^{m-k}$ comes by taking sum over
$\tau$ which fix some set $S \subseteq [m]$ of size $(m-k)$, and
the elements $\prod_{i \in S} M'_{i,i}$ contribute $(-1)^{m-k}
\lambda^{m-k}$ and the coefficient comes from the constant term in
$\sum_{\tau \in Perm([m]-S)} sgn(\tau) \prod_{i \notin S} M'_{i,
\tau(i)}$. This, by induction hypothesis, is equal to $\sum_{S,
|S|=m-k} det(M_{[m]-S})$. Hence
\[ c_{m-k} = (-1)^{m-k} \sum_{S, |S|=m-k} det(M_{[m]-S}) = (-1)^{m-k}
\sum_{\tiny \begin{array}{l} B, B~ principal \\ k-minor~ of~ M
\end{array}} det(B) \]
\end{proof}

Volume sampling leads to the following existence result for interpolative low-rank approximation.
\begin{theorem}\label{EXISTENCE}
Any matrix $A$ contains a set of $2k\log (k+1) + (4k/\eps)$ rows in whose span lies a rank-$k$ matrix $\tilde{A}_k$ with the property that
\[
\|A-\tilde{A}_k\|_F^2 \le (1+\eps) \|A-A_k\|_F^2.
\]
\end{theorem}
The proof follows from using Theorem \ref{KROWS} followed by multiple rounds of adaptive length-squared sampling.
\begin{exercise}\label{ex:EXISTENCE}
Prove Theorem \ref{EXISTENCE}.
\end{exercise}

The next exercise gives a fast procedure that approximates the volume sampling distribution.

\begin{exercise}
Let $S$ be a subset of $k$ rows of a given matrix $A$ generated as follows: The first row is picked from $LS_{row(A)}$. The $i$'th row is picked from $LS_{row(\hat{A}^i)}$ where $\hat{A}^i$ is the projection of $A$ orthogonal to the span of the first $i-1$ rows chosen.
\begin{enumerate}
\item Show that
\[
\E\left(\|A-\pi_{S}(A)\|_F^2\right) \le (k+1)! \|A-A_k\|_F^2.
\]
\item As in Exercise \ref{ex:EXISTENCE}, use adaptive length-squared sampling to reduce the error to $(1+\eps)$. What is the overall time complexity and the total number of rows sampled?
\end{enumerate}
\end{exercise}


\subsection{A lower bound}
The following proposition shows
that Theorem \ref{KROWS} is tight.
\begin{proposition}\label{TIGHT}
Given any $\epsilon > 0$, there exists a $(k+1) \times (k+1)$
matrix $A$ such that for any subset $S$ of $k$ rows of $A$,
\[ \| A - \pi_{S,k} (A) \|_F^2 \geq (1-\epsilon)~(k+1)~ \| A - A_k \|_F^2
\]
\end{proposition}
\begin{proof}
%\begin{figure}
%\centerline{\psrotatefirst
%\psfig{figure=simplex.ps,height=3in,angle=-90}}
%\caption{\label{first} The $(k+1)$-bound is asymptotically tight.}
%\end{figure}
The tight example consists of a matrix with $k+1$ rows which are
the vertices of a regular $k$-dimensional simplex lying on the
affine hyperplane $\{ X_{k+1} = \alpha \}$ in $\R^{k+1}$. Let
$A^{(1)}, A^{(2)}, \ldots, A^{(k+1)}$ be the vertices with the
point $p = (0,0, \ldots, 0, \alpha)$ as their centroid. For
$\alpha$ small enough, the best $k$ dimensional subspace for these
points is given by $\{ X_{k+1}=0 \}$ and
\[ \| A - A_k
\|_F^2 = (k+1) \alpha^2 \] Consider any subset of $k$ points from
these, say $S = \{ A^{(1)}, A^{(2)}, \ldots, A^{(k)} \}$, and let
$H_S$ be the linear subspace spanning them. Then,
\[
\| A - \pi_{S,k}(A) \|_F^2 = d(A^{(k+1)}, H_S)^2.
\]
We claim that for any $\eps > 0$, $\alpha$ can be chosen small
enough so that
\[
d(A^{(k+1)}, H_S) \ge \sqrt{(1-\eps)} (k+1)\alpha.
\]
Choose $\alpha$ small enough so that $d(p,H_S) \ge
\sqrt{(1-\eps)}\alpha$. Now
\[
\frac{d(A^{(k+1)},H_S)}{d(p,H_S)} = \frac{d(A^{(k+1)},
\mbox{conv}(A^{(1)},\ldots,A^{(k)}))}{d(p,
\mbox{conv}(A^{(1)},\ldots,A^{(k)}))} = k+1
\]
since the points form a simplex and $p$ is their centroid. The
claim follows. Hence,
\[ \| A - \pi_{S,k} (A) \|_F^2 = d(A^{(k+1)}, H_S)^2 \geq (1-\epsilon)~
(k+1)^2~ \alpha^2 = (1-\epsilon)~ (k+1)~ \| A - A_k \|_F^2 \]
\end{proof}

\begin{exercise}
Extend the above lower bound to show that for $0 \le \eps \le 1/2$, there exist matrices for which one needs $\Omega(k/\eps)$ rows to span a rank-$k$ matrix that is a $(1+\eps)$ approximation.
\end{exercise}

\section{Isotropic random projection}

In this section, we describe another randomized algorithm which also gives relative
approximations to the optimal rank-$k$ matrix with roughly the same time complexity.
Moreover, it makes only {\em two} passes over the input data.

The idea behind the algorithm can be understood by going back to the matrix
multiplication algorithm described in Chapter \ref{chap:mm}. There to multiply two
matrices $A, B$, we picked random columns of $A$ and rows of $B$ and thus derived an
estimate for $AB$ from these samples. The error bound derived was additive and this is
unavoidable. Suppose that we first project the rows of $A$ randomly to a low-dimensional
subspace, i.e., compute $AR$ where $R$ is random and $n \times k$, and similarly project
the columns of $B$, then we can use the estimate $ARR^TB$. For low-rank approximation, the
idea extends naturally: first project the rows of $A$ using a random matrix $R$, then
project $A$ to the span of the columns of $AR$ (which is low dimensional), and finally
find the best rank $k$ approximation of this projection.

\begin{center}
\fbox{\parbox{4.7in}{
\begin{minipage}{4.5in}
\begin{sf}
{\bf Isotropic RP}\\

Input: $A \in \R^{m \times n}$ with $M$ non-zero entries, integers
$k \leq m$, error $\epsilon > 0$.

Output: A rank $k$ matrix $\tilde{A}_k$.

\begin{enumerate}
\item Let $l = Ck/\eps$ and $S$ be a random $l \times n$ matrix; compute $B=SA$.

\item Project $A$  o the span of the rows of $B$ to get $\tilde{A}$.

\item Output $\tilde{A}_k$, the best rank-$k$ approximation of $\tilde{A}$.

\end{enumerate}
\end{sf}
\end{minipage}
}} \end{center}

\begin{theorem}\label{thm:isoRP}
Let $A$ be an $m \times n$ real matrix with $M$ nonzeros. Let $0 < \eps < 1$ and $S$ be an $r \times n$
random matrix with i.i.d. Bernoulli entries with mean zero and $r \ge Ck/\eps$ where $C$ is a universal constant.
Then with probability at least $3/4$,
\[
\|A - \proj_{SA,k}(A)\|_F \le (1 + \eps) \|A - A_k\|_F
\]
and the singular vectors spanning $\proj_{SA,k}(A)$ can be computed in two passes over the data
in $O(Mr+(m+n)r^2)$ time using $O((m + n)r^2)$ space.
\end{theorem}

\begin{proof}(Outline)
Consider the rank $k$ matrix $D = A_kVV^T$ where $SA = U \Sigma V^T$ is the SVD of $SA$. The rows of $D$ lie in the span of the rows of $SA$.
Hence,
\[
\|A - \proj_{SA, k} A\|_F^2 \le \|A-D\|_F^2 = \|A-A_k\|_F^2 + \|A_k - D\|_F^2.
\]
We will now show that
\[
\|A_k -D\|_F^2 \le 2\eps \|A-A_k\|_F^2
\]
which completes the proof.

To see this, we can view each row of $A-A_k$ as a linear regression problem, namely,
\[
\min_{x} \|A^{(j)} - A_kx\|
\]
for $j=1,\ldots,n$ and let $x_1, \ldots, x_n$ be the solutions. The best approximation of $A^{(j)}$ from the row span of $A_k$ is $A_k^{(j)}$.
For a general linear regression problem,
\[
\min_x \| Ax -b \|
\]
the solution is $x = A^+ b$ where if $A = \hat{U}\hat{\Sigma}\hat{V}^T$ is the SVD of $A$, then $A^+ = \hat{V}\hat{\Sigma}^{-1}\hat{U}^T$
(see Exerice \ref{ex:regression}).
Now consider the linear regressions
\[
\min_x \|(SA)^{(j)} - (SA_k)x\|
\]
for $j=1,\ldots n$. Let their solutions be $\tilde{x}_1, \ldots, \tilde{x}_n$.
Then, there exist vectors $w_1, \ldots w_n$ orthogonal to the column span of $U_k$ and $\beta_1, \ldots, \beta_n \in \R^k$ such that
\begin{eqnarray*}
w_j &=& A^{(j)} - A_k^{(j)} \\
U\beta_j &=& A_k\tilde{x}_j - A_kx_j\\
\end{eqnarray*}
From this (through a series of computations), we have, for $j=1,\ldots, n$,
\[
(U_k^TS^TSU_k)\beta_j = U_k^TS^TSw_j
\]
Now we choose $r$ large enough so that $\sigma^2(SU) \ge 1/\sqrt{2}$ with probability at least $7/8$ and hence,
\[
\|A-D\|_F^2 = \sum_{j=1}^n \beta_j^2 \le 2\sum_{i=1}^n \|U_k^TS^TSw_j\|^2 \le 2\eps\sum_{j=1}^n\|w_j\|^2 = 2\eps\sum_{j=1}^n \|A-A_k\|_F^2.
\]
Here the penultimate step we used the fact that random projection preserves inner products approximately, i.e., given that
$w_j$ is orthogonal to $U_k$,
\[
|U_k^TS^TSw_j| \le \eps^2\|w_j\|^2.
\]
\end{proof}

\begin{exercise}\label{ex:regression}
Let $A$ be an $m \times n$ matrix with $m > n$ and $A = U\Sigma V^T$ be its SVD. Let $b \in \R^m$.
Then the point $x^*$ which minimizes $\| Ax -b\|$ is given by $x^*= V\Sigma^{-1}U^Tb$.
\end{exercise}

\section{Discussion}

In this chapter we saw asymptotically tight bounds on the number of rows/columns whose span contains a near-optimal rank-$k$ approximation of a given matrix. We also saw two different algorithms for obtaining such an approximation efficiently. Adaptive sampling was introduced in \cite{DRVW06}, volume sampling in \cite{DV2006} and isotropic RP in \cite{Sarlos06}.

The existence of such sparse interpolative approximations has a nice application to clustering. Given a set of points in $\R^n$, and integers $j,k$, the projective clustering problem asks for a set of $j$ $k$-dimensional subspaces such that the sum of squared distances of each point to its nearest subspace is minimized. Other objective functions, e.g., maximum distance or sum of distances have also been studied. The interpolative approximation suggests a simple enumerative algorithm: the optimal set of subspaces induce a partition of the point set; for each part, the subspace is given by the best rank-$k$ approximation of the subset (the SVD subspace). From the theorems of this chapter, we know that a good approximation to the latter lies in the span of a small number ($k/\eps$) of points. So, we simply enumerate over all subsets of points of this size, choosing $j$ of them at a time. For each such choice, we have to consider all "distinct" $k$-dimensional subspaces in their span. This can be achieved by a discrete set of subspaces of exponential size, but only in $k$ and $\eps$. For each choice of $j$ $k$-dimensional subspaces we compute the value of the objective function and output the minimum overall.

It is an open question to implement exact volume sampling efficiently, i.e., in time polynomial in both $n$ and $k$. Another open question is to approximate a given matrix efficiently (nearly linear time or better) while incurring low error in the spectral norm.

\chapter{Extensions of SVD}\label{chap:extensions}

In this chapter, we discuss two extensions of SVD which provide substantial improvements or breakthroughs for some problems. The first is an extension of low-rank approximation from matrices to tensors (used in Chapter \ref{chap:maxrcsp}). Then we study an affine-invariant version of PCA, called {\em Isotropic PCA}. At first glance, this appears to be a contradiction in terms; however, here is a natural definition with applications (learning mixtures). %Finally we discuss a noise-tolerant version of PCA called {\em Robust} PCA.

\section{Tensor decomposition via sampling}\label{sec:tensorsvd}

We recall the basic set up.
Corresponding to an $r$-dimensional tensor $A$, there is an $r$-linear form
which for a set of $r$ vectors,
$x^{(1)},x^{(2)},\ldots x^{(r-1)},x^{(r)} \in \R^n$, is defined as
$$A(x^{(1)},x^{(2)},\ldots x^{(r)})=
\sum_{i_1,i_2,\ldots i_{r} }
A_{i_1,i_2,\ldots i_{r-1},i_r} x^{(1)}_{i_1}x^{(2)}_{i_2},
\ldots x^{(r)}_{i_{r}}.$$
Recall the two norms of interest for tensors, the Frobenius norm and the $2$-norm:
\begin{eqnarray*}
||A||_F &=&\left(\sum A^2_{i_1,i_2,\ldots i_r}\right)^{\frac{1}{2}}\\
||A||_2 &=& \max_{x^{(1)},x^{(2)},\ldots x^{(r)}}
{A ( x^{(1)},x^{(2)},\ldots x^{(r-1)},x^{(r)} )
\over |x^{(1)}| |x^{(2)}|\ldots }.
\end{eqnarray*}

We begin with the existence of
a low-rank tensor decomposition.

\begin{lemma}\label{lem:LowRankEXIST} For any tensor $A$, and any $\epsilon>0$, there exist
$k\leq 1/\epsilon^2$ rank-1 tensors, $B_1,B_2,\ldots B_k$
such that
$$||A-(B_1+B_2+\ldots B_k)||_2\leq \epsilon ||A||_F.$$
\end{lemma}

\begin{proof}
If $||A||_2\leq\epsilon ||A||_F$, then
we are done. If not, there are vectors
$x^{(1)},x^{(2)},\ldots,x^{(r)}$, all of length 1 such that
\[
A(x^{(1)},x^{(2)},\ldots,x^{(r)}) \geq \epsilon ||A||_F.
\]
Now consider the $r$-dimensional array
\[
B=A-(A(x^{(1)},x^{(2)},\ldots,x^{(r)})) x^{(1)}\otimes x^{(2)}\otimes
\ldots x^{(r)}.
\]
It is easy to see that
\[
||B||_F^2=||A||_F^2-(A(x,y,z,\ldots )^2).
\]
We can repeat on $B$ and clearly this
process will only go on for at most $1/\epsilon^2$ steps.
\end{proof}

Recall that for any $r-1$ vectors $x^{(1)},x^{(2)},\ldots x^{(r-1)}$,
the vector
$A(x^{(1)},x^{(2)},\ldots x^{(r-1)},\cdot )$
has $i$'th component
\[
\sum_{i_1,i_2,\ldots i_{r-1}}
A_{i_1,i_2,\ldots i_{r-1},i} x^{(1)}_{i_1}x^{(2)}_{i_2},
\ldots x^{(r-1)}_{i_{r-1}}.
\]

We now present an algorithm to solve the following problem:
Given an $r$-dimensional tensor $A$, find unit vectors $x^{(1)},x^{(2)},\ldots,x^{(r)}$
maximizing $A(x^{(1)},x^{(2)},\ldots,x^{(r)})$ to within {\em additive error}
$\epsilon ||A||_F/2$.






\begin{center}
\fbox{\parbox{4.7in}{
\begin{minipage}{4.5 in}
\begin{sf}
{\bf Tensor decomposition}\\

Set $\eta = \eps^2/100r\sqrt{n}$ and $s = 10^5r^3/\eps^2$.

\begin{enumerate}
\item  Pick $s$ random $(r-1)$-tuples $(i_1,i_2,\ldots i_{r-1})$
with probabilities proportional
to the sum of squared entries on the line defined by it:
$$p(i_1,i_2,\ldots i_{r-1})=
{\sum_{i} A^2_{i_1,i_2,\ldots i_{r-1}, i}\over ||A||_F^2}.$$
Let $I$ be the set of $s$ $r-1$ tuples picked.

\item For each $i_1,i_2,\ldots i_{r-1}\in I$, enumerate all
possible values of $\hat{z}^{(1)}_{i_1},\hat{z}^{(2)}_{i_2},\ldots
\hat{z}^{(r-1)}_{i_{r-1}}$ whose coordinates are
in the set
$$J=\{ -1,-1+\eta ,-1+2\eta, \ldots 0,\ldots 1-\eta,1\}^{s(r-1)}.$$

\begin{enumerate}
\item
%Each $u\in J$ lead
%$\hat z^{(t)}_{i_t}=u_{(l-1)(r-1)+t}$ if $i_t$ is the $t$'th coordinate
%in the $l$'th $r-1$ tuple in $I$.
For each set of $\hat{z}^{(t)}$, for each $i\in V_r$, compute
$$y_i = \sum_{(i_1,\ldots i_{r-1})\in I} A(i_1,\ldots i_{r-1},i)
\hat z^{(1)}_{i_1}\ldots \hat z^{(r-1)}_{i_{r-1}}.$$
and normalize the resulting vector $y$ to be a unit vector.
%(a candidate for $z^{(r)})$.

\item Consider the $(r-1)$-dimensional array
$A(y)$ defined by
$$(A(y))_{i_1,i_2,\ldots i_{r-1}} = \sum_{i} A_{i_1,i_2,i_3\ldots
i_{r-1},i}  \quad y_i$$
and apply the algorithm recursively to find the optimum
$$A(y) (x^{(1)},x^{(2)},\ldots x^{(r-1)})$$
with $|x^{(1)}|=\ldots |x^{(r-1)}|=1$
to within additive error $\epsilon ||A(y)||_F/2$.
%(Note that $||A(y)||_F \leq ||A||_F$ by Cauchy-Schwartz).
\end{enumerate}

\item Output the set of vectors that give the maximum among all the
candidates.

\end{enumerate}
\end{sf}
\end{minipage}
}}
\end{center}

To see the idea behind the algorithm, let
$z^{(1)},z^{(2)},\ldots z^{(r)}$ be unit vectors
that maximize $A(x^{(1)},x^{(2)},\ldots,x^{(r)})$.
Since
\[
A(z^{(1)},\ldots z^{(r-1)},z^{(r)})=
z^{(r)}\cdot A(z^{(1)},\ldots z^{(r-1)},\cdot ),
\]
we have
\[
z^{(r)}=\frac{A(z^{(1)},z^{(2)},\ldots z^{(r-1)},\cdot )}
{|A(z^{(1)},z^{(2)},\ldots z^{(r-1)},\cdot )|}.
\]
Thus, $z^{(r)}$ is a function of $z^{(1)},z^{(2)},\ldots z^{(r-1)}$.
Therefore, we can estimate
the components of $z^{(r)}$ given random terms
in the sum $A(z^{(1)},\ldots z^{(r-1)},\cdot )$.
We will need only $s=O(r^3/\epsilon^2)$
terms for a good estimate.
Also, we do not need to know the
 $z^{(1)},z^{(2)},\ldots, z^{(r-1)}$ completely; only $s(r-1)$ of
coordinates in total will suffice. We
enumerate all possibilities for the values of these coordinates
%(in steps of a certain size) and one of the sets of coordinates we
%enumerate will correspond to the optimal
%$z^{(1)},z^{(2)},\ldots z^{(r-1)}$, whence we get the an estimate of
%$z^{(r)}$.
For each candidate $z^{(r)}$, we can reduce the problem
to maximizing an $(r-1)$-dimensional tensor and we solve this
recursively. We then take the best candidate set of vectors.

We proceed to analyze the algorithm and prove the following theorem.

\begin{theorem}\label{thm:FastTensor}
For any tensor $A$, and any $\epsilon >0$, we can find $k$
rank-1 tensors $B_1,B_2,\ldots B_k$, where $k\leq 4/\epsilon^2$,
in time $(n/\eps)^{O(1/\epsilon^4)}$
such that with probability at least $3/4$
we have
$$||A-(B_1+B_2+\ldots B_k)||_2\leq \epsilon ||A||_F.$$
\end{theorem}

For $r=2$, the running time can be improved to a fixed polynomial in $n$ and exponential only in $\eps$.
We begin by bounding the error introduced by the discretization.

\begin{lemma}
Let $z^{(1)},z^{(2)},\ldots z^{(r-1)}$ be the optimal
unit vectors. Suppose $w^{(1)},w^{(2)},\ldots w^{(r-1)}$ are obtained
from the $z$'s by rounding each coordinate down to the
nearest integer multiple of $\eta $, with $0 \le \eta < 1$. Then,
\[
\left| A(z^{(1)},\ldots z^{(r-1)},\cdot ) -
A(w^{(1)},\ldots w^{(r-1)},\cdot )\right|
\leq \eta r \sqrt{n}\|A\|_F.
%\frac{\epsilon^2}{100} ||A||_F.
\]
\end{lemma}

\begin{proof}
We can write
\begin{eqnarray*}
&&\left| A(z^{(1)},z^{(2)},\ldots z^{(r-1)},\cdot ) -
A(w^{(1)},w^{(2)},\ldots w^{(r-1)},\cdot )\right| \\
&&\leq
\left| A(z^{(1)},z^{(2)},\ldots z^{(r-1)},\cdot ) -
A(w^{(1)},z^{(2)},\ldots z^{(r-1)},\cdot )\right| +\\
&& \quad \left| A(w^{(1)},z^{(2)},\ldots z^{(r-1)},\cdot ) -
A(w^{(1)},w^{(2)},z^{(3)},\ldots z^{(r-1)},\cdot )\right| \ldots
\end{eqnarray*}
A typical term above is
\begin{eqnarray*}
&&|A(w^{(1)},\ldots w^{(t)},z^{(t+1)}, \ldots z^{(r-1)},\cdot )-A(w^{(1)},\ldots w^{(t)},w^{(t+1)},z^{(t+2)},
\ldots z^{(r-1)},\cdot )|\\
&&\leq \left| C(z^{(t+1)}-w^{(t+1)})\right| \\
&&\leq ||C||_2 |z^{(t+1)}-w^{(t+1)}|\\
&&\leq ||C||_F \eta \sqrt n\leq ||A||_F\eta \sqrt n.
\end{eqnarray*}
Here, $C$ is the matrix defined as the matrix whose $ij$'th entry is
\[
\sum_{j_1, \ldots j_t, j_{t+2}\ldots j_{r-1}}
A_{j_1,\ldots j_t, i, j_{t+2},\ldots j_{r-1}, j}
w^{(1)}_{j_1}\ldots w^{(t)}_{j_t}z^{(t+2)}_{j_{t+2}}\ldots z^{(r-1)}_{j_{r-1}}
\]
The claim follows.
\end{proof}

We analyze the error incurred by sampling in the next two lemmas.
%For convenience, we assume here that the $s$ trials sample
%without replacement - i.e., no $i_1$ is picked in more than
%1 trial, similarly no $i_2$ is picked more than once etc.
%[Since $s<<n$, this is not restrictive and does not change
%the results; in the final paper, we will supply a rigorous
%argument.]

\begin{lemma}\label{lem:tensormoments}
For an $(r-1)$-tuple $(i_1,i_2,\ldots i_{r-1})
\in I$, define the random variables
variables $X_i$ for $i=1,\ldots,n$ by
$$X_i= {A_{i_1,i_2,\ldots i_{r-1},i}w^{(1)}_{i_1}w^{(2)}_{i_2}\ldots
w^{(r-1)}_{i_{r-1}}
\over p(i_1,i_2,\ldots i_{r-1})} .$$
Then,
\[
\E(X_i)=A(w^{(1)},w^{(2)}\ldots w^{(r-1)},\cdot)_i.
\]
and
\[
\var(X_i) \le \|A\|_F^2.
\]
\end{lemma}
\begin{proof}
The expectation is immediate, while the variance can be estimated as follows:
\begin{eqnarray*}
\sum_{i} \var (X_i) &\leq& \sum_i \sum_{i_1,i_2,\ldots }
{   A^2_{i_1,i_2,\ldots i_{r-1},i} (w^{(1)}_{i_1}\ldots w^{(r-1)}_{i_{r-1}})^2
\over p(i_1,i_2,\ldots ) }\\
&\leq& \sum_{i_1,i_2,\ldots }
{(z^{(1)}_{i_1}\ldots z^{(r-1)}_{i_{r-1}})^2
\over p(i_1,i_2,\ldots ) }
\sum_i
A^2_{i_1,i_2,\ldots i_{r-1},i}\\
&\leq& ||A||_F^2 .
\end{eqnarray*}
\end{proof}

\begin{lemma}\label{lem:tensor-sampling-error}
Define
\[
\zeta = A(z^{(1)},z^{(2)},\ldots z^{(r-1)},\cdot).
%\quad \mbox{and} \quad \Delta = y - s\zeta.
\]
In the list of candidate vectors enumerated by the algorithm will be a vector $y$ such that
\[
\|A\left( {y\over|y|} \right)- A\left({\zeta \over |\zeta|}\right)\|_F\leq \frac{\epsilon}{10r} \|A\|_F.
\]
\end{lemma}

\begin{proof}
Consider
the vector $y$ computed by the algorithm when all $\hat z^{(t)}$ are set to
$w^{(t)}$, the rounded optimal vectors.
This will clearly happen sometime during the enumeration.
This $y_i$ is
just the sum of $s$ i.i.d. copies of $X_i$, one for each element of $I$.
Thus, we have that
$$E(y) = s A(w^{(1)},w^{(2)}\ldots w^{(r-1)},\cdot )$$
and
$$\var (y) = E( |y-E(y)|^2) \leq s ||A||_F^2.$$
%We will sketch the rest of the argument.

From the above, it follows that with probability
at least $1-(1/10r)$, we have
$$|\Delta |\leq  10r\sqrt s ||A||_F.$$
Using this,
\begin{eqnarray*}
\left| {y\over |y|}-{\zeta \over |\zeta |}\right| &=&
\frac{\left| (y|\zeta |-\zeta |y|)\right|}{|y||\zeta |} \\
&=& {1\over |y||\zeta |}
\left| (\Delta + s\zeta )|\zeta |-\zeta (|y|-s|\zeta |+s|\zeta |)\right|\\
&\leq& \frac{2|\Delta|}{(s|y|)} \leq \frac{\epsilon}{50r^2},
\end{eqnarray*}
assuming $|y |\geq \epsilon ||A||_F/100r$. If this
assumption does not hold,
we know that the $|\zeta |\leq \epsilon ||A||_F/20r$
and in this case, the all-zero tensor
is a good approximation to the optimum.
From this, it follows that
$$\|A\left( {y\over|y|} \right)- A\left({\zeta \over |\zeta|}\right)\|_F\leq \frac{\epsilon}{10r} \|A\|_F.$$
\end{proof}

Thus, for any $r-1$ unit length
vectors $a^{(1)},a^{(2)},\ldots a^{(r-1)}$, we have
$$\left| A\left(a^{(1)},\ldots a^{(r-1)},{y\over |y|}\right)-
A\left( a^{(1)},\ldots a^{(r-1)}, {\zeta \over |\zeta| }\right)\right|
\leq \frac{\epsilon}{10r} ||A||_F.$$
In words, the optimal set of vectors for $A(y/|y|)$
are nearly optimal for $A(\zeta / |\zeta |)$.
Since $z^{(r)} = \zeta/|\zeta|$,
the optimal vectors for the latter problem are $z^{(1)}, \ldots, z^{(r-1)}$. Applying this argument at every phase of the algorithm, we get a bound on the total error of $\eps\|A\|_F/10$.

The running time of algorithm is dominated by the number of candidates we
enumerate, and is at most
\[
\mbox{poly}(n)\left(\frac{1}{\eta}\right)^{s^2r} = \left(\frac{n}{\eps}\right)^{O(1/\eps^4)}.
\]

This completes the proof of Theorem \ref{thm:FastTensor}.

\section{Isotropic PCA}

In this section we discuss an extension of Principal Component Analysis (PCA) that
is able to go beyond standard PCA in identifying ``important''
directions. Suppose the covariance matrix of the input (distribution or
point set in $\R^n$) is a multiple of the identity. Then, PCA reveals
no information --- the second moment along any direction is the
same. The extension, called
{\em isotropic PCA}, can reveal interesting information in such
settings. In Chapter \ref{chap:mixtures}, we used this technique to give an affine-invariant clustering
algorithm for points in $\R^n$. When applied to the problem of
unraveling mixtures of arbitrary Gaussians from unlabeled samples, the
algorithm yields strong guarantees.

To illustrate the technique, consider the uniform distribution on the
set $X = \{(x,y) \in \mathbb{R}^2 : x \in \{-1,1\}, y \in
[-\sqrt{3},\sqrt{3}]\}$, which is isotropic.  Suppose this
distribution is rotated in an unknown way and that we would like to
recover the original $x$ and $y$ axes.  For each point in a sample, we
may project it to the unit circle and compute the covariance matrix of
the resulting point set.  The $x$ direction will correspond to the
greater eigenvector, the $y$ direction to the other.
%See Figure \ref{fig:2d-eg} for an illustration.
Instead of projection onto the
unit circle, this process may also be thought of as importance
weighting, a technique which allows one to simulate one distribution
with another.  In this case, we are simulating a distribution over the
set $X$, where the density function is proportional to $(1 +
y^2)^{-1}$, so that points near $(1,0)$ or $(-1,0)$ are more probable.

More generally, isotropic PCA first puts a given distribution in isotropic position, then reweights points using a spherically symmetric distribution and performs PCA on this reweighted distribution.
The core of PCA is finding a direction that maximizes the second moment.
When a distribution is isotropic, the second moment of a random point $X$ is the same for any direction $v$, i.e., $\E((v^TX)^2)$ is constant. In this situation, one could look for directions which maximize higher moments, e.g., the fourth moment. However, finding such directions seems to be hard. Roughly speaking, isotropic PCA finds directions which maximize a certain weighted averages of higher moments.

%\begin{figure}[h]
%\center
%\includegraphics[height=2in]{2d-eg}\label{fig:2d-eg}
%\caption{Mapping points to the unit circle and then finding the
%direction of maximum variance reveals the orientation of this
%isotropic distribution.}
%\end{figure}

In the description below, the input to the algorithm is a $m \times n$ matrix (rows are points in $\R^n$).

\begin{center}
\fbox{\parbox{4.7in}{
\begin{minipage}{4.5 in}
\begin{sf}
{\bf Isotropic PCA}\\

\begin{enumerate}
\item  Apply an isotropic transformation to the input data, so that the mean of the resulting data is zero and its covariance matrix is the identity.
\item  Weight each point using the density of a spherically symmetric weight function centered at zero, e.g., a spherical Gaussian.
\item Perform PCA on the weighted data.
\end{enumerate}
\end{sf}
\end{minipage}
}}
\end{center}

In the application to Gaussian mixtures, the reweighting density is indeed a spherical Gaussian.

%\section{Robust PCA}

\section{Discussion}
Tensors are natural generalizations of matrices and seem to appear in many data sets, e.g., network traffic -- (sender, receiver, time), or the web (document, term, hyperlink). However, many algorithmic problems that can be solved efficiently for matrices appear to be harder or intractable. Even finding the vector that maximizes the spectral norm of a tensor is NP-hard. Thus, it seems important to understand what properties of tensors or classes of tensors are algorithmically useful. The sampling-based tensor approximation presented here is from \cite{FKKV05}.

Isotropic PCA  was introduced in Brubaker and Vempala \cite{Brubaker2008} and applied to learning mixtures. It would be interesting to see if other problems could be tackled using this tool. In particular, the directions identified by the procedure might have significance in convex geometry and functional analysis.

\backmatter

%-----------------------------------------------------------------------------
% Beginning of biblio.tex
%-----------------------------------------------------------------------------
%
% AMS-LaTeX 1.2 sample file for a monograph, based on amsbook.cls.
% This is a data file input by chapter.tex.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\bibliographystyle{amsalpha}
\bibliography{spectralbook}
\end{document}

\begin{thebibliography}{A}




%examples
%\bibitem [A]{A} T. Aoki, \textit{Calcul exponentiel des op\'erateurs
%microdifferentiels d'ordre infini.} I, Ann. Inst. Fourier (Grenoble)
%\textbf{33} (1983), 227--250.
%
%
%\bibitem [D]{D} R. A. DeVore, \textit{Approximation of functions},
%Proc. Sympos. Appl. Math. vol. 36,
%Amer. Math. Soc., Providence, RI, 1986, pp. 34--56.


\bibitem{AlonSpencer} N. Alon and J. Spencer, \textit{The Probabilistic Method}, Wiley, 2000.

\bibitem{BHI} M. Badoiu, S. Har-Peled, and P. Indyk, \textit{Approximate clustering via core-sets},  Proc. of STOC, 250--257, 2002.

\bibitem{berry}
M.~W.~Berry, S.~T.~Dumais, and G.~W.~O'Brien,
\textit{Using linear algebra for intelligent information
retrieval}, SIAM Review {\bf 37}(4), 573--595, 1995.

\bibitem{BertV} D.~Bertsimas and S.~Vempala:
\textit{Solving convex programs by random walks},
JACM {\bf 51}(4), 2004.

\bibitem{DG} S. Dasgupta and A. Gupta, \textit{An elementary proof of the Johnson-Lindenstrauss Lemma}, U.C. Berkeley Tech. Rep. TR-99-006, 1999.

\bibitem{delaPena} V. H. de la Pena and E. Gine, Decoupling: From Dependence to Independence.
SIAM Review, Vol. 42, No. 2 (Jun., 2000), pp. 345-347.


\bibitem{DK} P. Drineas, R. Kannan. ``Pass Efficient Algorithm for
  approximating large matrices,'' Proceedings of 14th SODA, 2003.
  Proceedings of the 31st ICALP, 2004.

\bibitem{lsichi}
S.T.~Dumais, G.W.~Furnas, T.K.~Landauer, and S.~Deerwester,
\textit{Using latent semantic analysis to
improve information retrieval},  Proc. of CHI:
Conference on Human Factors in Computing, 281--285, 1988.

\bibitem{lsidum}
S.T.~Dumais, \textit{Improving the retrieval of information
from external sources}, Behavior Research
Methods, Instruments and Computers {\bf 23}(2), 229--236, 1991.


\bibitem{E}
H. Edelsbrunner, \textit{Algorithms in Combinatorial Geometry}, Springer, 1987.

\bibitem{F} W.~Feller,
{\em An Introduction to Probability Theory and Its Applications},
John Wiley and Sons, Inc., 1957.

\bibitem{FKKR} W. Fernandez de la Vega, M. Karpinski, C. Kenyon and Y. Rabani,
\textit{Approximation Schemes for Clustering Problems}, Proc. of STOC, 50--58, 2003.

\bibitem{FKV-focs}
A.~Frieze, R.~Kannan and S.~Vempala,
\textit{Fast Monte-Carlo Algorithms for finding low-rank
approximations}, Proc. of FOCS, 370--378, 1998.

\bibitem{FKV} A. Frieze, R. Kannan, S. Vempala.  ``Fast Monte-Carlo
  algorithms for finding low-rank approximations.''
Journal of the ACM, 51(6):1025-1041, 2004.

\bibitem{FM} P.~Frankl and H.~Maehara, \textit{The Johnson-Lindenstrauss Lemma and
the Sphericity of some graphs}, J. Comb. Theory B {\bf 44},
355--362, 1988.

\bibitem{golub}
G.~Golub and C.~Reinsch, \textit{Handbook for matrix computation II,
{L}inear {A}lgebra},  Springer-Verlag, New York, 1971.

\bibitem{GLS} M.~Gr{\"o}tschel, L.~Lov{\'a}sz, A. Schrijver,
{\sl Geometric Algorithms and Combinatorial Optimization},
Springer, 1988.

\bibitem{HLS} H. van der Holst, L. Lov\'{a}sz and A. Schrijver, \textit{The
Colin de Verdi\`{e}re graph parameter}, Bolyai Soc. Math. Stud. {\bf 7}, J\'anos Bolyai Math. Soc., Budapest, 29--85, 1999.

\bibitem{HRR} M. Henzinger, P. Raghavan, S. Rajagopalan.  ``Computing
  on Data Streams.''  Technical Note 1998-011, Digital Systems
  Research Center, Palo Alto, CA, May 1998.

\bibitem{JL} W.~B.~Johnson and J.~Lindenstrauss, \textit{Extensions of Lipshitz
mapping into Hilbert space}, Contemporary Mathematics {\bf 26},
189--206, 1984.


\bibitem{LovSha} L. Lov\'{a}sz, \textit{On the Shannon capacity of a graph},
IEEE Trans. Inform. Theory {\bf 25}, 1--7, 1979.



\bibitem{OR} R. Ostrovsky and Y. Rabani, \textit{Polynomial-time approximation schemes for geometric $k$-clustering}, JACM {\bf 49}(2), 139--156, 2002.

\bibitem{Papa-complexity} C.~H.~Papadimitriou, \textit{Computational Complexity}, Addison Wesley, 1994.

\bibitem{VC} V.~N. Vapnik and A.~Ya. Chervonenkis, \textit{On the uniform
convergence of relative frequencies of events to their probabilities},
Theory of Probability and its applications, {\bf  XVI}(2), 264--280, 1971.

\end{thebibliography}


%\include{appendix}
%\include{index}

\end{document}













